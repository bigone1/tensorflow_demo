{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 把数据转存到tfrecord，然后从tfrecord拿数据训练网络\n",
    "\n",
    "\n",
    "具体一些细节和疑问：\n",
    "所谓那套mnist的jpg从哪取:\n",
    "如果自己转，需要面临的问题有，txt文件和里边对应标签的处理。繁琐一些。\n",
    "先找到他提供的数据\n",
    "\n",
    "单次拿数据能否超过capacity数量？测试使用coord是否多此一举？还是coord反而是为了解决这个问题而存在的？\n",
    "\n",
    "可以再多练的：\n",
    "tfrecord基本操作。\n",
    "管道和线程\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "INPUT_SIZE = 784\n",
    "OUTPUT_SIZE = 10\n",
    "L1_SIZE = 500\n",
    "\n",
    "BATCH_SIZE = 200\n",
    "LEARNING_RATE_BASE = 0.1\n",
    "LEARNING_RATE_DECAY = 0.99\n",
    "REGULAR_SCALE = 0.0001\n",
    "STEPS = 50000\n",
    "MOVING_AVERAGE_DECAY = 0.99\n",
    "MODEL_SAVE_PATH = './model/'\n",
    "MODEL_NAME = 'mnist_model'\n",
    "\n",
    "DATA_DIR = 'MNIST_data'#本例没太大用，走tfrecord了\n",
    "\n",
    "train_num_examples = 60000#也就是mnist.train.num_examples返回的那个数值。\n",
    "\n",
    "\n",
    "\n",
    "image_train_path = 'tensorflow_mooc/fc4/mnist_data_jpg/mnist_train_jpg_60000/'\n",
    "label_train_path = 'tensorflow_mooc/fc4/mnist_data_jpg/mnist_train_jpg_60000.txt'\n",
    "image_test_path = 'tensorflow_mooc/fc4/mnist_data_jpg/mnist_test_jpg_10000/'\n",
    "label_test_path = 'tensorflow_mooc/fc4/mnist_data_jpg/mnist_test_jpg_10000.txt'\n",
    "\n",
    "tfRecord_train = 'tensorflow_mooc/fc4/data/mnist_train.tfrecords'\n",
    "tfRecord_test = 'tensorflow_mooc/fc4/data/mnist_test.tfrecords'\n",
    "\n",
    "data_path = 'tensorflow_mooc/fc4/my_mnist_tfRecord/'#同tensorflow_mooc/fc4/data\n",
    "resize_height = 28\n",
    "resize_width = 28\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def read_tfRecord(tfRecord_path):\n",
    "    filename_queue = tf.train.string_input_producer([tfRecord_path])#这应该是拿着路径生成一个文件名的队列\n",
    "    reader = tf.TFRecordReader()#reader对象\n",
    "    _, serialized_example = reader.read(filename_queue)#读出来，已经是example了？\n",
    "    features = tf.parse_single_example(serialized_example, features={'label':tf.FixedLenFeature([10],tf.int64),\n",
    "                                                                    'img_raw':tf.FixedLenFeature([],tf.string)})\n",
    "    img = tf.decode_raw(features['img_raw'], tf.uint8)\n",
    "    img.set_shape([784])\n",
    "    img = tf.cast(img, tf.float32) * (1./255.)\n",
    "    label = tf.cast(features['label'], tf.float32)\n",
    "    return img, label\n",
    "    \n",
    "def get_tfRecord(num, isTrain = True):\n",
    "    if isTrain:\n",
    "        tfRecord_path = tfRecord_train\n",
    "    else:\n",
    "        tfRecord_path = tfRecord_test\n",
    "    img, label = read_tfRecord(tfRecord_path)\n",
    "    \n",
    "    #他说”顺序取出capacity个，打乱顺序“\n",
    "    #取出capacity个打乱？从总样本中随机取capacity个？前者没意义，打乱再取，是随机batch，取再打乱，有什么意义？\n",
    "    #测试，如果是他说的，很容易验证，比如第二批，内部怎么打乱，所有元素都是按顺序取的第二批。把size弄成1好观察。\n",
    "    \n",
    "    #batch_size = num是穿进来的参数，和capacity不同，capacity更像是池内部的属性，这个应该是每次取出的\n",
    "    #举例，batch_size=100,那么这个容器就应该是1000个，900个，800个，700个，达到min，然后填满？\n",
    "    img_batch, label_batch = tf.train.shuffle_batch([img, label],\n",
    "                                                    batch_size = num,\n",
    "                                                   num_threads = 2,\n",
    "                                                   capacity = 1000,\n",
    "                                                   min_after_dequeue = 700)#因为是动态的，好下功能是池子剩余不到700就再填充。\n",
    "    \n",
    "    return img_batch, label_batch\n",
    "    \n",
    "    \n",
    "def write_tfRecord(tfRecordName, image_path, label_path):\n",
    "    writer = tf.python_io.TFRecordWriter(tfRecordName)\n",
    "    num_pic = 0\n",
    "    f = open(label_path, 'r')\n",
    "    contents = f.readlines()\n",
    "    f.close()\n",
    "    for content in contents:#遍历所有行（数据和标签）\n",
    "        value = content.split()#空格分割\n",
    "        img_path = image_path + value[0]\n",
    "        img = Image.open(img_path)\n",
    "        img_raw = img.tobytes()\n",
    "        labels = [0] * 10#one-hot形式\n",
    "        labels[int(value[1])] = 1\n",
    "        example = tf.train.Example(features = tf.train.Features(feature={#图片+标签\n",
    "            'img_raw':tf.train.Feature(bytes_list = tf.train.BytesList(value=[img_raw])),\n",
    "            'label':tf.train.Feature(int64_list = tf.train.Int64List(value=labels))\n",
    "        }))\n",
    "        writer.write(example.SerializeToString())\n",
    "        num_pic += 1\n",
    "        print('the number of picture is ',num_pic)\n",
    "    writer.close()\n",
    "    print('write tfrecord successfully')\n",
    "        \n",
    "def generate_tfRecord():\n",
    "    isExists = os.path.exists(data_path)\n",
    "    if not isExists:\n",
    "        os.makedirs(data_path)\n",
    "    else:\n",
    "        print('dir exists')\n",
    "    #从后两者取数据生成前者，所以，我还是得先准备好数据放到指定路径，目前没有\n",
    "    write_tfRecord(tfRecord_train, image_train_path, label_train_path)\n",
    "    write_tfRecord(tfRecord_test, image_test_path, label_test_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_weight(shape, regular_scale):\n",
    "    w = tf.Variable(tf.truncated_normal(shape,stddev=0.1))\n",
    "    if regular_scale != None:\n",
    "        tf.add_to_collection('regular_loss',tf.contrib.layers.l2_regularizer(regular_scale)(w))\n",
    "    return w\n",
    "def get_bias(shape):\n",
    "    b = tf.Variable(tf.zeros(shape))\n",
    "    return b\n",
    "def net(x,regular_scale):\n",
    "    w1 = get_weight([INPUT_SIZE,L1_SIZE], regular_scale)\n",
    "    b1 = get_bias([L1_SIZE])\n",
    "    l1 = tf.nn.relu(tf.matmul(x,w1)+b1)\n",
    "    \n",
    "    w2 = get_weight([L1_SIZE, OUTPUT_SIZE], regular_scale)\n",
    "    b2 = get_bias([OUTPUT_SIZE])\n",
    "    y = tf.matmul(l1,w2)+b2\n",
    "    return y  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def train():\n",
    "    x = tf.placeholder(tf.float32, [None,INPUT_SIZE])\n",
    "    y_ = tf.placeholder(tf.float32, [None,OUTPUT_SIZE])\n",
    "    y = net(x,REGULAR_SCALE)\n",
    "    global_step = tf.Variable(0, trainable = False)\n",
    "    \n",
    "    ce = tf.nn.sparse_softmax_cross_entropy_with_logits(logits = y, labels = tf.argmax(y_,1))#label需要转换成非one-hot\n",
    "    cem = tf.reduce_mean(ce)\n",
    "    loss = cem + tf.add_n(tf.get_collection('regular_loss'))\n",
    "    \n",
    "    learning_rate = tf.train.exponential_decay(LEARNING_RATE_BASE,global_step,\n",
    "                                               train_num_examples / BATCH_SIZE,\n",
    "                                              LEARNING_RATE_DECAY,\n",
    "                                               staircase = True)\n",
    "    train_op = tf.train.GradientDescentOptimizer(learning_rate).minimize(loss, global_step = global_step)\n",
    "    \n",
    "    ema = tf.train.ExponentialMovingAverage(MOVING_AVERAGE_DECAY, global_step)\n",
    "    ema_op = ema.apply(tf.trainable_variables())\n",
    "    with tf.control_dependencies([train_op, ema_op]):\n",
    "        train_op = tf.no_op(name='train')#train_op重名，也可以改名\n",
    "    \n",
    "    saver = tf.train.Saver()\n",
    "    img_batch, label_batch = get_tfRecord(BATCH_SIZE, isTrain = True)#替换原来的数据来源\n",
    "    \n",
    "    \n",
    "    writer = tf.summary.FileWriter('model_mnist_in_tfrecord_graph')\n",
    "    with tf.Session() as sess:\n",
    "        tf.global_variables_initializer().run()\n",
    "        \n",
    "        #断点续训：\n",
    "        ckpt = tf.train.get_checkpoint_state(MODEL_SAVE_PATH)\n",
    "        if ckpt and ckpt.model_checkpoint_path:\n",
    "            saver.restore(sess, ckpt.model_checkpoint_path)\n",
    "            \n",
    "        #如何使for循环中的img_batch每次不同？用sess.run()从管道取的,赋值给xs，ys\n",
    "        #临时开启多线程，提升管道的读取效率。\n",
    "        coord = tf.train.Coordinator()\n",
    "        threads = tf.train.start_queue_runners(sess = sess, coord = coord)\n",
    "        \n",
    "        for i in range(STEPS):\n",
    "            xs, ys = sess.run([img_batch, label_batch])#data.train.next_batch(BATCH_SIZE)\n",
    "            _, loss_val, step = sess.run([train_op,loss,global_step],feed_dict={x:xs, y_:ys})#替换原来的数据来源\n",
    "            if i % 1000 == 0:\n",
    "                print('after %d steps, total loss is %g'%(step,loss_val))\n",
    "                saver.save(sess, os.path.join(MODEL_SAVE_PATH, MODEL_NAME), global_step=global_step)\n",
    "        coord.request_stop()\n",
    "        coord.join(threads)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from ./model/mnist_model-54002\n",
      "after 54003 steps, total loss is 0.124507\n",
      "after 55003 steps, total loss is 0.122255\n",
      "after 56003 steps, total loss is 0.118437\n",
      "after 57003 steps, total loss is 0.128632\n",
      "after 58003 steps, total loss is 0.120297\n",
      "after 59003 steps, total loss is 0.121506\n",
      "after 60003 steps, total loss is 0.129775\n",
      "after 61003 steps, total loss is 0.122284\n",
      "after 62003 steps, total loss is 0.121707\n",
      "after 63003 steps, total loss is 0.117267\n",
      "after 64003 steps, total loss is 0.120999\n",
      "after 65003 steps, total loss is 0.116291\n",
      "after 66003 steps, total loss is 0.124315\n",
      "INFO:tensorflow:Error reported to Coordinator: <class 'tensorflow.python.framework.errors_impl.CancelledError'>, Run call was cancelled\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-6-31984fdcd049>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m     \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0m__name__\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'__main__'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m     \u001b[0mmain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-6-31984fdcd049>\u001b[0m in \u001b[0;36mmain\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mmain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m     \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0m__name__\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'__main__'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0mmain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-5-950095e07db4>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m()\u001b[0m\n\u001b[1;32m     38\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     39\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mSTEPS\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 40\u001b[0;31m             \u001b[0mxs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mys\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msess\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mimg_batch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabel_batch\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;31m#data.train.next_batch(BATCH_SIZE)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     41\u001b[0m             \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss_val\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstep\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msess\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtrain_op\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mglobal_step\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mfeed_dict\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mxs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mys\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;31m#替换原来的数据来源\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     42\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mi\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;36m1000\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/qw/anaconda3/envs/py36/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    787\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    788\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 789\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    790\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    791\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/qw/anaconda3/envs/py36/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    995\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    996\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[0;32m--> 997\u001b[0;31m                              feed_dict_string, options, run_metadata)\n\u001b[0m\u001b[1;32m    998\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    999\u001b[0m       \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/qw/anaconda3/envs/py36/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_run\u001b[0;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1130\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1131\u001b[0m       return self._do_call(_run_fn, self._session, feed_dict, fetch_list,\n\u001b[0;32m-> 1132\u001b[0;31m                            target_list, options, run_metadata)\n\u001b[0m\u001b[1;32m   1133\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1134\u001b[0m       return self._do_call(_prun_fn, self._session, handle, feed_dict,\n",
      "\u001b[0;32m/home/qw/anaconda3/envs/py36/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1137\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1138\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1139\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1140\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1141\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/qw/anaconda3/envs/py36/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[0;34m(session, feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[1;32m   1119\u001b[0m         return tf_session.TF_Run(session, options,\n\u001b[1;32m   1120\u001b[0m                                  \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1121\u001b[0;31m                                  status, run_metadata)\n\u001b[0m\u001b[1;32m   1122\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1123\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_prun_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msession\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "def main():\n",
    "    train()\n",
    "if __name__ == '__main__':\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ema_restore length: 4\n",
      "ema_restore: {'Variable_1/ExponentialMovingAverage': <tf.Variable 'Variable_1:0' shape=(500,) dtype=float32_ref>, 'Variable_3/ExponentialMovingAverage': <tf.Variable 'Variable_3:0' shape=(10,) dtype=float32_ref>, 'Variable/ExponentialMovingAverage': <tf.Variable 'Variable:0' shape=(784, 500) dtype=float32_ref>, 'Variable_2/ExponentialMovingAverage': <tf.Variable 'Variable_2:0' shape=(500, 10) dtype=float32_ref>}\n",
      "saver: <tensorflow.python.training.saver.Saver object at 0x7f4b0c9f0780>\n",
      "saver print end\n",
      "INFO:tensorflow:Restoring parameters from ./model/mnist_model-66003\n",
      "after 66003teps, accuracy is 0.981100\n",
      "INFO:tensorflow:Restoring parameters from ./model/mnist_model-66003\n",
      "after 66003teps, accuracy is 0.981200\n",
      "INFO:tensorflow:Restoring parameters from ./model/mnist_model-66003\n",
      "after 66003teps, accuracy is 0.981500\n",
      "INFO:tensorflow:Restoring parameters from ./model/mnist_model-66003\n",
      "after 66003teps, accuracy is 0.981500\n",
      "INFO:tensorflow:Restoring parameters from ./model/mnist_model-66003\n",
      "after 66003teps, accuracy is 0.981000\n",
      "INFO:tensorflow:Restoring parameters from ./model/mnist_model-66003\n",
      "after 66003teps, accuracy is 0.980900\n",
      "INFO:tensorflow:Restoring parameters from ./model/mnist_model-66003\n",
      "after 66003teps, accuracy is 0.981300\n",
      "INFO:tensorflow:Restoring parameters from ./model/mnist_model-66003\n",
      "after 66003teps, accuracy is 0.981000\n",
      "INFO:tensorflow:Restoring parameters from ./model/mnist_model-66003\n",
      "after 66003teps, accuracy is 0.981000\n",
      "INFO:tensorflow:Restoring parameters from ./model/mnist_model-66003\n",
      "after 66003teps, accuracy is 0.980900\n"
     ]
    }
   ],
   "source": [
    "#测试\n",
    "\n",
    "import tensorflow as tf\n",
    "import time\n",
    "TEST_INTERVAL_SECS = 5\n",
    "TEST_NUM = 10000\n",
    "def test():\n",
    "    with tf.Graph().as_default() as g:\n",
    "        x = tf.placeholder(tf.float32, [None, INPUT_SIZE])\n",
    "        y_ = tf.placeholder(tf.float32, [None, OUTPUT_SIZE])\n",
    "        y = net(x,None)#Net结构是直接读取的定义，ema的变量都是从这找的\n",
    "        \n",
    "        ema = tf.train.ExponentialMovingAverage(MOVING_AVERAGE_DECAY)\n",
    "        #这是一套映射：如果变量有ema，就读ema，否则，用普通。\n",
    "        #用ema resotre来初始化saver，影响saver.restore的读取内容吗？也就是说，能读取bias吗？\n",
    "        #根据形状判断（因为变量名不可读），应该是有bias的\n",
    "        ema_restore = ema.variables_to_restore()\n",
    "        #打印的含义？为何有Exponential，是已经读到了？还是按规则自己假设的变量名？应该是按规则预设的变量名\n",
    "        print('ema_restore length:',len(ema_restore))\n",
    "        print('ema_restore:',ema_restore)\n",
    "        saver = tf.train.Saver(ema_restore)\n",
    "        print('saver:',saver)#<class 'tensorflow.python.training.saver.Saver'>\n",
    "        print('saver print end')\n",
    "        correct_prediction = tf.equal(tf.argmax(y,1), tf.argmax(y_,1))\n",
    "        accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
    "        \n",
    "        \n",
    "        img_batch, label_batch = get_tfRecord(TEST_NUM, isTrain = False)#取数据流程同train\n",
    "        #为了测这个，可以写个for循环，任意次\n",
    "#         while True:#把N秒一次改为总共一次\n",
    "        for i in range(10):\n",
    "            with tf.Session() as sess:\n",
    "                ckpt = tf.train.get_checkpoint_state(MODEL_SAVE_PATH)\n",
    "                if ckpt and ckpt.model_checkpoint_path:\n",
    "                    saver.restore(sess, ckpt.model_checkpoint_path)#他只是从proto拿了路径，还是自己的restore\n",
    "                    global_step = ckpt.model_checkpoint_path.split('/')[-1].split('-')[-1]\n",
    "                    \n",
    "                    #在这个大循环下，使用这个模式是否多此一举？\n",
    "                    #capacity只有1000的，你这要获取10000？拿得出来？\n",
    "                    #如果不用，会怎么样？\n",
    "                    \n",
    "                    coord = tf.train.Coordinator()\n",
    "                    threads = tf.train.start_queue_runners(sess = sess, coord = coord)\n",
    "                    xs, ys = sess.run([img_batch, label_batch])\n",
    "                    accuracy_score = sess.run(accuracy,feed_dict={x:xs,y_:ys})\n",
    "                    \n",
    "                    print('after % steps, accuracy is %f'%(global_step,accuracy_score))\n",
    "\n",
    "                    coord.request_stop()\n",
    "                    coord.join(threads)\n",
    "                    \n",
    "                else:\n",
    "                    print('no ckpt')\n",
    "                    return\n",
    "            time.sleep(TEST_INTERVAL_SECS)\n",
    "def main():\n",
    "    test()\n",
    "if __name__ == '__main__':\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 做一套小数据集的tfrecord，做测试"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dir exists\n",
      "the number of picture is  1\n",
      "the number of picture is  2\n",
      "the number of picture is  3\n",
      "the number of picture is  4\n",
      "the number of picture is  5\n",
      "the number of picture is  6\n",
      "the number of picture is  7\n",
      "the number of picture is  8\n",
      "the number of picture is  9\n",
      "the number of picture is  10\n",
      "the number of picture is  11\n",
      "the number of picture is  12\n",
      "the number of picture is  13\n",
      "the number of picture is  14\n",
      "the number of picture is  15\n",
      "the number of picture is  16\n",
      "the number of picture is  17\n",
      "the number of picture is  18\n",
      "the number of picture is  19\n",
      "the number of picture is  20\n",
      "write tfrecord successfully\n",
      "the number of picture is  1\n",
      "the number of picture is  2\n",
      "the number of picture is  3\n",
      "the number of picture is  4\n",
      "the number of picture is  5\n",
      "the number of picture is  6\n",
      "the number of picture is  7\n",
      "the number of picture is  8\n",
      "the number of picture is  9\n",
      "the number of picture is  10\n",
      "the number of picture is  11\n",
      "the number of picture is  12\n",
      "the number of picture is  13\n",
      "the number of picture is  14\n",
      "the number of picture is  15\n",
      "the number of picture is  16\n",
      "the number of picture is  17\n",
      "the number of picture is  18\n",
      "the number of picture is  19\n",
      "the number of picture is  20\n",
      "write tfrecord successfully\n"
     ]
    }
   ],
   "source": [
    "mini_data_path = 'tensorflow_mooc/fc4/mini_tfRecord/'#同tensorflow_mooc/fc4/data\n",
    "mini_tfRecord_train = 'tensorflow_mooc/fc4/mini_tfRecord/mini_mnist_train.tfrecords'\n",
    "mini_tfRecord_test = 'tensorflow_mooc/fc4/mini_tfRecord/mini_mnist_test.tfrecords'\n",
    "\n",
    "MINI_BATCH_NUM = 20\n",
    "\n",
    "def write_tfRecord_mini(tfRecordName, image_path, label_path):\n",
    "    writer = tf.python_io.TFRecordWriter(tfRecordName)\n",
    "    num_pic = 0\n",
    "    f = open(label_path, 'r')\n",
    "#     contents = f.readlines()\n",
    "    contents = [f.readline() for x in range(MINI_BATCH_NUM)]\n",
    "    f.close()\n",
    "    for content in contents[:MINI_BATCH_NUM]:#mini，做少点\n",
    "        value = content.split()#空格分割\n",
    "        img_path = image_path + value[0]\n",
    "        img = Image.open(img_path)\n",
    "        img_raw = img.tobytes()\n",
    "        labels = [0] * 10#one-hot形式\n",
    "        labels[int(value[1])] = 1\n",
    "        example = tf.train.Example(features = tf.train.Features(feature={#图片+标签\n",
    "            'img_raw':tf.train.Feature(bytes_list = tf.train.BytesList(value=[img_raw])),\n",
    "            'label':tf.train.Feature(int64_list = tf.train.Int64List(value=labels))\n",
    "        }))\n",
    "        writer.write(example.SerializeToString())\n",
    "        num_pic += 1\n",
    "        print('the number of picture is ',num_pic)\n",
    "    writer.close()\n",
    "    print('write tfrecord successfully')\n",
    "        \n",
    "def generate_tfRecord_mini():\n",
    "    isExists = os.path.exists(mini_data_path)\n",
    "    if not isExists:\n",
    "        os.makedirs(mini_data_path)\n",
    "    else:\n",
    "        print('dir exists')\n",
    "    write_tfRecord_mini(mini_tfRecord_train, image_train_path, label_train_path)\n",
    "    write_tfRecord_mini(mini_tfRecord_test, image_test_path, label_test_path)\n",
    "generate_tfRecord_mini()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'dict'>\n",
      "{'img_raw': <tf.Tensor 'ParseSingleExample/Squeeze_img_raw:0' shape=() dtype=string>, 'label': <tf.Tensor 'ParseSingleExample/Squeeze_label:0' shape=(10,) dtype=int64>}\n",
      "img: Tensor(\"DecodeRaw:0\", shape=(?,), dtype=uint8)\n",
      "reshape img: Tensor(\"DecodeRaw:0\", shape=(784,), dtype=uint8)\n",
      "cast img: Tensor(\"mul:0\", shape=(784,), dtype=float32)\n",
      "Tensor(\"mul:0\", shape=(784,), dtype=float32)\n",
      "(784,)\n",
      "Tensor(\"Cast_1:0\", shape=(10,), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "#label应该是所有样本的label都算，raw也是。运行卡死了。可能train六万个数据太大了，尝试做一个小文件\n",
    "#20个数据仍然会卡？\n",
    "#不能直接run提取？\n",
    "def read_tfRecord_mini(tfRecord_path):\n",
    "    filename_queue = tf.train.string_input_producer([tfRecord_path])#这应该是拿着路径生成一个文件名的队列\n",
    "    reader = tf.TFRecordReader()#reader对象\n",
    "    _, serialized_example = reader.read(filename_queue)#读出来，已经是example了？\n",
    "    features = tf.parse_single_example(serialized_example, features={'label':tf.FixedLenFeature([10],tf.int64),\n",
    "                                                                    'img_raw':tf.FixedLenFeature([],tf.string)})\n",
    "    print(type(features))\n",
    "    print(features)\n",
    "    img = tf.decode_raw(features['img_raw'], tf.uint8)\n",
    "    print('img:',img)\n",
    "    img.set_shape([784])\n",
    "    print('reshape img:',img)\n",
    "    img = tf.cast(img, tf.float32) * (1./255.)\n",
    "    print('cast img:',img)\n",
    "    label = tf.cast(features['label'], tf.float32)\n",
    "    return img, label\n",
    "\n",
    "img,label = read_tfRecord_mini(mini_tfRecord_train)\n",
    "print(img)\n",
    "print(img.shape)\n",
    "print(label)\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    print('run img:',sess.run(label))\n",
    "#     print('run img:',sess.run(img))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#运行卡死了。可能train六万个数据太大了，尝试做一个小文件\n",
    "    \n",
    "#他说”顺序取出capacity个，打乱顺序“\n",
    "#取出capacity个打乱？从总样本中随机取capacity个？前者没意义，打乱再取，是随机batch，取再打乱，有什么意义？\n",
    "#测试，如果是他说的，很容易验证，比如第二批，内部怎么打乱，所有元素都是按顺序取的第二批。把size弄成1好观察。\n",
    "\n",
    "#batch_size = num是穿进来的参数，和capacity不同，capacity更像是池内部的属性，这个应该是每次取出的\n",
    "#举例，batch_size=100,那么这个容器就应该是1000个，900个，800个，700个，达到min，然后填满？\n",
    "def get_tfRecord_mini(num, isTrain = True):\n",
    "    if isTrain:\n",
    "        tfRecord_path = tfRecord_train\n",
    "    else:\n",
    "        tfRecord_path = tfRecord_test\n",
    "    img, label = read_tfRecord_mini(tfRecord_path)\n",
    "\n",
    "    img_batch, label_batch = tf.train.shuffle_batch([img, label],\n",
    "                                                    batch_size = num,\n",
    "                                                   num_threads = 2,\n",
    "                                                   capacity = 2,\n",
    "                                                   min_after_dequeue = 1)\n",
    "    return img_batch, label_batch\n",
    "\n",
    "#测试，一次取3个，大于capacity的2个，会怎么样\n",
    "img_batch, label_batch = get_tfRecord_mini(2,True)\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    print(sess.run(img_batch))\n",
    "    print(sess.run(label_batch))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 基本接口和示例"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "writer = tf.python_io.TFRecordWriter('recordname')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "2\n"
     ]
    }
   ],
   "source": [
    "#测coord\n",
    "#普通python循环在这里不行，既然他也绑定sess，肯定还是得用tf的操作\n",
    "#不好直观测，这个需要多个线程，本例是用的start_queue_runners直接黑盒做的。\n",
    "a = tf.Variable(1)\n",
    "op = tf.assign_add(a,1)\n",
    "b = tf.Variable(1)\n",
    "op2 = tf.assign_add(b,1)\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    tf.global_variables_initializer().run()\n",
    "    print(sess.run(a))\n",
    "    sess.run(op)\n",
    "    print(sess.run(a))\n",
    "    coord = tf.train.Coordinator()\n",
    "    threads = tf.train.start_queue_runners(sess = sess, coord = coord)\n",
    "#     for i in range(1000):\n",
    "#         print(sess.run(a),end=',')\n",
    "#         sess.run(op)\n",
    "#     for i in range(1000):\n",
    "#         print(sess.run(b),end=',')\n",
    "#         sess.run(op2)\n",
    "    coord.request_stop()\n",
    "    coord.join(threads)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# help(tf.train.Coordinator)\n",
    "#这个协调器打开以后，打开多个线程，能等待他们一起结束\n",
    "# .|  #### Usage:\n",
    "#  |  \n",
    "#  |  ```python\n",
    "#  |  # Create a coordinator.\n",
    "#  |  coord = Coordinator()\n",
    "#  |  # Start a number of threads, passing the coordinator to each of them.\n",
    "#  |  ...start thread 1...(coord, ...)\n",
    "#  |  ...start thread N...(coord, ...)\n",
    "#  |  # Wait for all the threads to terminate.\n",
    "#  |  coord.join(threads)\n",
    "#  |  ```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on function start_queue_runners in module tensorflow.python.training.queue_runner_impl:\n",
      "\n",
      "start_queue_runners(sess=None, coord=None, daemon=True, start=True, collection='queue_runners')\n",
      "    Starts all queue runners collected in the graph.\n",
      "    \n",
      "    This is a companion method to `add_queue_runner()`.  It just starts\n",
      "    threads for all queue runners collected in the graph.  It returns\n",
      "    the list of all threads.\n",
      "    \n",
      "    Args:\n",
      "      sess: `Session` used to run the queue ops.  Defaults to the\n",
      "        default session.\n",
      "      coord: Optional `Coordinator` for coordinating the started threads.\n",
      "      daemon: Whether the threads should be marked as `daemons`, meaning\n",
      "        they don't block program exit.\n",
      "      start: Set to `False` to only create the threads, not start them.\n",
      "      collection: A `GraphKey` specifying the graph collection to\n",
      "        get the queue runners from.  Defaults to `GraphKeys.QUEUE_RUNNERS`.\n",
      "    \n",
      "    Raises:\n",
      "      ValueError: if `sess` is None and there isn't any default session.\n",
      "      TypeError: if `sess` is not a `tf.Session` object.\n",
      "    \n",
      "    Returns:\n",
      "      A list of threads.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(tf.train.start_queue_runners)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "virtual py36",
   "language": "python",
   "name": "py36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
