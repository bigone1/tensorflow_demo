{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 定义网络模型，从npy读取vgg参数，进行预测。\n",
    "对应相对路径vgg_mooc\n",
    "模型参数npy去根目录找，避免重复。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#utils.py\n",
    "# 读取图片的工具\n",
    "from skimage import io,transform\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "from pylab import mpl\n",
    "\n",
    "#help(mpl.rcParams)#带验证的字典，验证函数已经定义并且和rc参数绑定好\n",
    "mpl.rcParams['font.sans-serif'] = ['SimHei']#显示中文标签\n",
    "mpl.rcParams['axes.unicode_minus'] = False #显示正负号\n",
    "\n",
    "#读取图片，居中裁剪并缩放，打印显示三者\n",
    "def load_image(path):\n",
    "    fig = plt.figure('Centre and Resize')\n",
    "    img = io.imread(path)\n",
    "    #shape是611*711*3,RGB的，后边要把RGB转换一下。\n",
    "    print(img.shape)\n",
    "    ax0 = fig.add_subplot(131)\n",
    "    ax0.set_xlabel(u'Original Picture')\n",
    "    ax0.imshow(img)\n",
    "    \n",
    "    #宽和高取一个，比短边多出来的部分的一半(有一个是空)做起点，\n",
    "    #其实就是，居中截一个正方形。\n",
    "    short_edge = min(img.shape[:2])\n",
    "    y = int((img.shape[0] - short_edge) / 2)\n",
    "    x = int((img.shape[1] - short_edge) / 2)\n",
    "    #print('x:',x,' y:',y) \n",
    "    crop_img = img[y:y+short_edge, x:x+short_edge]#裁剪，\n",
    "    ax1 = fig.add_subplot(132)\n",
    "    ax1.set_xlabel(u'Centre Picture')\n",
    "    ax1.imshow(crop_img)\n",
    "    \n",
    "    re_img = transform.resize(crop_img, (224, 224))#缩放\n",
    "    \n",
    "    ax2 = fig.add_subplot(133)\n",
    "    ax2.set_xlabel(u'Resize Picture')\n",
    "    ax2.imshow(re_img)\n",
    "    \n",
    "    img_ready = re_img.reshape((1,224,224,3))#转成tf用的数据维度\n",
    "    return img_ready\n",
    "def percent(value):\n",
    "    return '%.2f%%'%(value*100)\n",
    "    \n",
    "#测试    \n",
    "load_image('pic/0.jpg')\n",
    "plt.show()#手动show一下。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#vgg16.py\n",
    "#重建网络结构并读取网络参数\n",
    "import inspect\n",
    "import os\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import time\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "VGG_MEAN = [103.939, 116.779, 123.68]#手动设的平均值，这顺序已经不是RGB了。\n",
    "\n",
    "class Vgg16():\n",
    "    def __init__(self, vgg16_path = None):\n",
    "        if vgg16_path is None:\n",
    "            vgg16_path = os.path.join(os.getcwd(), '../../model_saved/vgg.npy')#相对路径\n",
    "            self.data_dict = np.load(vgg16_path, encoding='latin1').item()#item()是全部读出，遍历。\n",
    "    def net(self, images):\n",
    "        print('buid model started')\n",
    "        #怎么叫建立模型开始？这个不就是预测流程么？只有复制变量建立运算图的时间，不值得统计，可能只是为了证明直接load比训练要快吧。\n",
    "        #因为init没有建立模型，只是读了字典，这是把散的变量堆成一个网络结构（计算图）所用时间。\n",
    "        #forward还没执行sess.run，是两步，todo：测一下这个打印是会在哪一步发生？我觉得是第一步，一次搭建，多次sess.run()\n",
    "        start_time = time.time()\n",
    "        #预处理\n",
    "        rgb_scaled = images * 255.0\n",
    "        red, green, blue = tf.split(rgb_scaled, 3, 3)\n",
    "        bgr = tf.concat([blue - VGG_MEAN[0], green - VGG_MEAN[1], red - VGG_MEAN[2]], 3)\n",
    "        \n",
    "        self.conv1_1 = self.conv_layer(bgr,'conv1_1')\n",
    "        self.conv1_2 = self.conv_layer(self.conv1_1, 'conv1_2')\n",
    "        self.pool1 = self.max_pool_2x2(self.conv1_2, 'pool1')\n",
    "        \n",
    "        self.conv2_1 = self.conv_layer(self.pool1, 'conv2_1')\n",
    "        self.conv2_2 = self.conv_layer(self.conv2_1, 'conv2_2')\n",
    "        self.pool2 = self.max_pool_2x2(self.conv2_2, 'pool2')\n",
    "        \n",
    "        self.conv3_1 = self.conv_layer(self.pool2, \"conv3_1\")\n",
    "        self.conv3_2 = self.conv_layer(self.conv3_1, \"conv3_2\")\n",
    "        self.conv3_3 = self.conv_layer(self.conv3_2, \"conv3_3\")\n",
    "        self.pool3 = self.max_pool_2x2(self.conv3_3, \"pool3\")\n",
    "        \n",
    "        self.conv4_1 = self.conv_layer(self.pool3, \"conv4_1\")\n",
    "        self.conv4_2 = self.conv_layer(self.conv4_1, \"conv4_2\")\n",
    "        self.conv4_3 = self.conv_layer(self.conv4_2, \"conv4_3\")\n",
    "        self.pool4 = self.max_pool_2x2(self.conv4_3, \"pool4\")\n",
    "        \n",
    "        self.conv5_1 = self.conv_layer(self.pool4, \"conv5_1\")\n",
    "        self.conv5_2 = self.conv_layer(self.conv5_1, \"conv5_2\")\n",
    "        self.conv5_3 = self.conv_layer(self.conv5_2, \"conv5_3\")\n",
    "        self.pool5 = self.max_pool_2x2(self.conv5_3, \"pool5\")\n",
    "        \n",
    "        self.fc6 = self.fc_layer(self.pool5, 'fc6')\n",
    "        self.relu6 = tf.nn.relu(self.fc6)\n",
    "        self.fc7 = self.fc_layer(self.relu6, 'fc7')\n",
    "        self.relu7 = tf.nn.relu(self.fc7)\n",
    "        self.fc8 = self.fc_layer(self.relu7, 'fc8')\n",
    "        self.prob = tf.nn.softmax(self.fc8, name = 'prob')#不return，直接取成员。\n",
    "        \n",
    "        end_time = time.time()\n",
    "        print('time sonsuming:',end_time - start_time)\n",
    "        self.data_dict = None#清空了。。。\n",
    "        \n",
    "        \n",
    "    def conv_layer(self, x, name):\n",
    "        w = self.get_weights(name)\n",
    "        b = self.get_biases(name)\n",
    "        #conv的ksize和strides维度不一致（比如，3,3,512,512），而pool一致(不过pool也不用weights，conv的ksize就是weights size）。\n",
    "        conv = tf.nn.conv2d(x, w, [1,1,1,1], padding='SAME')\n",
    "        conv = tf.nn.bias_add(conv, b)\n",
    "        relu = tf.nn.relu(conv)\n",
    "        return relu\n",
    "        \n",
    "    def get_weights(self, name):\n",
    "        return tf.constant(self.data_dict[name][0], name = 'filter')\n",
    "    \n",
    "    def get_biases(self, name):\n",
    "        return tf.constant(self.data_dict[name][1], name = 'biases')\n",
    "    \n",
    "    def max_pool_2x2(self, x, name):\n",
    "        return tf.nn.max_pool(x, ksize=[1,2,2,1], strides = [1,2,2,1],padding=\"SAME\",name=name)\n",
    "        \n",
    "    def fc_layer(self, x, name):#relu没放在内部，因为最后一层也算fc，但是不需要relu。\n",
    "        with tf.variable_scope(name):\n",
    "            dims = x.get_shape().as_list()\n",
    "            dim = 1\n",
    "            for i in dims[1:]:#集成了一个flatten操作，兼容fc之间和conv到fc过渡两种形态。\n",
    "                dim *= i\n",
    "            x = tf.reshape(x, [-1,dim])\n",
    "\n",
    "            w = self.get_fc_weights(name)#self.data_dict[name][0]\n",
    "            b = self.get_biases(name)#self.data_dict[name][1]\n",
    "            wx_plus_b = tf.nn.bias_add(tf.matmul(x, w),b)\n",
    "            return wx_plus_b\n",
    "    \n",
    "    def get_fc_weights(self, name):#只是起名不一样。我觉得起名不影响读变量吧？从dict拿的时候给对key就行了吧。todo:替换了试一下。\n",
    "        return tf.constant(self.data_dict[name][0],name = 'weights')\n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(611, 711, 3)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/qw/anaconda3/envs/py36/lib/python3.6/site-packages/skimage/transform/_warps.py:84: UserWarning: The default mode, 'constant', will be changed to 'reflect' in skimage 0.15.\n",
      "  warn(\"The default mode, 'constant', will be changed to 'reflect' in \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "buid model started\n",
      "time sonsuming: 1.0472784042358398\n"
     ]
    }
   ],
   "source": [
    "#app.py\n",
    "# 应用网络结构做出预测\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "from Nclasses import labels\n",
    "img_ready = load_image('pic/0.jpg') \n",
    "\n",
    "fig=plt.figure(u\"Top-5 预测结果\") \n",
    "\n",
    "with tf.Session() as sess:\n",
    "    images = tf.placeholder(tf.float32, [1, 224, 224, 3])\n",
    "    vgg = Vgg16() \n",
    "    vgg.net(images) ##既然有dict清空操作，第二遍会怎样？当然是完蛋了！\n",
    "    probability = sess.run(vgg.prob, feed_dict={images:img_ready})\n",
    "    print(type(probablity))\n",
    "    print(len(probablity))\n",
    "    top5 = np.argsort(probability[0])[-1:-6:-1]#probablity取[0]是取第一个数据，实际也只有一个数据\n",
    "    print(\"top5:\",top5)#排名前五的分类下标\n",
    "    values = []\n",
    "    bar_label = []\n",
    "    for n, i in enumerate(top5): #i是分类下标\n",
    "        print(\"n:\",n)\n",
    "        print(\"i:\",i)\n",
    "        values.append(probability[0][i]) #取这些概率值\n",
    "        bar_label.append(labels[i]) #添加对应的真正标签字符串\n",
    "        print( i, \":\", labels[i], \"----\", percent(probability[0][i]) )\n",
    "        \n",
    "    ax = fig.add_subplot(111) \n",
    "    ax.bar(range(len(values)), values, tick_label=bar_label, width=0.5, fc='g')\n",
    "    ax.set_ylabel(u'probabilityit') \n",
    "    ax.set_title(u'Top-5') \n",
    "    for a,b in zip(range(len(values)), values):\n",
    "        ax.text(a, b+0.0005, percent(b), ha='center', va = 'bottom', fontsize=7)   \n",
    "    plt.show() \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/home/qw/Documents/tf_demo'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 测试区，测试代码"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RGB\n"
     ]
    }
   ],
   "source": [
    "#ndarray可没有.mode\n",
    "#验证一下图片模式是否RGB\n",
    "from PIL import Image\n",
    "img = Image.open('pic/0.jpg')\n",
    "print(img.mode)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[0m\u001b[01;32mvgg.npy\u001b[0m*\r\n"
     ]
    }
   ],
   "source": [
    "ls '../../model_saved'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'dict'>\n",
      "16\n",
      "<class 'list'>\n",
      "2\n",
      "<class 'numpy.ndarray'>\n",
      "(3, 3, 512, 512)\n",
      "(512,)\n",
      "biases: <class 'numpy.ndarray'>\n",
      "biases: (512,)\n",
      "(3, 512, 512)\n",
      "(3, 512, 512)\n",
      "(25088, 4096)\n",
      "(4096,)\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "vgg16_path = os.path.join(os.getcwd(), '../../model_saved/vgg.npy')#相对路径\n",
    "# data = np.load(vgg16_path, encoding='latin1')\n",
    "# print(type(data))#<class 'numpy.ndarray'>\n",
    "# print(data.shape)#()\n",
    "# print(data.size)#1\n",
    "\n",
    "#item()迭代，读出所有变量\n",
    "#以字典形式存在，每个层名对应两个list，weights和biases，conv和fc全是如此。\n",
    "\n",
    "data_dict = np.load(vgg16_path, encoding='latin1').item()\n",
    "print(type(data_dict))#<class 'dict'>\n",
    "print(len(data_dict))#16\n",
    "print(type(data_dict['conv5_1']))#16\n",
    "print(len(data_dict['conv5_1']))#每个key对应的value是两个数组，一个weights，一个biases\n",
    "print(type(data_dict['conv5_1'][0]))\n",
    "# print(len(data_dict['conv5_1'][0]))#len=3，不是3个，看下边的shape比较清晰。\n",
    "print(data_dict['conv5_1'][0].shape)#3,3,512,512，出入通道512,卷积核3*3\n",
    "print(data_dict['conv5_1'][1].shape)#(512,)\n",
    "print('biases:',type(data_dict['conv5_1'][1]))\n",
    "# print('biases:',len(data_dict['conv5_1'][1]))#512\n",
    "print('biases:',data_dict['conv5_1'][1].shape)#512\n",
    "print((data_dict['conv5_1'][0][0].shape))\n",
    "print((data_dict['conv5_1'][0][1].shape))\n",
    "print(data_dict['fc6'][0].shape)#\n",
    "print(data_dict['fc6'][1].shape)#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on getset descriptor numpy.ndarray.shape:\n",
      "\n",
      "shape\n",
      "    Tuple of array dimensions.\n",
      "    \n",
      "    Notes\n",
      "    -----\n",
      "    May be used to \"reshape\" the array, as long as this would not\n",
      "    require a change in the total number of elements\n",
      "    \n",
      "    Examples\n",
      "    --------\n",
      "    >>> x = np.array([1, 2, 3, 4])\n",
      "    >>> x.shape\n",
      "    (4,)\n",
      "    >>> y = np.zeros((2, 3, 4))\n",
      "    >>> y.shape\n",
      "    (2, 3, 4)\n",
      "    >>> y.shape = (3, 8)\n",
      "    >>> y\n",
      "    array([[ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.],\n",
      "           [ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.],\n",
      "           [ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.]])\n",
      "    >>> y.shape = (3, 6)\n",
      "    Traceback (most recent call last):\n",
      "      File \"<stdin>\", line 1, in <module>\n",
      "    ValueError: total size of new array must be unchanged\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(np.ndarray.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 1.  2.  2.]\n",
      " [ 2.  3.  4.]]\n",
      "[[ 0.15536241  0.42231882  0.42231882]\n",
      " [ 0.09003057  0.24472848  0.66524094]]\n",
      "[ 0.15536241  0.42231882  0.42231882]\n"
     ]
    }
   ],
   "source": [
    "#softmax输出不是二维，什么输出是需要用[0]取一下来着？\n",
    "#本例softmax有一个[0]操作，是因为batch需要占一个维度，虽然只有一个图像，那个维度还是要有。\n",
    "import tensorflow as tf\n",
    "# help(tf.nn.softmax)\n",
    "a = tf.Variable([[1.,2.,2.],[2.,3.,4.]])\n",
    "soft = tf.nn.softmax(a)\n",
    "with tf.Session() as sess:\n",
    "    tf.global_variables_initializer().run()\n",
    "    print(sess.run(a))\n",
    "    print(sess.run(soft))\n",
    "    print(sess.run(soft)[0])#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 附件"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "virtual py36",
   "language": "python",
   "name": "py36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
