{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#import add_layer\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#定义一个典型的层，层的内部随便嵌套加一些scope。\n",
    "#scope被自动重命名：layer/weights_1              layer/weights_2\n",
    "#variable自动重命名：layer/weights_1/W2_1 ayer/weights_1/W2_2\n",
    "def add_layer(inputs, in_size, out_size, activation_function=None):\n",
    "        with tf.name_scope('layer'):\n",
    "                with tf.name_scope('weights'):\n",
    "                        Weights = tf.Variable(tf.random_normal([in_size,out_size]),name='W')\n",
    "                with tf.name_scope('weights'):\n",
    "                        Weights2 = tf.Variable(tf.random_normal([in_size,out_size]),name='W2')\n",
    "                        Weights2 = tf.Variable(tf.random_normal([in_size,out_size]),name='W2')\n",
    "                with tf.variable_scope('weights'):\n",
    "                        Weights22 = tf.Variable(tf.random_normal([in_size,out_size]),name='W_variable2')\n",
    "                with tf.name_scope('biases'):\n",
    "                        biases = tf.Variable(tf.zeros([1,out_size]) + 0.1,name='b')\n",
    "                with tf.name_scope('Wx_plus_b'):\n",
    "                        Wx_plus_b = tf.add(tf.matmul(inputs,Weights), biases)\n",
    "                if activation_function is None:\n",
    "                        outputs = Wx_plus_b\n",
    "                else:\n",
    "                        outputs = activation_function(Wx_plus_b)\n",
    "                return outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#生成数据\n",
    "x_data = np.linspace(-1,1,300)[:,np.newaxis]\n",
    "noise = np.random.normal(0,0.05,x_data.shape)\n",
    "y_data = np.square(x_data) - 0.5 + noise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#定义网络\n",
    "with tf.name_scope('inputs'):\n",
    "        xs = tf.placeholder(tf.float32,[None,1],name='x_input')\n",
    "        ys = tf.placeholder(tf.float32,[None,1],name='y_input')\n",
    "\n",
    "#l1 = add_layer(x_data, 1, 10, activation_function = tf.nn.relu)#without phdr\n",
    "l1 = add_layer(xs, 1, 10, activation_function = tf.nn.relu)\n",
    "prediction = add_layer(l1,10,1,activation_function = None)\n",
    "with tf.name_scope('loss'):\n",
    "        loss = tf.reduce_mean(tf.reduce_sum(\n",
    "                        tf.square(ys - prediction),reduction_indices=[1]))\n",
    "        #tf.square(y_data - prediction),reduction_indices=[1]))#without phdr\n",
    "#optimizer = GradientDescent\n",
    "#train = optimizer.minimize()\n",
    "with tf.name_scope('train'):\n",
    "        train_step = tf.train.GradientDescentOptimizer(0.5).minimize(loss)\n",
    "init = tf.global_variables_initializer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5.98005\n",
      "0.0916592\n",
      "0.0916592\n",
      "0.0916592\n",
      "0.0916592\n",
      "0.0916592\n",
      "0.0916592\n",
      "0.0916592\n",
      "0.0916592\n",
      "0.0916592\n",
      "0.0916592\n",
      "0.0916592\n",
      "0.0916592\n",
      "0.0916592\n",
      "0.0916592\n",
      "0.0916592\n",
      "0.0916592\n",
      "0.0916592\n",
      "0.0916592\n",
      "0.0916592\n"
     ]
    }
   ],
   "source": [
    "with tf.Session() as sess:\n",
    "#本例其实返回值没什么用，打印graph也不用单独去run一下writer。只要声明了FileWriter对象就有了图。\n",
    "        writer = tf.summary.FileWriter(\"logs/\",sess.graph)                \n",
    "        writer2 = tf.summary.FileWriter(\"logs2/\",sess.graph) \n",
    "        sess.run(init)\n",
    "        for i in range(1000):\n",
    "            sess.run(train_step,{xs:x_data, ys:y_data})\n",
    "#placeholder is for sake of mini batch\n",
    "            if i % 50 == 0:\n",
    "                print(sess.run(loss,{xs:x_data,ys:y_data}))\n",
    "                prediction_value = sess.run(prediction,{xs:x_data})\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on class FileWriter in module tensorflow.python.summary.writer.writer:\n",
      "\n",
      "class FileWriter(SummaryToEventTransformer)\n",
      " |  Writes `Summary` protocol buffers to event files.\n",
      " |  \n",
      " |  The `FileWriter` class provides a mechanism to create an event file in a\n",
      " |  given directory and add summaries and events to it. The class updates the\n",
      " |  file contents asynchronously. This allows a training program to call methods\n",
      " |  to add data to the file directly from the training loop, without slowing down\n",
      " |  training.\n",
      " |  \n",
      " |  Method resolution order:\n",
      " |      FileWriter\n",
      " |      SummaryToEventTransformer\n",
      " |      builtins.object\n",
      " |  \n",
      " |  Methods defined here:\n",
      " |  \n",
      " |  __init__(self, logdir, graph=None, max_queue=10, flush_secs=120, graph_def=None, filename_suffix=None)\n",
      " |      Creates a `FileWriter` and an event file.\n",
      " |      \n",
      " |      On construction the summary writer creates a new event file in `logdir`.\n",
      " |      This event file will contain `Event` protocol buffers constructed when you\n",
      " |      call one of the following functions: `add_summary()`, `add_session_log()`,\n",
      " |      `add_event()`, or `add_graph()`.\n",
      " |      \n",
      " |      If you pass a `Graph` to the constructor it is added to\n",
      " |      the event file. (This is equivalent to calling `add_graph()` later).\n",
      " |      \n",
      " |      TensorBoard will pick the graph from the file and display it graphically so\n",
      " |      you can interactively explore the graph you built. You will usually pass\n",
      " |      the graph from the session in which you launched it:\n",
      " |      \n",
      " |      ```python\n",
      " |      ...create a graph...\n",
      " |      # Launch the graph in a session.\n",
      " |      sess = tf.Session()\n",
      " |      # Create a summary writer, add the 'graph' to the event file.\n",
      " |      writer = tf.summary.FileWriter(<some-directory>, sess.graph)\n",
      " |      ```\n",
      " |      \n",
      " |      The other arguments to the constructor control the asynchronous writes to\n",
      " |      the event file:\n",
      " |      \n",
      " |      *  `flush_secs`: How often, in seconds, to flush the added summaries\n",
      " |         and events to disk.\n",
      " |      *  `max_queue`: Maximum number of summaries or events pending to be\n",
      " |         written to disk before one of the 'add' calls block.\n",
      " |      \n",
      " |      Args:\n",
      " |        logdir: A string. Directory where event file will be written.\n",
      " |        graph: A `Graph` object, such as `sess.graph`.\n",
      " |        max_queue: Integer. Size of the queue for pending events and summaries.\n",
      " |        flush_secs: Number. How often, in seconds, to flush the\n",
      " |          pending events and summaries to disk.\n",
      " |        graph_def: DEPRECATED: Use the `graph` argument instead.\n",
      " |        filename_suffix: A string. Every event file's name is suffixed with\n",
      " |          `suffix`.\n",
      " |  \n",
      " |  add_event(self, event)\n",
      " |      Adds an event to the event file.\n",
      " |      \n",
      " |      Args:\n",
      " |        event: An `Event` protocol buffer.\n",
      " |  \n",
      " |  close(self)\n",
      " |      Flushes the event file to disk and close the file.\n",
      " |      \n",
      " |      Call this method when you do not need the summary writer anymore.\n",
      " |  \n",
      " |  flush(self)\n",
      " |      Flushes the event file to disk.\n",
      " |      \n",
      " |      Call this method to make sure that all pending events have been written to\n",
      " |      disk.\n",
      " |  \n",
      " |  get_logdir(self)\n",
      " |      Returns the directory where event file will be written.\n",
      " |  \n",
      " |  reopen(self)\n",
      " |      Reopens the EventFileWriter.\n",
      " |      \n",
      " |      Can be called after `close()` to add more events in the same directory.\n",
      " |      The events will go into a new events file.\n",
      " |      \n",
      " |      Does nothing if the EventFileWriter was not closed.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from SummaryToEventTransformer:\n",
      " |  \n",
      " |  add_graph(self, graph, global_step=None, graph_def=None)\n",
      " |      Adds a `Graph` to the event file.\n",
      " |      \n",
      " |      The graph described by the protocol buffer will be displayed by\n",
      " |      TensorBoard. Most users pass a graph in the constructor instead.\n",
      " |      \n",
      " |      Args:\n",
      " |        graph: A `Graph` object, such as `sess.graph`.\n",
      " |        global_step: Number. Optional global step counter to record with the\n",
      " |          graph.\n",
      " |        graph_def: DEPRECATED. Use the `graph` parameter instead.\n",
      " |      \n",
      " |      Raises:\n",
      " |        ValueError: If both graph and graph_def are passed to the method.\n",
      " |  \n",
      " |  add_meta_graph(self, meta_graph_def, global_step=None)\n",
      " |      Adds a `MetaGraphDef` to the event file.\n",
      " |      \n",
      " |      The `MetaGraphDef` allows running the given graph via\n",
      " |      `saver.import_meta_graph()`.\n",
      " |      \n",
      " |      Args:\n",
      " |        meta_graph_def: A `MetaGraphDef` object, often as returned by\n",
      " |          `saver.export_meta_graph()`.\n",
      " |        global_step: Number. Optional global step counter to record with the\n",
      " |          graph.\n",
      " |      \n",
      " |      Raises:\n",
      " |        TypeError: If both `meta_graph_def` is not an instance of `MetaGraphDef`.\n",
      " |  \n",
      " |  add_run_metadata(self, run_metadata, tag, global_step=None)\n",
      " |      Adds a metadata information for a single session.run() call.\n",
      " |      \n",
      " |      Args:\n",
      " |        run_metadata: A `RunMetadata` protobuf object.\n",
      " |        tag: The tag name for this metadata.\n",
      " |        global_step: Number. Optional global step counter to record with the\n",
      " |          StepStats.\n",
      " |      \n",
      " |      Raises:\n",
      " |        ValueError: If the provided tag was already used for this type of event.\n",
      " |  \n",
      " |  add_session_log(self, session_log, global_step=None)\n",
      " |      Adds a `SessionLog` protocol buffer to the event file.\n",
      " |      \n",
      " |      This method wraps the provided session in an `Event` protocol buffer\n",
      " |      and adds it to the event file.\n",
      " |      \n",
      " |      Args:\n",
      " |        session_log: A `SessionLog` protocol buffer.\n",
      " |        global_step: Number. Optional global step value to record with the\n",
      " |          summary.\n",
      " |  \n",
      " |  add_summary(self, summary, global_step=None)\n",
      " |      Adds a `Summary` protocol buffer to the event file.\n",
      " |      \n",
      " |      This method wraps the provided summary in an `Event` protocol buffer\n",
      " |      and adds it to the event file.\n",
      " |      \n",
      " |      You can pass the result of evaluating any summary op, using\n",
      " |      @{tf.Session.run} or\n",
      " |      @{tf.Tensor.eval}, to this\n",
      " |      function. Alternatively, you can pass a `tf.Summary` protocol\n",
      " |      buffer that you populate with your own data. The latter is\n",
      " |      commonly done to report evaluation results in event files.\n",
      " |      \n",
      " |      Args:\n",
      " |        summary: A `Summary` protocol buffer, optionally serialized as a string.\n",
      " |        global_step: Number. Optional global step value to record with the\n",
      " |          summary.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data descriptors inherited from SummaryToEventTransformer:\n",
      " |  \n",
      " |  __dict__\n",
      " |      dictionary for instance variables (if defined)\n",
      " |  \n",
      " |  __weakref__\n",
      " |      list of weak references to the object (if defined)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(tf.summary.FileWriter)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "virtual py36",
   "language": "python",
   "name": "py36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
