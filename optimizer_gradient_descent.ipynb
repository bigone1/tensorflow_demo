{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## demo1:不用x，直接train变量。label是1,predition是3,需要把predition变成1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 0  variable is  3.0  and the loss is  4.0\n",
      "epoch: 100  variable is  1.26524  and the loss is  0.0703518\n",
      "epoch: 200  variable is  1.03518  and the loss is  0.00123735\n",
      "epoch: 300  variable is  1.00466  and the loss is  2.17602e-05\n",
      "epoch: 400  variable is  1.00062  and the loss is  3.82637e-07\n",
      "epoch: 500  variable is  1.00008  and the loss is  6.72662e-09\n",
      "epoch: 600  variable is  1.00001  and the loss is  1.20281e-10\n",
      "epoch: 700  variable is  1.0  and the loss is  8.18545e-12\n",
      "epoch: 800  variable is  1.0  and the loss is  8.18545e-12\n",
      "epoch: 900  variable is  1.0  and the loss is  8.18545e-12\n"
     ]
    }
   ],
   "source": [
    "label = tf.constant(1,dtype = tf.float32)\n",
    "prediction = tf.Variable(3,dtype=tf.float32)\n",
    "\n",
    "#define losses and train,手动定义一个均方误差\n",
    "mse_loss = tf.square(prediction - label)\n",
    "optimizer = tf.train.GradientDescentOptimizer(0.01)\n",
    "train_step = optimizer.minimize(mse_loss)\n",
    "\n",
    "init = tf.global_variables_initializer()\n",
    "with tf.Session() as sess:\n",
    "    sess.run(init)\n",
    "    for epoch in range(1000):\n",
    "        if epoch % 100 == 0:\n",
    "            print('epoch:',epoch,' variable is ', sess.run(prediction), ' and the loss is ',sess.run(mse_loss))\n",
    "        sess.run(train_step)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## var_list的使用：\n",
    "选定变量去训练，虽然y由三个w影响，var_list只指定w2,只训练w2,其他当constant用。\n",
    "x的输入限定为1,label固定为1,y预测是w1+w2+w3，w2应该被训练成-7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "variable is w1: 4.0  w2: 4.0  w3: 4.0  and the loss is  1331.0\n",
      "variable is w1: 4.0  w2: -6.68793  w3: 4.0  and the loss is  0.030393\n",
      "variable is w1: 4.0  w2: -6.83934  w3: 4.0  and the loss is  0.00414684\n",
      "variable is w1: 4.0  w2: -6.89173  w3: 4.0  and the loss is  0.00126916\n",
      "variable is w1: 4.0  w2: -6.91833  w3: 4.0  and the loss is  0.000544715\n",
      "variable is w1: 4.0  w2: -6.93443  w3: 4.0  and the loss is  0.000281887\n",
      "variable is w1: 4.0  w2: -6.94522  w3: 4.0  and the loss is  0.000164352\n",
      "variable is w1: 4.0  w2: -6.95297  w3: 4.0  and the loss is  0.00010405\n",
      "variable is w1: 4.0  w2: -6.95879  w3: 4.0  and the loss is  6.99961e-05\n",
      "variable is w1: 4.0  w2: -6.96333  w3: 4.0  and the loss is  4.93202e-05\n"
     ]
    }
   ],
   "source": [
    "label = tf.constant(1,dtype = tf.float32)\n",
    "x = tf.placeholder(dtype = tf.float32)\n",
    "w1 = tf.Variable(4,dtype=tf.float32)\n",
    "w2 = tf.Variable(4,dtype=tf.float32)\n",
    "w3 = tf.constant(4,dtype=tf.float32)\n",
    "\n",
    "y_predict = w1*x+w2*x+w3*x\n",
    "\n",
    "#define losses and train\n",
    "#make_up_loss = tf.square(y_predict - label)\n",
    "#make_up_loss = (y_predict - label)**2\n",
    "make_up_loss = (y_predict - label)**3\n",
    "optimizer = tf.train.GradientDescentOptimizer(0.01)\n",
    "train_step = optimizer.minimize(make_up_loss,var_list = w2)\n",
    "\n",
    "init = tf.global_variables_initializer()\n",
    "with tf.Session() as sess:\n",
    "    sess.run(init)\n",
    "    for epoch in range(1000):\n",
    "        if epoch % 100 == 0:\n",
    "            w1_,w2_,w3_,loss_ = sess.run([w1,w2,w3,make_up_loss],feed_dict={x:1})\n",
    "            print('variable is w1:',w1_,' w2:',w2_,' w3:',w3_, ' and the loss is ',loss_)\n",
    "        sess.run(train_step,{x:1})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 利用tf.get_collection和scope来获取变量\n",
    "具体的公式一样，期望训练结果一样"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "variable is w1: 4.0  w2: 4.0  w3: 4.0  and the loss is  1331.0\n",
      "variable is w1: 4.0  w2: -6.68793  w3: 4.0  and the loss is  0.030393\n",
      "variable is w1: 4.0  w2: -6.83934  w3: 4.0  and the loss is  0.00414684\n",
      "variable is w1: 4.0  w2: -6.89173  w3: 4.0  and the loss is  0.00126916\n",
      "variable is w1: 4.0  w2: -6.91833  w3: 4.0  and the loss is  0.000544715\n",
      "variable is w1: 4.0  w2: -6.93443  w3: 4.0  and the loss is  0.000281887\n",
      "variable is w1: 4.0  w2: -6.94522  w3: 4.0  and the loss is  0.000164352\n",
      "variable is w1: 4.0  w2: -6.95297  w3: 4.0  and the loss is  0.00010405\n",
      "variable is w1: 4.0  w2: -6.95879  w3: 4.0  and the loss is  6.99961e-05\n",
      "variable is w1: 4.0  w2: -6.96333  w3: 4.0  and the loss is  4.93202e-05\n"
     ]
    }
   ],
   "source": [
    "label = tf.constant(1,dtype = tf.float32)\n",
    "x = tf.placeholder(dtype = tf.float32)\n",
    "w1 = tf.Variable(4,dtype=tf.float32)\n",
    "with tf.name_scope(name='selected_variable_to_trian'):\n",
    "    w2 = tf.Variable(4,dtype=tf.float32)\n",
    "w3 = tf.constant(4,dtype=tf.float32)\n",
    "\n",
    "y_predict = w1*x+w2*x+w3*x\n",
    "\n",
    "#define losses and train\n",
    "make_up_loss = (y_predict - label)**3\n",
    "optimizer = tf.train.GradientDescentOptimizer(0.01)\n",
    "\n",
    "output_vars = tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES, scope='selected_variable_to_trian')\n",
    "train_step = optimizer.minimize(make_up_loss,var_list = output_vars)\n",
    "\n",
    "init = tf.global_variables_initializer()\n",
    "with tf.Session() as sess:\n",
    "    sess.run(init)\n",
    "    for epoch in range(1000):\n",
    "        if epoch % 100 == 0:\n",
    "            w1_,w2_,w3_,loss_ = sess.run([w1,w2,w3,make_up_loss],feed_dict={x:1})\n",
    "            print('variable is w1:',w1_,' w2:',w2_,' w3:',w3_, ' and the loss is ',loss_)\n",
    "        sess.run(train_step,{x:1})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## trainable参数，var_list之外的限定训练的方法\n",
    "用trainable=False限制w1,var_list不指定\n",
    "用constant限制w3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "variable is w1: 4.0  w2: 4.0  w3: 4.0  and the loss is  1331.0\n",
      "variable is w1: 4.0  w2: -6.68793  w3: 4.0  and the loss is  0.030393\n",
      "variable is w1: 4.0  w2: -6.83934  w3: 4.0  and the loss is  0.00414684\n",
      "variable is w1: 4.0  w2: -6.89173  w3: 4.0  and the loss is  0.00126916\n",
      "variable is w1: 4.0  w2: -6.91833  w3: 4.0  and the loss is  0.000544715\n",
      "variable is w1: 4.0  w2: -6.93443  w3: 4.0  and the loss is  0.000281887\n",
      "variable is w1: 4.0  w2: -6.94522  w3: 4.0  and the loss is  0.000164352\n",
      "variable is w1: 4.0  w2: -6.95297  w3: 4.0  and the loss is  0.00010405\n",
      "variable is w1: 4.0  w2: -6.95879  w3: 4.0  and the loss is  6.99961e-05\n",
      "variable is w1: 4.0  w2: -6.96333  w3: 4.0  and the loss is  4.93202e-05\n"
     ]
    }
   ],
   "source": [
    "label = tf.constant(1,dtype = tf.float32)\n",
    "x = tf.placeholder(dtype = tf.float32)\n",
    "w1 = tf.Variable(4,dtype=tf.float32,trainable=False)\n",
    "w2 = tf.Variable(4,dtype=tf.float32)\n",
    "w3 = tf.constant(4,dtype=tf.float32)\n",
    "\n",
    "y_predict = w1*x+w2*x+w3*x\n",
    "\n",
    "#define losses and train\n",
    "make_up_loss = (y_predict - label)**3\n",
    "optimizer = tf.train.GradientDescentOptimizer(0.01)\n",
    "\n",
    "train_step = optimizer.minimize(make_up_loss)\n",
    "\n",
    "init = tf.global_variables_initializer()\n",
    "with tf.Session() as sess:\n",
    "    sess.run(init)\n",
    "    for epoch in range(1000):\n",
    "        if epoch % 100 == 0:\n",
    "            w1_,w2_,w3_,loss_ = sess.run([w1,w2,w3,make_up_loss],feed_dict={x:1})\n",
    "            print('variable is w1:',w1_,' w2:',w2_,' w3:',w3_, ' and the loss is ',loss_)\n",
    "        sess.run(train_step,{x:1})\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 另一种获取variable列表的方式，tf.GraphKeys.TRAINABLE_VARIABLES\n",
    "配合trainable=False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "variable is w1: 4.0  w2: 4.0  w3: 4.0  and the loss is  1331.0\n",
      "variable is w1: 4.0  w2: -6.68793  w3: 4.0  and the loss is  0.030393\n",
      "variable is w1: 4.0  w2: -6.83934  w3: 4.0  and the loss is  0.00414684\n",
      "variable is w1: 4.0  w2: -6.89173  w3: 4.0  and the loss is  0.00126916\n",
      "variable is w1: 4.0  w2: -6.91833  w3: 4.0  and the loss is  0.000544715\n",
      "variable is w1: 4.0  w2: -6.93443  w3: 4.0  and the loss is  0.000281887\n",
      "variable is w1: 4.0  w2: -6.94522  w3: 4.0  and the loss is  0.000164352\n",
      "variable is w1: 4.0  w2: -6.95297  w3: 4.0  and the loss is  0.00010405\n",
      "variable is w1: 4.0  w2: -6.95879  w3: 4.0  and the loss is  6.99961e-05\n",
      "variable is w1: 4.0  w2: -6.96333  w3: 4.0  and the loss is  4.93202e-05\n"
     ]
    }
   ],
   "source": [
    "label = tf.constant(1,dtype = tf.float32)\n",
    "x = tf.placeholder(dtype = tf.float32)\n",
    "w1 = tf.Variable(4,dtype=tf.float32,trainable=False)\n",
    "w2 = tf.Variable(4,dtype=tf.float32)\n",
    "w3 = tf.constant(4,dtype=tf.float32)\n",
    "\n",
    "y_predict = w1*x+w2*x+w3*x\n",
    "\n",
    "#define losses and train\n",
    "make_up_loss = (y_predict - label)**3\n",
    "optimizer = tf.train.GradientDescentOptimizer(0.01)\n",
    "\n",
    "output_vars = tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES)\n",
    "train_step = optimizer.minimize(make_up_loss,var_list = output_vars)\n",
    "\n",
    "init = tf.global_variables_initializer()\n",
    "with tf.Session() as sess:\n",
    "    sess.run(init)\n",
    "    for epoch in range(1000):\n",
    "        if epoch % 100 == 0:\n",
    "            w1_,w2_,w3_,loss_ = sess.run([w1,w2,w3,make_up_loss],feed_dict={x:1})\n",
    "            print('variable is w1:',w1_,' w2:',w2_,' w3:',w3_, ' and the loss is ',loss_)\n",
    "        sess.run(train_step,{x:1})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 分解：计算梯度和应用梯度更新变量两步走"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "variable is w1: 4.0  w2: 4.0  w3: 4.0  and the loss is  1331.0\n",
      "gradient: [(363.0, 4.0)]\n",
      "variable is w1: 4.0  w2: -6.68793  w3: 4.0  and the loss is  0.030393\n",
      "gradient: [(0.29217088, -6.6879258)]\n",
      "variable is w1: 4.0  w2: -6.83934  w3: 4.0  and the loss is  0.00414684\n",
      "gradient: [(0.077434242, -6.8393407)]\n",
      "variable is w1: 4.0  w2: -6.89173  w3: 4.0  and the loss is  0.00126916\n",
      "gradient: [(0.03516667, -6.8917308)]\n",
      "variable is w1: 4.0  w2: -6.91833  w3: 4.0  and the loss is  0.000544715\n",
      "gradient: [(0.020009406, -6.9183311)]\n",
      "variable is w1: 4.0  w2: -6.93443  w3: 4.0  and the loss is  0.000281887\n",
      "gradient: [(0.012897477, -6.934432)]\n",
      "variable is w1: 4.0  w2: -6.94522  w3: 4.0  and the loss is  0.000164352\n",
      "gradient: [(0.0090012942, -6.9452238)]\n",
      "variable is w1: 4.0  w2: -6.95297  w3: 4.0  and the loss is  0.00010405\n",
      "gradient: [(0.0066366661, -6.9529657)]\n",
      "variable is w1: 4.0  w2: -6.95879  w3: 4.0  and the loss is  6.99961e-05\n",
      "gradient: [(0.0050953072, -6.9587879)]\n",
      "variable is w1: 4.0  w2: -6.96333  w3: 4.0  and the loss is  4.93202e-05\n",
      "gradient: [(0.004034637, -6.9633274)]\n"
     ]
    }
   ],
   "source": [
    "label = tf.constant(1,dtype = tf.float32)\n",
    "x = tf.placeholder(dtype = tf.float32)\n",
    "w1 = tf.Variable(4,dtype=tf.float32,trainable=False)\n",
    "w2 = tf.Variable(4,dtype=tf.float32)\n",
    "w3 = tf.Variable(4,dtype=tf.float32)\n",
    "\n",
    "y_predict = w1*x+w2*x+w3*x\n",
    "\n",
    "#define losses and train\n",
    "make_up_loss = (y_predict - label)**3\n",
    "optimizer = tf.train.GradientDescentOptimizer(0.01)\n",
    "\n",
    "w2_gradient = optimizer.compute_gradients(loss = make_up_loss, var_list = w2)\n",
    "train_step = optimizer.apply_gradients(grads_and_vars = (w2_gradient))\n",
    "\n",
    "init = tf.global_variables_initializer()\n",
    "with tf.Session() as sess:\n",
    "    sess.run(init)\n",
    "    for epoch in range(1000):\n",
    "        if epoch % 100 == 0:\n",
    "            w1_,w2_,w3_,loss_,w2_gradient_ = sess.run([w1,w2,w3,make_up_loss,w2_gradient],feed_dict={x:1})\n",
    "            print('variable is w1:',w1_,' w2:',w2_,' w3:',w3_, ' and the loss is ',loss_)\n",
    "            print('gradient:',w2_gradient_)\n",
    "        sess.run(train_step,{x:1})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 各种loss公式测试"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "variable is w1: 4.0  w2: 4.0  w3: 4.0  and the loss is  121.0\n",
      "variable is w1: 4.0  w2: -5.54118  w3: 4.0  and the loss is  2.12814\n",
      "variable is w1: 4.0  w2: -6.80653  w3: 4.0  and the loss is  0.0374299\n",
      "variable is w1: 4.0  w2: -6.97434  w3: 4.0  and the loss is  0.000658266\n",
      "variable is w1: 4.0  w2: -6.9966  w3: 4.0  and the loss is  1.15817e-05\n",
      "variable is w1: 4.0  w2: -6.99955  w3: 4.0  and the loss is  2.03911e-07\n",
      "variable is w1: 4.0  w2: -6.99994  w3: 4.0  and the loss is  3.4961e-09\n",
      "variable is w1: 4.0  w2: -6.99999  w3: 4.0  and the loss is  1.30967e-10\n",
      "variable is w1: 4.0  w2: -6.99999  w3: 4.0  and the loss is  1.30967e-10\n",
      "variable is w1: 4.0  w2: -6.99999  w3: 4.0  and the loss is  1.30967e-10\n"
     ]
    }
   ],
   "source": [
    "label = tf.constant(1,dtype = tf.float32)\n",
    "x = tf.placeholder(dtype = tf.float32)\n",
    "w1 = tf.Variable(4,dtype=tf.float32)\n",
    "w2 = tf.Variable(4,dtype=tf.float32)\n",
    "w3 = tf.constant(4,dtype=tf.float32)\n",
    "\n",
    "y_predict = w1*x+w2*x+w3*x\n",
    "\n",
    "#define losses and train\n",
    "#make_up_loss = tf.losses.sigmoid_cross_entropy(y_predict,label)\n",
    "make_up_loss = tf.losses.mean_squared_error(y_predict,label)\n",
    "#make_up_loss = (y_predict - label)**2\n",
    "#make_up_loss = (y_predict - label)**3\n",
    "optimizer = tf.train.GradientDescentOptimizer(0.01)\n",
    "train_step = optimizer.minimize(make_up_loss,var_list = w2)\n",
    "\n",
    "init = tf.global_variables_initializer()\n",
    "with tf.Session() as sess:\n",
    "    sess.run(init)\n",
    "    for epoch in range(1000):\n",
    "        if epoch % 100 == 0:\n",
    "            w1_,w2_,w3_,loss_ = sess.run([w1,w2,w3,make_up_loss],feed_dict={x:1})\n",
    "            print('variable is w1:',w1_,' w2:',w2_,' w3:',w3_, ' and the loss is ',loss_)\n",
    "        sess.run(train_step,{x:1})\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 手动梯度下降：手动计算g并且手动更新w。\n",
    "y是3,x是placeholder，w是变量，p是w*x。\n",
    "### 错误示范：如果学习率过大，不收敛，如果学习率是1,刚好固定两个值来回跳"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-2.0], 2.0, 2.0]\n",
      "variable is w: 2.0  g is  [-2.0]   and the loss is  1.0\n",
      "variable is w: 4.0  g is  [2.0]   and the loss is  1.0\n",
      "variable is w: 2.0  g is  [-2.0]   and the loss is  1.0\n",
      "variable is w: 4.0  g is  [2.0]   and the loss is  1.0\n",
      "variable is w: 2.0  g is  [-2.0]   and the loss is  1.0\n",
      "variable is w: 4.0  g is  [2.0]   and the loss is  1.0\n",
      "variable is w: 2.0  g is  [-2.0]   and the loss is  1.0\n",
      "variable is w: 4.0  g is  [2.0]   and the loss is  1.0\n",
      "variable is w: 2.0  g is  [-2.0]   and the loss is  1.0\n",
      "variable is w: 4.0  g is  [2.0]   and the loss is  1.0\n"
     ]
    }
   ],
   "source": [
    "y = tf.constant(3,dtype = tf.float32)\n",
    "x = tf.placeholder(dtype = tf.float32)\n",
    "w = tf.Variable(2,dtype=tf.float32)\n",
    "#prediction\n",
    "p = w*x\n",
    "\n",
    "#define losses\n",
    "l = tf.square(p - y)\n",
    "g = tf.gradients(l, w)\n",
    "learning_rate = tf.constant(1,dtype=tf.float32)\n",
    "init = tf.global_variables_initializer()\n",
    "\n",
    "#update\n",
    "update = tf.assign(w, w - learning_rate * g[0])\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    sess.run(init)\n",
    "    print(sess.run([g,p,w], {x: 1}))\n",
    "    for epoch in range(10):\n",
    "        if epoch % 1 == 0:\n",
    "            w_,g_,l_ = sess.run([w,g,l],feed_dict={x:1})\n",
    "            print('variable is w:',w_, ' g is ',g_,'  and the loss is ',l_)\n",
    "\n",
    "        _ = sess.run(update,feed_dict={x:1})\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 利用python的接口进行梯度下降\n",
    "todo：还有点问题，接口变了？"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "max() arg is an empty sequence",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-12-3e8ee9092a41>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0mmake_up_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msquare\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_predict\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mlabel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mSymbol\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'loss'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m \u001b[0mg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgradient\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmake_up_loss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     13\u001b[0m \u001b[0mg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdiff\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmake_up_loss\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'g is '\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/qw/anaconda3/envs/py36/lib/python3.6/site-packages/numpy/lib/function_base.py\u001b[0m in \u001b[0;36mgradient\u001b[0;34m(f, *varargs, **kwargs)\u001b[0m\n\u001b[1;32m   1579\u001b[0m     \u001b[0;31m# normalize axis values:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1580\u001b[0m     \u001b[0maxes\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtuple\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mN\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mx\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0;36m0\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0maxes\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1581\u001b[0;31m     \u001b[0;32mif\u001b[0m \u001b[0mmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maxes\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>=\u001b[0m \u001b[0mN\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mmin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maxes\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1582\u001b[0m         \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"'axis' entry is out of bounds\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1583\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: max() arg is an empty sequence"
     ]
    }
   ],
   "source": [
    "from sympy import *\n",
    "import numpy as np\n",
    "label = 1\n",
    "x = 1\n",
    "w = 4\n",
    "learning_rate = 1\n",
    "y_predict = w*x\n",
    "\n",
    "#define losses and train\n",
    "make_up_loss = tf.square(y_predict - label)\n",
    "loss = Symbol('loss')\n",
    "g = np.gradient(make_up_loss, w)\n",
    "g = diff(make_up_loss,w)\n",
    "print('g is ',g)\n",
    "#update\n",
    "def update():\n",
    "    w = w - learning_rate * g * w\n",
    "\n",
    "for _ in range(5):\n",
    "    update()\n",
    "    print('variable w is:', w, ' and the loss is ',make_up_loss)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "diff l2 w2: 10*w2\n",
      "final der is  10*w2**2\n",
      "[(w - 3)**2]\n",
      "<class 'numpy.ndarray'>\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'list' object has no attribute 'free_symbols'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-13-b70e2421db80>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ml_array\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ml\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 26\u001b[0;31m \u001b[0mg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdiff\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ml_array\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     27\u001b[0m \u001b[0mfinal_g\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mg\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mw_origin\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m \u001b[0mlearning_rate\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/qw/anaconda3/envs/py36/lib/python3.6/site-packages/sympy/core/function.py\u001b[0m in \u001b[0;36mdiff\u001b[0;34m(f, *symbols, **kwargs)\u001b[0m\n\u001b[1;32m   1689\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mAttributeError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1690\u001b[0m         \u001b[0;32mpass\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1691\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mDerivative\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0msymbols\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1692\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1693\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/qw/anaconda3/envs/py36/lib/python3.6/site-packages/sympy/core/function.py\u001b[0m in \u001b[0;36m__new__\u001b[0;34m(cls, expr, *variables, **assumptions)\u001b[0m\n\u001b[1;32m   1091\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mevaluate\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1092\u001b[0m             \u001b[0msymbol_set\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msc\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0msc\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mvariable_count\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0msc\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_Symbol\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1093\u001b[0;31m             \u001b[0;32mif\u001b[0m \u001b[0msymbol_set\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdifference\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mexpr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfree_symbols\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1094\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mS\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mZero\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1095\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'list' object has no attribute 'free_symbols'"
     ]
    }
   ],
   "source": [
    "from sympy import *\n",
    "import numpy as np\n",
    "w2=Symbol('w2')\n",
    "l2 = 5*(w2**2)\n",
    "deri = diff(l2,w2)#this is a expression,not a value\n",
    "#print(type(float(deri)))\n",
    "print('diff l2 w2:',deri)\n",
    "#print('diff l2 w2:',deri(2))#error:'mul'object?\n",
    "print('final der is ',deri * w2)\n",
    "'''\n",
    "'''\n",
    "y = 3\n",
    "x = 1\n",
    "w_origin = 2\n",
    "w=Symbol('w')\n",
    "#prediction\n",
    "p = w*x\n",
    "\n",
    "\n",
    "\n",
    "#define losses\n",
    "l = (p - y)**2\n",
    "l_array = np.array([l])\n",
    "print(l_array)\n",
    "print(type(np.array(l)))\n",
    "g = diff(l_array, w)\n",
    "final_g = g * w_origin\n",
    "learning_rate = 1\n",
    "\n",
    "#update\n",
    "for _ in range(5):\n",
    "\n",
    "    print('variable is w:',w_origin, ' g is ',final_g,'  and the loss is ',l)\n",
    "\n",
    "    w = w - learning_rate * final_g"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## tf momentum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-2.0], 2.0, 2.0]\n",
      "variable is w: 2.0  g is  [-2.0]   and the loss is  1.0\n",
      "variable is w: 3.0027  g is  [0.0054097176]   and the loss is  7.31626e-06\n",
      "variable is w: 3.00002  g is  [3.9577484e-05]   and the loss is  3.91594e-10\n",
      "variable is w: 3.0  g is  [-1.4305115e-06]   and the loss is  5.11591e-13\n",
      "variable is w: 3.0  g is  [-1.4305115e-06]   and the loss is  5.11591e-13\n",
      "variable is w: 3.0  g is  [-1.4305115e-06]   and the loss is  5.11591e-13\n"
     ]
    }
   ],
   "source": [
    "y = tf.constant(3,dtype = tf.float32)\n",
    "x = tf.placeholder(dtype = tf.float32)\n",
    "w = tf.Variable(2,dtype=tf.float32)\n",
    "#prediction\n",
    "p = w*x\n",
    "\n",
    "#define losses\n",
    "l = tf.square(p - y)\n",
    "g = tf.gradients(l, w)\n",
    "Mu = 0.8\n",
    "LR = tf.constant(0.01,dtype=tf.float32)\n",
    "\n",
    "init = tf.group(tf.global_variables_initializer(),tf.local_variables_initializer())\n",
    "\n",
    "#update w\n",
    "update = tf.train.MomentumOptimizer(LR, Mu).minimize(l)\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    sess.run(init)\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    sess.run(tf.local_variables_initializer())\n",
    "    print(sess.run([g,p,w], {x: 1}))\n",
    "    for epoch in range(300):\n",
    "        if epoch % 50 == 0:\n",
    "            w_,g_,l_ = sess.run([w,g,l],feed_dict={x:1})\n",
    "            print('variable is w:',w_, ' g is ',g_, '  and the loss is ',l_)\n",
    "        sess.run([update],feed_dict={x:1})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 手写momentum\n",
    "算法简单，就是维护一个带衰减的动量v，然后用v更新w。\n",
    "\n",
    "#demo5.2:manual momentum in tensorflow\n",
    "group有问题，要求顺序关系，不能用group，分别运行update1和update2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-2.0], 2.0, 2.0]\n",
      "variable is w: 2.0  g is  [-2.0]  v is  0.0   and the loss is  1.0\n",
      "variable is w: 3.0027  g is  [0.0054097176]  v is  0.000820529   and the loss is  7.31626e-06\n",
      "variable is w: 3.00002  g is  [3.9577484e-05]  v is  1.64438e-06   and the loss is  3.91594e-10\n",
      "variable is w: 3.0  g is  [-1.4305115e-06]  v is  -7.11467e-08   and the loss is  5.11591e-13\n",
      "variable is w: 3.0  g is  [-1.4305115e-06]  v is  -7.15256e-08   and the loss is  5.11591e-13\n",
      "variable is w: 3.0  g is  [-1.4305115e-06]  v is  -7.15256e-08   and the loss is  5.11591e-13\n"
     ]
    }
   ],
   "source": [
    "y = tf.constant(3,dtype = tf.float32)\n",
    "x = tf.placeholder(dtype = tf.float32)\n",
    "w = tf.Variable(2,dtype=tf.float32)\n",
    "#prediction\n",
    "p = w*x\n",
    "\n",
    "#define losses\n",
    "l = tf.square(p - y)\n",
    "g = tf.gradients(l, w)\n",
    "Mu = 0.8\n",
    "LR = tf.constant(0.01,dtype=tf.float32)\n",
    "#v = tf.Variable(0,tf.float32)#error?secend param is not dtype?\n",
    "v = tf.Variable(0,dtype = tf.float32)\n",
    "init = tf.global_variables_initializer()\n",
    "\n",
    "#update w\n",
    "update1 = tf.assign(v, Mu * v + g[0] * LR )\n",
    "update2 = tf.assign(w, w - v)\n",
    "#update = tf.group(update1,update2)#wrong sequence!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    sess.run(init)\n",
    "    print(sess.run([g,p,w], {x: 1}))\n",
    "    for epoch in range(300):\n",
    "        if epoch % 50 == 0:\n",
    "            w_,g_,l_,v_ = sess.run([w,g,l,v],feed_dict={x:1})\n",
    "            print('variable is w:',w_, ' g is ',g_, ' v is ',v_,'  and the loss is ',l_)\n",
    "\n",
    "        _ = sess.run([update1],feed_dict={x:1})\n",
    "        _ = sess.run([update2],feed_dict={x:1})\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TF adagrad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "variable is w: 2.0 g: [-2.0]   and the loss is  1.0\n",
      "variable is w: 2.98227 g: [-0.035456657]   and the loss is  0.000314294\n",
      "variable is w: 2.99964 g: [-0.00072860718]   and the loss is  1.32717e-07\n",
      "variable is w: 2.99999 g: [-1.5258789e-05]   and the loss is  5.82077e-11\n",
      "variable is w: 3.0 g: [0.0]   and the loss is  0.0\n",
      "variable is w: 3.0 g: [0.0]   and the loss is  0.0\n"
     ]
    }
   ],
   "source": [
    "\n",
    "y = tf.constant(3,dtype = tf.float32)\n",
    "x = tf.placeholder(dtype = tf.float32)\n",
    "w = tf.Variable(2,dtype=tf.float32)\n",
    "#prediction\n",
    "p = w*x\n",
    "\n",
    "#define losses\n",
    "l = tf.square(p - y)\n",
    "g = tf.gradients(l, w)\n",
    "LR = tf.constant(0.6,dtype=tf.float32)\n",
    "optimizer = tf.train.AdagradOptimizer(LR)\n",
    "update = optimizer.minimize(l)\n",
    "init = tf.global_variables_initializer()\n",
    "\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    sess.run(init)\n",
    "    #print(sess.run([g,p,w], {x: 1}))\n",
    "    for epoch in range(30):\n",
    "        if epoch % 5 == 0:\n",
    "            w_,l_,g_ = sess.run([w,l,g],feed_dict={x:1})\n",
    "            print('variable is w:',w_, 'g:',g_ ,'  and the loss is ',l_)\n",
    "        _ = sess.run(update,feed_dict={x:1})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## adagrad 手动\n",
    "分子学习率和梯度，分母开方二次导数和正则项，二次导数是用往届梯度累加近似模拟的。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-2.0], 2.0, 2.0]\n",
      "variable is w: 2.0  g is  [-2.0]  g_sec_ is  4.0   and the loss is  1.0\n",
      "variable is w: 2.98976  g is  [-0.020480156]  g_sec_ is  4.00042   and the loss is  0.000104859\n",
      "variable is w: 2.9999  g is  [-0.00020980835]  g_sec_ is  4.00042   and the loss is  1.10049e-08\n",
      "variable is w: 3.0  g is  [-1.9073486e-06]  g_sec_ is  4.00042   and the loss is  9.09495e-13\n",
      "variable is w: 3.0  g is  [0.0]  g_sec_ is  4.00042   and the loss is  0.0\n",
      "variable is w: 3.0  g is  [0.0]  g_sec_ is  4.00042   and the loss is  0.0\n"
     ]
    }
   ],
   "source": [
    "#with tf.name_scope('initial'):\n",
    "y = tf.constant(3,dtype = tf.float32)\n",
    "x = tf.placeholder(dtype=tf.float32)\n",
    "w = tf.Variable(2,dtype=tf.float32,expected_shape=[1])\n",
    "second_derivative = tf.Variable(0,dtype=tf.float32)\n",
    "LR = tf.constant(0.6,dtype=tf.float32)\n",
    "Regular = 1e-8\n",
    "\n",
    "#prediction\n",
    "p = w*x\n",
    "#loss\n",
    "l = tf.square(p - y)\n",
    "#gradients\n",
    "g = tf.gradients(l, w)\n",
    "#print(g)\n",
    "#print(tf.square(g))\n",
    "\n",
    "#update\n",
    "update1 = tf.assign_add(second_derivative,tf.square(g[0]))\n",
    "g_final = LR * g[0] / (tf.sqrt(second_derivative) + Regular)\n",
    "update2 = tf.assign(w, w - g_final)\n",
    "\n",
    "#update = tf.assign(w, w - LR * g[0])\n",
    "\n",
    "init = tf.global_variables_initializer()\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    sess.run(init)\n",
    "    print(sess.run([g,p,w], {x: 1}))\n",
    "    for epoch in range(30):\n",
    "        if epoch % 5 == 0:\n",
    "            _ = sess.run(update1,feed_dict={x:1.0})\n",
    "            w_,g_,l_,g_sec_ = sess.run([w,g,l,second_derivative],feed_dict={x:1.0})\n",
    "            print('variable is w:',w_, ' g is ',g_,' g_sec_ is ',g_sec_,'  and the loss is ',l_)\n",
    "        #sess.run(g_final)\n",
    "\n",
    "        _ = sess.run(update2,feed_dict={x:1.0})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#没有SGD这个optimizer，只有GD，SGD是策略，不是算法。\n",
    "there is no SGD and mini-batch optimizer,just GD\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "virtual py36",
   "language": "python",
   "name": "py36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
