{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "用神经网络解决fizzbuzz\n",
    "\n",
    "随着你想达到的功能不同，会遇到很多新问题。\n",
    "\n",
    "神经网络是越深越大越好，但是明白瓶颈在哪，zhidao 如何改进才是重点。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1 0 0 0 0 0 0 0 0 0]\n",
      "[0 1 0 0 0 0 0 0 0 0]\n",
      "[1 1 0 0 0 0 0 0 0 0]\n",
      "[0 0 0 1 0 0 0 0 0 0]\n",
      "[0 0 0 0 0 0 0 0 0 1]\n",
      "[1 1 1 1 1 1 1 1 1 1]\n"
     ]
    }
   ],
   "source": [
    "#将数字转换为二进制形式。one-hot\n",
    "#i是输入，num_digits是位数限制\n",
    "def binary_encode(i, num_digits):\n",
    "    return np.array([i >> d & 1 for d in range(num_digits)])\n",
    "\n",
    "#这个二进制的数组是左边低位，右边高位\n",
    "print(binary_encode(1,10))\n",
    "print(binary_encode(2,10))\n",
    "print(binary_encode(3,10))\n",
    "print(binary_encode(8,10))\n",
    "print(binary_encode(512,10))\n",
    "print(binary_encode(1023,10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 1 0 0]\n",
      "[0 0 1 0]\n",
      "[0 1 0 0]\n",
      "[0 0 0 1]\n",
      "[0 1 0 0]\n",
      "[0 0 1 0]\n"
     ]
    }
   ],
   "source": [
    "#生成标签，one-hot\n",
    "def fizz_buzz_encode(i):\n",
    "    if i % 15 == 0:\n",
    "        return np.array([0,0,0,1])\n",
    "    elif i % 5 == 0:\n",
    "        return np.array([0,0,1,0])\n",
    "    elif i % 3 == 0:\n",
    "        return np.array([0,1,0,0])\n",
    "    else:\n",
    "        return np.array([1,0,0,0])\n",
    "print(fizz_buzz_encode(3))\n",
    "print(fizz_buzz_encode(5))\n",
    "print(fizz_buzz_encode(6))\n",
    "print(fizz_buzz_encode(15))\n",
    "print(fizz_buzz_encode(18))\n",
    "print(fizz_buzz_encode(20))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#最后输出打印fizzbuz用的\n",
    "def fizz_buzz(i, prediction):\n",
    "    return [str(i), 'fizz', 'buzz', 'fizzbuzz'][prediction]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(923, 10)\n",
      "(923, 4)\n",
      "(100, 10)\n"
     ]
    }
   ],
   "source": [
    "#数据不包含前一百，要预测前一百\n",
    "\n",
    "#但是，如果扩展一下呢？训练集是1024,测试集是十倍，会怎么样？\n",
    "NUM_DIGITS = 10\n",
    "train_X = np.array([binary_encode(i, NUM_DIGITS) for i in range(101, 2**NUM_DIGITS)])\n",
    "train_Y = np.array([fizz_buzz_encode(i) for i in range(101, 2**NUM_DIGITS)])\n",
    "print(train_X.shape)\n",
    "print(train_Y.shape)\n",
    "\n",
    "\n",
    "numbers = np.arange(1,101)\n",
    "#两种写法一样\n",
    "test_X = np.array([binary_encode(i, NUM_DIGITS) for i in range(1, 101)])\n",
    "# test_X2 = np.transpose(binary_encode(numbers,NUM_DIGITS))\n",
    "# print(test_X == test_X2)\n",
    "test_Y = np.array([fizz_buzz_encode(i) for i in range(1, 101)])\n",
    "print(test_X.shape)\n",
    "# print(test_Y)\n",
    "# print(test_Y.shape)\n",
    "# print(test_X)\n",
    "# print(test_Y)\n",
    "#1000以上的预测:实际上，1000到2000,预测就很差了，观察数据，预测fizz buzz还算准确。\n",
    "#可见fizz buzz的规律不是瓶颈，瓶颈是神经网络没见过那么多的数。\n",
    "#因为是onehot的，那么2000比1000会多出一位，而训练数据没有那么大的数\n",
    "#不过，最重要的是另一点，我的NUM——DIGITS设定是10,所以神经网络能认知的数据，就不包含1024以上这个范围，所以，扩展一下NUM——DIGITS\n",
    "#这个NUM——DIGITS设定必须统一，不能训练的时候用10位one-hot，而测试的时候用个11位的one-hot\n",
    "numbers_1000_2000 = np.arange(1000,2000)\n",
    "X_1000_2000 = np.array([binary_encode(i, NUM_DIGITS) for i in range(1000, 2000)])\n",
    "Y_1000_2000 = np.array([fizz_buzz_encode(i) for i in range(1000, 2000)])\n",
    "# print(above1000_X.shape)\n",
    "# print(above1000_X.shape[0])\n",
    "# print(above1000_Y.shape)\n",
    "# print(test_X[:10])\n",
    "# print()\n",
    "# print(above1000_X[:10])\n",
    "\n",
    "numbers_1000_1100 = np.arange(1000,1100)\n",
    "X_1000_1100 = np.array([binary_encode(i, NUM_DIGITS) for i in range(1000, 1100)])\n",
    "Y_1000_1100 = np.array([fizz_buzz_encode(i) for i in range(1000, 1100)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor(\"MatMul_1:0\", shape=(?, 4), dtype=float32)\n",
      "Tensor(\"ArgMax:0\", shape=(?,), dtype=int64)\n"
     ]
    }
   ],
   "source": [
    "#隐藏单元数的设置，1024个数据，1000个单元的话几乎能完成全记忆（但是训练的时候不使用前100个数），100可能不太够？\n",
    "#如果只是fit训练集，其实足够，因为fizz_buzz简单，就三种情况。重点是能否预测新数据。\n",
    "#关于训练数据包含前100的情况，也不用对比测试了，因为训练用100个神经元就是100%\n",
    "\n",
    "NUM_HIDDEN = 100#不能准确预测未知数据。\n",
    "NUM_HIDDEN = 1000\n",
    "BATCH_SIZE = 128\n",
    "\n",
    "\n",
    "X = tf.placeholder('float', [None, NUM_DIGITS])\n",
    "Y = tf.placeholder('float', [None, 4])\n",
    "def init_weights(shape):\n",
    "    return tf.Variable(tf.random_normal(shape, stddev = 0.01))\n",
    "w_h = init_weights([NUM_DIGITS,NUM_HIDDEN])\n",
    "w_o = init_weights([NUM_HIDDEN, 4])\n",
    "def model(X, w_h, w_o):\n",
    "    h = tf.nn.relu(tf.matmul(X, w_h))\n",
    "    return tf.matmul(h,w_o)\n",
    "\n",
    "y_ = model(X, w_h, w_o)\n",
    "cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits = y_, labels = Y))#强制要求带变量名。。。。。\n",
    "train_op = tf.train.GradientDescentOptimizer(0.05).minimize(cost)\n",
    "print(y_)\n",
    "predict = tf.argmax(y_, 1)\n",
    "print(predict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "after 0 epoch,the accuracy is 0.534128\n",
      "after 1000 epoch,the accuracy is 0.784399\n",
      "after 2000 epoch,the accuracy is 0.976165\n",
      "after 3000 epoch,the accuracy is 1.000000\n",
      "after 4000 epoch,the accuracy is 1.000000\n",
      "after 5000 epoch,the accuracy is 1.000000\n",
      "after 6000 epoch,the accuracy is 1.000000\n",
      "after 7000 epoch,the accuracy is 1.000000\n",
      "after 8000 epoch,the accuracy is 1.000000\n",
      "after 9000 epoch,the accuracy is 1.000000\n",
      "test accuracy: 1.0\n",
      "['1' '2' 'fizz' '4' 'buzz' 'fizz' '7' '8' 'fizz' 'buzz' '11' 'fizz' '13'\n",
      " '14' 'fizzbuzz' '16' '17' 'fizz' '19' 'buzz' 'fizz' '22' '23' 'fizz'\n",
      " 'buzz' '26' 'fizz' '28' '29' 'fizzbuzz' '31' '32' 'fizz' '34' 'buzz'\n",
      " 'fizz' '37' '38' 'fizz' 'buzz' '41' 'fizz' '43' '44' 'fizzbuzz' '46' '47'\n",
      " 'fizz' '49' 'buzz' 'fizz' '52' '53' 'fizz' 'buzz' '56' 'fizz' '58' '59'\n",
      " 'fizzbuzz' '61' '62' 'fizz' '64' 'buzz' 'fizz' '67' '68' 'fizz' 'buzz'\n",
      " '71' 'fizz' '73' '74' 'fizzbuzz' '76' '77' 'fizz' '79' 'buzz' 'fizz' '82'\n",
      " '83' 'fizz' 'buzz' '86' 'fizz' '88' '89' 'fizzbuzz' '91' '92' 'fizz' '94'\n",
      " 'buzz' 'fizz' '97' '98' 'fizz' 'buzz']\n",
      "1000_2000 accuracy: 0.22\n",
      "['buzz' '1001' 'fizz' '1003' '1004' 'fizzbuzz' '1006' '1007' 'fizz' '1009'\n",
      " 'buzz' 'fizz' '1012' '1013' 'fizz' 'buzz' '1016' 'fizz' '1018' '1019'\n",
      " 'fizzbuzz' '1021' '1022' 'fizz' '1024' '1025' '1026' 'fizz' '1028' 'buzz'\n",
      " 'fizz' '1031' '1032' 'fizz' 'buzz' '1035' 'fizz' '1037' '1038' 'fizzbuzz'\n",
      " '1040' '1041' 'fizz' '1043' 'buzz' 'fizz' '1046' '1047' 'fizz' 'buzz'\n",
      " '1050' 'fizz' '1052' '1053' 'fizzbuzz' '1055' '1056' 'fizz' '1058' 'buzz'\n",
      " 'fizz' '1061' '1062' 'fizz' 'buzz' '1065' 'fizz' '1067' '1068' 'fizzbuzz'\n",
      " '1070' '1071' 'fizz' '1073' 'buzz' 'fizz' '1076' '1077' 'fizz' 'buzz'\n",
      " '1080' 'fizz' '1082' '1083' 'fizzbuzz' '1085' '1086' 'fizz' '1088' 'buzz'\n",
      " 'fizz' '1091' '1092' 'fizz' 'buzz' '1095' 'fizz' '1097' '1098' 'fizzbuzz'\n",
      " '1100' '1101' 'fizz' '1103' 'buzz' 'fizz' '1106' '1107' 'fizz' 'buzz'\n",
      " '1110' 'fizz' '1112' '1113' 'fizzbuzz' '1115' '1116' 'fizz' '1118' 'buzz'\n",
      " 'fizz' '1121' '1122' 'fizz' 'buzz' '1125' 'fizz' '1127' '1128' 'fizzbuzz'\n",
      " '1130' '1131' 'fizz' '1133' 'buzz' 'fizz' '1136' '1137' 'fizz' 'buzz'\n",
      " '1140' 'fizz' '1142' '1143' 'fizzbuzz' '1145' '1146' 'fizz' '1148' 'buzz'\n",
      " 'fizz' '1151' '1152' 'fizz' 'buzz' '1155' 'fizz' '1157' '1158' 'fizzbuzz'\n",
      " '1160' '1161' 'fizz' '1163' 'buzz' 'fizz' '1166' '1167' 'fizz' 'buzz'\n",
      " '1170' 'fizz' '1172' '1173' 'fizzbuzz' '1175' '1176' 'fizz' '1178' 'buzz'\n",
      " 'fizz' '1181' '1182' 'fizz' 'buzz' '1185' 'fizz' '1187' '1188' 'fizzbuzz'\n",
      " '1190' '1191' 'fizz' '1193' 'buzz' 'fizz' '1196' '1197' 'fizz' 'buzz'\n",
      " '1200' 'fizz' '1202' '1203' 'fizzbuzz' '1205' '1206' 'fizz' '1208' 'buzz'\n",
      " 'fizz' '1211' '1212' 'fizz' 'buzz' '1215' 'fizz' '1217' '1218' 'fizzbuzz'\n",
      " '1220' '1221' 'fizz' '1223' 'buzz' 'fizz' '1226' '1227' 'fizz' 'buzz'\n",
      " '1230' 'fizz' '1232' '1233' 'fizzbuzz' '1235' '1236' 'fizz' '1238' 'buzz'\n",
      " 'fizz' '1241' '1242' 'fizz' 'buzz' '1245' 'fizz' '1247' '1248' 'fizzbuzz'\n",
      " '1250' '1251' 'fizz' '1253' 'buzz' 'fizz' '1256' '1257' 'fizz' 'buzz'\n",
      " '1260' 'fizz' '1262' '1263' 'fizzbuzz' '1265' '1266' 'fizz' '1268' 'buzz'\n",
      " 'fizz' '1271' '1272' 'fizz' 'buzz' '1275' 'fizz' '1277' '1278' 'fizzbuzz'\n",
      " '1280' '1281' 'fizz' '1283' 'buzz' 'fizz' '1286' '1287' 'fizz' 'buzz'\n",
      " '1290' 'fizz' '1292' '1293' 'fizzbuzz' '1295' '1296' 'fizz' '1298' 'buzz'\n",
      " 'fizz' '1301' '1302' 'fizz' 'buzz' '1305' 'fizz' '1307' '1308' 'fizzbuzz'\n",
      " '1310' '1311' 'fizz' '1313' 'buzz' 'fizz' '1316' '1317' 'fizz' 'buzz'\n",
      " '1320' 'fizz' '1322' '1323' 'fizzbuzz' '1325' '1326' 'fizz' '1328' 'buzz'\n",
      " 'fizz' '1331' '1332' 'fizz' 'buzz' '1335' 'fizz' '1337' '1338' 'fizzbuzz'\n",
      " '1340' '1341' 'fizz' '1343' 'buzz' 'fizz' '1346' '1347' 'fizz' 'buzz'\n",
      " '1350' 'fizz' '1352' '1353' 'fizzbuzz' '1355' '1356' 'fizz' '1358' 'buzz'\n",
      " 'fizz' '1361' '1362' 'fizz' 'buzz' '1365' 'fizz' '1367' '1368' 'fizzbuzz'\n",
      " '1370' '1371' 'fizz' '1373' 'buzz' 'fizz' '1376' '1377' 'fizz' 'buzz'\n",
      " '1380' 'fizz' '1382' '1383' 'fizzbuzz' '1385' '1386' 'fizz' '1388' 'buzz'\n",
      " 'fizz' '1391' '1392' 'fizz' 'buzz' '1395' 'fizz' '1397' '1398' 'fizzbuzz'\n",
      " '1400' '1401' 'fizz' '1403' 'buzz' 'fizz' '1406' '1407' 'fizz' 'buzz'\n",
      " '1410' 'fizz' '1412' '1413' 'fizzbuzz' '1415' '1416' 'fizz' '1418' 'buzz'\n",
      " 'fizz' '1421' '1422' 'fizz' 'buzz' '1425' 'fizz' '1427' '1428' 'fizzbuzz'\n",
      " '1430' '1431' 'fizz' '1433' 'buzz' 'fizz' '1436' '1437' 'fizz' 'buzz'\n",
      " '1440' 'fizz' '1442' '1443' 'fizzbuzz' '1445' '1446' 'fizz' '1448' 'buzz'\n",
      " 'fizz' '1451' '1452' 'fizz' 'buzz' '1455' 'fizz' '1457' '1458' 'fizzbuzz'\n",
      " '1460' '1461' 'fizz' '1463' 'buzz' 'fizz' '1466' '1467' 'fizz' 'buzz'\n",
      " '1470' 'fizz' '1472' '1473' 'fizzbuzz' '1475' '1476' 'fizz' '1478' 'buzz'\n",
      " 'fizz' '1481' '1482' 'fizz' 'buzz' '1485' 'fizz' '1487' '1488' 'fizzbuzz'\n",
      " '1490' '1491' 'fizz' '1493' 'buzz' 'fizz' '1496' '1497' 'fizz' 'buzz'\n",
      " '1500' 'fizz' '1502' '1503' 'fizzbuzz' '1505' '1506' 'fizz' '1508' 'buzz'\n",
      " 'fizz' '1511' '1512' 'fizz' 'buzz' '1515' 'fizz' '1517' '1518' 'fizzbuzz'\n",
      " '1520' '1521' 'fizz' '1523' 'buzz' 'fizz' '1526' '1527' 'fizz' 'buzz'\n",
      " '1530' 'fizz' '1532' '1533' 'fizzbuzz' '1535' '1536' 'fizz' '1538' 'buzz'\n",
      " 'fizz' '1541' '1542' 'fizz' 'buzz' '1545' 'fizz' '1547' '1548' 'fizzbuzz'\n",
      " '1550' '1551' 'fizz' '1553' 'buzz' 'fizz' '1556' '1557' 'fizz' 'buzz'\n",
      " '1560' 'fizz' '1562' '1563' 'fizzbuzz' '1565' '1566' 'fizz' '1568' 'buzz'\n",
      " 'fizz' '1571' '1572' 'fizz' 'buzz' '1575' 'fizz' '1577' '1578' 'fizzbuzz'\n",
      " '1580' '1581' 'fizz' '1583' 'buzz' 'fizz' '1586' '1587' 'fizz' 'buzz'\n",
      " '1590' 'fizz' '1592' '1593' 'fizzbuzz' '1595' '1596' 'fizz' '1598' 'buzz'\n",
      " 'fizz' '1601' '1602' 'fizz' 'buzz' '1605' 'fizz' '1607' '1608' 'fizzbuzz'\n",
      " '1610' '1611' 'fizz' '1613' 'buzz' 'fizz' '1616' '1617' 'fizz' 'buzz'\n",
      " '1620' 'fizz' '1622' '1623' 'fizzbuzz' '1625' '1626' 'fizz' '1628' 'buzz'\n",
      " 'fizz' '1631' '1632' 'fizz' 'buzz' '1635' 'fizz' '1637' '1638' 'fizzbuzz'\n",
      " '1640' '1641' 'fizz' '1643' 'buzz' 'fizz' '1646' '1647' 'fizz' 'buzz'\n",
      " '1650' 'fizz' '1652' '1653' 'fizzbuzz' '1655' '1656' 'fizz' '1658' 'buzz'\n",
      " 'fizz' '1661' '1662' 'fizz' 'buzz' '1665' 'fizz' '1667' '1668' 'fizzbuzz'\n",
      " '1670' '1671' 'fizz' '1673' 'buzz' 'fizz' '1676' '1677' 'fizz' 'buzz'\n",
      " '1680' 'fizz' '1682' '1683' 'fizzbuzz' '1685' '1686' 'fizz' '1688' 'buzz'\n",
      " 'fizz' '1691' '1692' 'fizz' 'buzz' '1695' 'fizz' '1697' '1698' 'fizzbuzz'\n",
      " '1700' '1701' 'fizz' '1703' 'buzz' 'fizz' '1706' '1707' 'fizz' 'buzz'\n",
      " '1710' 'fizz' '1712' '1713' 'fizzbuzz' '1715' '1716' 'fizz' '1718' 'buzz'\n",
      " 'fizz' '1721' '1722' 'fizz' 'buzz' '1725' 'fizz' '1727' '1728' 'fizzbuzz'\n",
      " '1730' '1731' 'fizz' '1733' 'buzz' 'fizz' '1736' '1737' 'fizz' 'buzz'\n",
      " '1740' 'fizz' '1742' '1743' 'fizzbuzz' '1745' '1746' 'fizz' '1748' 'buzz'\n",
      " 'fizz' '1751' '1752' 'fizz' 'buzz' '1755' 'fizz' '1757' '1758' 'fizzbuzz'\n",
      " '1760' '1761' 'fizz' '1763' 'buzz' 'fizz' '1766' '1767' 'fizz' 'buzz'\n",
      " '1770' 'fizz' '1772' '1773' 'fizzbuzz' '1775' '1776' 'fizz' '1778' 'buzz'\n",
      " 'fizz' '1781' '1782' 'fizz' 'buzz' '1785' 'fizz' '1787' '1788' 'fizzbuzz'\n",
      " '1790' '1791' 'fizz' '1793' 'buzz' 'fizz' '1796' '1797' 'fizz' 'buzz'\n",
      " '1800' 'fizz' '1802' '1803' 'fizzbuzz' '1805' '1806' 'fizz' '1808' 'buzz'\n",
      " 'fizz' '1811' '1812' 'fizz' 'buzz' '1815' 'fizz' '1817' '1818' 'fizzbuzz'\n",
      " '1820' '1821' 'fizz' '1823' 'buzz' 'fizz' '1826' '1827' 'fizz' 'buzz'\n",
      " '1830' 'fizz' '1832' '1833' 'fizzbuzz' '1835' '1836' 'fizz' '1838' 'buzz'\n",
      " 'fizz' '1841' '1842' 'fizz' 'buzz' '1845' 'fizz' '1847' '1848' 'fizzbuzz'\n",
      " '1850' '1851' 'fizz' '1853' 'buzz' 'fizz' '1856' '1857' 'fizz' 'buzz'\n",
      " '1860' 'fizz' '1862' '1863' 'fizzbuzz' '1865' '1866' 'fizz' '1868' 'buzz'\n",
      " 'fizz' '1871' '1872' 'fizz' 'buzz' '1875' 'fizz' '1877' '1878' 'fizzbuzz'\n",
      " '1880' '1881' 'fizz' '1883' 'buzz' 'fizz' '1886' '1887' 'fizz' 'buzz'\n",
      " '1890' 'fizz' '1892' '1893' 'fizzbuzz' '1895' '1896' 'fizz' '1898' 'buzz'\n",
      " 'fizz' '1901' '1902' 'fizz' 'buzz' '1905' 'fizz' '1907' '1908' 'fizzbuzz'\n",
      " '1910' '1911' 'fizz' '1913' 'buzz' 'fizz' '1916' '1917' 'fizz' 'buzz'\n",
      " '1920' 'fizz' '1922' '1923' 'fizzbuzz' '1925' '1926' 'fizz' '1928' 'buzz'\n",
      " 'fizz' '1931' '1932' 'fizz' 'buzz' '1935' 'fizz' '1937' '1938' 'fizzbuzz'\n",
      " '1940' '1941' 'fizz' '1943' 'buzz' 'fizz' '1946' '1947' 'fizz' 'buzz'\n",
      " '1950' 'fizz' '1952' '1953' 'fizzbuzz' '1955' '1956' 'fizz' '1958' 'buzz'\n",
      " 'fizz' '1961' '1962' 'fizz' 'buzz' '1965' 'fizz' '1967' '1968' 'fizzbuzz'\n",
      " '1970' '1971' 'fizz' '1973' 'buzz' 'fizz' '1976' '1977' 'fizz' 'buzz'\n",
      " '1980' 'fizz' '1982' '1983' 'fizzbuzz' '1985' '1986' 'fizz' '1988' 'buzz'\n",
      " 'fizz' '1991' '1992' 'fizz' 'buzz' '1995' 'fizz' '1997' '1998' 'fizzbuzz']\n",
      "1000 to 1100's accuracy: 0.4\n",
      "['buzz' '1001' 'fizz' '1003' '1004' 'fizzbuzz' '1006' '1007' 'fizz' '1009'\n",
      " 'buzz' 'fizz' '1012' '1013' 'fizz' 'buzz' '1016' 'fizz' '1018' '1019'\n",
      " 'fizzbuzz' '1021' '1022' 'fizz' '1024' '1025' '1026' 'fizz' '1028' 'buzz'\n",
      " 'fizz' '1031' '1032' 'fizz' 'buzz' '1035' 'fizz' '1037' '1038' 'fizzbuzz'\n",
      " '1040' '1041' 'fizz' '1043' 'buzz' 'fizz' '1046' '1047' 'fizz' 'buzz'\n",
      " '1050' 'fizz' '1052' '1053' 'fizzbuzz' '1055' '1056' 'fizz' '1058' 'buzz'\n",
      " 'fizz' '1061' '1062' 'fizz' 'buzz' '1065' 'fizz' '1067' '1068' 'fizzbuzz'\n",
      " '1070' '1071' 'fizz' '1073' 'buzz' 'fizz' '1076' '1077' 'fizz' 'buzz'\n",
      " '1080' 'fizz' '1082' '1083' 'fizzbuzz' '1085' '1086' 'fizz' '1088' 'buzz'\n",
      " 'fizz' '1091' '1092' 'fizz' 'buzz' '1095' 'fizz' '1097' '1098' 'fizzbuzz']\n"
     ]
    }
   ],
   "source": [
    "with tf.Session() as sess:\n",
    "    tf.global_variables_initializer().run()\n",
    "    #进行10000轮，每一轮打乱训练数据的顺序，用mini-batch训练\n",
    "    for epoch in range(10000):\n",
    "        p = np.random.permutation(range(len(train_X)))#生成随机序列\n",
    "        train_X, train_Y = train_X[p], train_Y[p]\n",
    "        for start in range(0, len(train_X), BATCH_SIZE):\n",
    "            end = start + BATCH_SIZE\n",
    "            sess.run(train_op, feed_dict={X:train_X[start:end], Y:train_Y[start:end]})\n",
    "        if epoch % 1000 == 0 :\n",
    "            prediction = sess.run(predict,feed_dict={X:train_X,Y:train_Y})\n",
    "            print('after %d epoch,the accuracy is %f'%(epoch, np.mean(np.argmax(train_Y,axis=1) == prediction)))\n",
    "        \n",
    "        \n",
    "    test_prediction = sess.run(predict, feed_dict={X:test_X})\n",
    "    output = np.vectorize(fizz_buzz)(numbers,test_prediction)\n",
    "    print('test accuracy:',np.mean(np.argmax(test_Y,axis=1) == test_prediction))\n",
    "    print(output)\n",
    " \n",
    "\n",
    "    test_prediction2 = sess.run(predict, feed_dict={X:X_1000_2000})\n",
    "    output = np.vectorize(fizz_buzz)(numbers_1000_2000,test_prediction2)\n",
    "    print('1000_2000 accuracy:',np.mean(np.argmax(Y_1000_2000,axis=1) == test_prediction2))\n",
    "    print(output)\n",
    "    \n",
    "    test_prediction3 = sess.run(predict, feed_dict={X:X_1000_1100})\n",
    "    output = np.vectorize(fizz_buzz)(numbers_1000_1100,test_prediction3)\n",
    "    print('1000 to 1100\\'s accuracy:',np.mean(np.argmax(Y_1000_1100,axis=1) == test_prediction3))\n",
    "    print(output)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 这是用100个神经元进行的训练，19错了，32错了，80错了，81错了，83错了，84错了，,87错了。\n",
    "['1' '2' 'fizz' '4' 'buzz' 'fizz' '7' '8' 'fizz' 'buzz' '11' 'fizz' '13'\n",
    " '14' 'fizzbuzz' '16' '17' 'fizz' 'fizz' 'buzz' '21' '22' '23' 'fizz'\n",
    " 'buzz' '26' 'fizz' '28' '29' 'fizzbuzz' '31' 'fizz' 'fizz' '34' 'buzz'\n",
    " 'fizz' '37' 'buzz' 'fizz' 'buzz' '41' 'fizz' '43' '44' 'fizzbuzz' '46'\n",
    " '47' 'fizz' '49' 'buzz' 'fizz' '52' '53' 'fizz' 'buzz' '56' 'fizz' '58'\n",
    " '59' 'fizzbuzz' '61' '62' 'fizz' '64' 'buzz' 'fizz' '67' '68' '69' 'buzz'\n",
    " '71' 'fizz' '73' '74' 'fizzbuzz' '76' '77' 'fizz' '79' '80' '81' '82'\n",
    " 'fizz' '84' 'buzz' '86' '87' '88' '89' 'fizzbuzz' '91' '92' 'fizz' '94'\n",
    " 'buzz' 'fizz' '97' '98' 'fizz' 'buzz']\n",
    "#这是用1000个神经元训练。训练变慢了倒是真的。测试准确率1\n",
    "['1' '2' 'fizz' '4' 'buzz' 'fizz' '7' '8' 'fizz' 'buzz' '11' 'fizz' '13'\n",
    " '14' 'fizzbuzz' '16' '17' 'fizz' '19' 'buzz' 'fizz' '22' '23' 'fizz'\n",
    " 'buzz' '26' 'fizz' '28' '29' 'fizzbuzz' '31' '32' 'fizz' '34' 'buzz'\n",
    " 'fizz' '37' '38' 'fizz' 'buzz' '41' 'fizz' '43' '44' 'fizzbuzz' '46' '47'\n",
    " 'fizz' '49' 'buzz' 'fizz' '52' '53' 'fizz' 'buzz' '56' 'fizz' '58' '59'\n",
    " 'fizzbuzz' '61' '62' 'fizz' '64' 'buzz' 'fizz' '67' '68' 'fizz' 'buzz'\n",
    " '71' 'fizz' '73' '74' 'fizzbuzz' '76' '77' 'fizz' '79' 'buzz' 'fizz' '82'\n",
    " '83' 'fizz' 'buzz' '86' 'fizz' '88' '89' 'fizzbuzz' '91' '92' 'fizz' '94'\n",
    " 'buzz' 'fizz' '97' '98' 'fizz' 'buzz']\n",
    "\n",
    "\n",
    "#这是用1000个神经元训练。\n",
    "#用1000-2000预测:0.22,惨不忍睹\n",
    "#1000 to 1100's accuracy: 0.4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1000以上的预测优化：\n",
    "\n",
    "#因为是onehot的，那么2000比1000会多出一位，而训练数据没有那么大的数，最重要的是另一点：我的NUM_DIGITS设定是10,所以神经网络能认知的数据，就不包含1024以上这个范围，所以，扩展一下NUM_DIGITS\n",
    "#这个NUM——DIGITS设定必须统一，不能训练的时候用10位one-hot，而测试的时候用个11位的one-hot\n",
    "\n",
    "\n",
    "#其他都不变，改成11位，神经元也不扩充，训练数据仍然用1024内，预测1000以上的数据。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1947, 11)\n",
      "(1947, 4)\n",
      "(100, 11)\n"
     ]
    }
   ],
   "source": [
    "NUM_DIGITS = 11\n",
    "train_X = np.array([binary_encode(i, NUM_DIGITS) for i in range(101, 2**NUM_DIGITS)])\n",
    "train_Y = np.array([fizz_buzz_encode(i) for i in range(101, 2**NUM_DIGITS)])\n",
    "print(train_X.shape)\n",
    "print(train_Y.shape)\n",
    "\n",
    "numbers = np.arange(1,101)\n",
    "#两种写法一样\n",
    "test_X = np.array([binary_encode(i, NUM_DIGITS) for i in range(1, 101)])\n",
    "test_Y = np.array([fizz_buzz_encode(i) for i in range(1, 101)])\n",
    "\n",
    "numbers_1000_2000 = np.arange(1000,2000)\n",
    "X_1000_2000 = np.array([binary_encode(i, NUM_DIGITS) for i in range(1000, 2000)])\n",
    "Y_1000_2000 = np.array([fizz_buzz_encode(i) for i in range(1000, 2000)])\n",
    "\n",
    "numbers_1000_1100 = np.arange(1000,1100)\n",
    "X_1000_1100 = np.array([binary_encode(i, NUM_DIGITS) for i in range(1000, 1100)])\n",
    "Y_1000_1100 = np.array([fizz_buzz_encode(i) for i in range(1000, 1100)])\n",
    "print(X_1000_1100.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor(\"MatMul_3:0\", shape=(?, 4), dtype=float32)\n",
      "Tensor(\"ArgMax_1:0\", shape=(?,), dtype=int64)\n"
     ]
    }
   ],
   "source": [
    "#隐藏单元数的设置，1024个数据，1000个单元的话几乎能完成全记忆（但是训练的时候不使用前100个数），100可能不太够？\n",
    "#如果只是fit训练集，其实足够，因为fizz_buzz简单，就三种情况。重点是能否预测新数据。\n",
    "#关于训练数据包含前100的情况，也不用对比测试了，因为训练用100个神经元就是100%\n",
    "\n",
    "NUM_HIDDEN = 100\n",
    "BATCH_SIZE = 128\n",
    "\n",
    "\n",
    "X = tf.placeholder('float', [None, NUM_DIGITS])\n",
    "Y = tf.placeholder('float', [None, 4])\n",
    "def init_weights(shape):\n",
    "    return tf.Variable(tf.random_normal(shape, stddev = 0.01))\n",
    "w_h = init_weights([NUM_DIGITS,NUM_HIDDEN])\n",
    "w_o = init_weights([NUM_HIDDEN, 4])\n",
    "def model(X, w_h, w_o):\n",
    "    h = tf.nn.relu(tf.matmul(X, w_h))\n",
    "    return tf.matmul(h,w_o)\n",
    "\n",
    "y_ = model(X, w_h, w_o)\n",
    "cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits = y_, labels = Y))#强制要求带变量名。。。。。\n",
    "train_op = tf.train.GradientDescentOptimizer(0.05).minimize(cost)\n",
    "print(y_)\n",
    "predict = tf.argmax(y_, 1)\n",
    "print(predict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "after 0 epoch,the accuracy is 0.533641\n",
      "after 1000 epoch,the accuracy is 0.659476\n",
      "after 2000 epoch,the accuracy is 0.955829\n",
      "after 3000 epoch,the accuracy is 0.993323\n",
      "after 4000 epoch,the accuracy is 0.997432\n",
      "after 5000 epoch,the accuracy is 1.000000\n",
      "after 6000 epoch,the accuracy is 1.000000\n",
      "after 7000 epoch,the accuracy is 1.000000\n",
      "test accuracy: 0.96\n",
      "['1' '2' 'fizz' '4' 'buzz' 'fizz' '7' '8' 'fizz' 'buzz' '11' 'fizz' '13'\n",
      " '14' 'fizzbuzz' '16' '17' 'fizz' '19' 'buzz' 'fizz' '22' '23' 'fizz'\n",
      " 'buzz' '26' 'fizz' '28' '29' 'fizzbuzz' '31' '32' 'fizz' '34' '35' 'fizz'\n",
      " '37' '38' 'fizz' 'buzz' '41' 'fizz' '43' '44' 'fizzbuzz' '46' '47' 'fizz'\n",
      " '49' 'buzz' 'fizz' '52' '53' 'fizz' '55' '56' 'fizz' '58' '59' 'fizzbuzz'\n",
      " '61' '62' 'fizz' '64' 'buzz' 'fizz' '67' '68' 'fizz' 'buzz' '71' 'fizz'\n",
      " '73' '74' 'fizzbuzz' '76' '77' 'fizz' '79' 'buzz' 'buzz' '82' '83' 'fizz'\n",
      " 'buzz' '86' 'fizz' '88' '89' 'fizzbuzz' '91' '92' 'fizz' '94' 'buzz'\n",
      " 'fizz' '97' 'fizz' 'fizz' 'buzz']\n",
      "1000 to 2000's accuracy: 1.0\n",
      "['buzz' '1001' 'fizz' '1003' '1004' 'fizzbuzz' '1006' '1007' 'fizz' '1009'\n",
      " 'buzz' 'fizz' '1012' '1013' 'fizz' 'buzz' '1016' 'fizz' '1018' '1019'\n",
      " 'fizzbuzz' '1021' '1022' 'fizz' '1024' 'buzz' 'fizz' '1027' '1028' 'fizz'\n",
      " 'buzz' '1031' 'fizz' '1033' '1034' 'fizzbuzz' '1036' '1037' 'fizz' '1039'\n",
      " 'buzz' 'fizz' '1042' '1043' 'fizz' 'buzz' '1046' 'fizz' '1048' '1049'\n",
      " 'fizzbuzz' '1051' '1052' 'fizz' '1054' 'buzz' 'fizz' '1057' '1058' 'fizz'\n",
      " 'buzz' '1061' 'fizz' '1063' '1064' 'fizzbuzz' '1066' '1067' 'fizz' '1069'\n",
      " 'buzz' 'fizz' '1072' '1073' 'fizz' 'buzz' '1076' 'fizz' '1078' '1079'\n",
      " 'fizzbuzz' '1081' '1082' 'fizz' '1084' 'buzz' 'fizz' '1087' '1088' 'fizz'\n",
      " 'buzz' '1091' 'fizz' '1093' '1094' 'fizzbuzz' '1096' '1097' 'fizz' '1099'\n",
      " 'buzz' 'fizz' '1102' '1103' 'fizz' 'buzz' '1106' 'fizz' '1108' '1109'\n",
      " 'fizzbuzz' '1111' '1112' 'fizz' '1114' 'buzz' 'fizz' '1117' '1118' 'fizz'\n",
      " 'buzz' '1121' 'fizz' '1123' '1124' 'fizzbuzz' '1126' '1127' 'fizz' '1129'\n",
      " 'buzz' 'fizz' '1132' '1133' 'fizz' 'buzz' '1136' 'fizz' '1138' '1139'\n",
      " 'fizzbuzz' '1141' '1142' 'fizz' '1144' 'buzz' 'fizz' '1147' '1148' 'fizz'\n",
      " 'buzz' '1151' 'fizz' '1153' '1154' 'fizzbuzz' '1156' '1157' 'fizz' '1159'\n",
      " 'buzz' 'fizz' '1162' '1163' 'fizz' 'buzz' '1166' 'fizz' '1168' '1169'\n",
      " 'fizzbuzz' '1171' '1172' 'fizz' '1174' 'buzz' 'fizz' '1177' '1178' 'fizz'\n",
      " 'buzz' '1181' 'fizz' '1183' '1184' 'fizzbuzz' '1186' '1187' 'fizz' '1189'\n",
      " 'buzz' 'fizz' '1192' '1193' 'fizz' 'buzz' '1196' 'fizz' '1198' '1199'\n",
      " 'fizzbuzz' '1201' '1202' 'fizz' '1204' 'buzz' 'fizz' '1207' '1208' 'fizz'\n",
      " 'buzz' '1211' 'fizz' '1213' '1214' 'fizzbuzz' '1216' '1217' 'fizz' '1219'\n",
      " 'buzz' 'fizz' '1222' '1223' 'fizz' 'buzz' '1226' 'fizz' '1228' '1229'\n",
      " 'fizzbuzz' '1231' '1232' 'fizz' '1234' 'buzz' 'fizz' '1237' '1238' 'fizz'\n",
      " 'buzz' '1241' 'fizz' '1243' '1244' 'fizzbuzz' '1246' '1247' 'fizz' '1249'\n",
      " 'buzz' 'fizz' '1252' '1253' 'fizz' 'buzz' '1256' 'fizz' '1258' '1259'\n",
      " 'fizzbuzz' '1261' '1262' 'fizz' '1264' 'buzz' 'fizz' '1267' '1268' 'fizz'\n",
      " 'buzz' '1271' 'fizz' '1273' '1274' 'fizzbuzz' '1276' '1277' 'fizz' '1279'\n",
      " 'buzz' 'fizz' '1282' '1283' 'fizz' 'buzz' '1286' 'fizz' '1288' '1289'\n",
      " 'fizzbuzz' '1291' '1292' 'fizz' '1294' 'buzz' 'fizz' '1297' '1298' 'fizz'\n",
      " 'buzz' '1301' 'fizz' '1303' '1304' 'fizzbuzz' '1306' '1307' 'fizz' '1309'\n",
      " 'buzz' 'fizz' '1312' '1313' 'fizz' 'buzz' '1316' 'fizz' '1318' '1319'\n",
      " 'fizzbuzz' '1321' '1322' 'fizz' '1324' 'buzz' 'fizz' '1327' '1328' 'fizz'\n",
      " 'buzz' '1331' 'fizz' '1333' '1334' 'fizzbuzz' '1336' '1337' 'fizz' '1339'\n",
      " 'buzz' 'fizz' '1342' '1343' 'fizz' 'buzz' '1346' 'fizz' '1348' '1349'\n",
      " 'fizzbuzz' '1351' '1352' 'fizz' '1354' 'buzz' 'fizz' '1357' '1358' 'fizz'\n",
      " 'buzz' '1361' 'fizz' '1363' '1364' 'fizzbuzz' '1366' '1367' 'fizz' '1369'\n",
      " 'buzz' 'fizz' '1372' '1373' 'fizz' 'buzz' '1376' 'fizz' '1378' '1379'\n",
      " 'fizzbuzz' '1381' '1382' 'fizz' '1384' 'buzz' 'fizz' '1387' '1388' 'fizz'\n",
      " 'buzz' '1391' 'fizz' '1393' '1394' 'fizzbuzz' '1396' '1397' 'fizz' '1399'\n",
      " 'buzz' 'fizz' '1402' '1403' 'fizz' 'buzz' '1406' 'fizz' '1408' '1409'\n",
      " 'fizzbuzz' '1411' '1412' 'fizz' '1414' 'buzz' 'fizz' '1417' '1418' 'fizz'\n",
      " 'buzz' '1421' 'fizz' '1423' '1424' 'fizzbuzz' '1426' '1427' 'fizz' '1429'\n",
      " 'buzz' 'fizz' '1432' '1433' 'fizz' 'buzz' '1436' 'fizz' '1438' '1439'\n",
      " 'fizzbuzz' '1441' '1442' 'fizz' '1444' 'buzz' 'fizz' '1447' '1448' 'fizz'\n",
      " 'buzz' '1451' 'fizz' '1453' '1454' 'fizzbuzz' '1456' '1457' 'fizz' '1459'\n",
      " 'buzz' 'fizz' '1462' '1463' 'fizz' 'buzz' '1466' 'fizz' '1468' '1469'\n",
      " 'fizzbuzz' '1471' '1472' 'fizz' '1474' 'buzz' 'fizz' '1477' '1478' 'fizz'\n",
      " 'buzz' '1481' 'fizz' '1483' '1484' 'fizzbuzz' '1486' '1487' 'fizz' '1489'\n",
      " 'buzz' 'fizz' '1492' '1493' 'fizz' 'buzz' '1496' 'fizz' '1498' '1499'\n",
      " 'fizzbuzz' '1501' '1502' 'fizz' '1504' 'buzz' 'fizz' '1507' '1508' 'fizz'\n",
      " 'buzz' '1511' 'fizz' '1513' '1514' 'fizzbuzz' '1516' '1517' 'fizz' '1519'\n",
      " 'buzz' 'fizz' '1522' '1523' 'fizz' 'buzz' '1526' 'fizz' '1528' '1529'\n",
      " 'fizzbuzz' '1531' '1532' 'fizz' '1534' 'buzz' 'fizz' '1537' '1538' 'fizz'\n",
      " 'buzz' '1541' 'fizz' '1543' '1544' 'fizzbuzz' '1546' '1547' 'fizz' '1549'\n",
      " 'buzz' 'fizz' '1552' '1553' 'fizz' 'buzz' '1556' 'fizz' '1558' '1559'\n",
      " 'fizzbuzz' '1561' '1562' 'fizz' '1564' 'buzz' 'fizz' '1567' '1568' 'fizz'\n",
      " 'buzz' '1571' 'fizz' '1573' '1574' 'fizzbuzz' '1576' '1577' 'fizz' '1579'\n",
      " 'buzz' 'fizz' '1582' '1583' 'fizz' 'buzz' '1586' 'fizz' '1588' '1589'\n",
      " 'fizzbuzz' '1591' '1592' 'fizz' '1594' 'buzz' 'fizz' '1597' '1598' 'fizz'\n",
      " 'buzz' '1601' 'fizz' '1603' '1604' 'fizzbuzz' '1606' '1607' 'fizz' '1609'\n",
      " 'buzz' 'fizz' '1612' '1613' 'fizz' 'buzz' '1616' 'fizz' '1618' '1619'\n",
      " 'fizzbuzz' '1621' '1622' 'fizz' '1624' 'buzz' 'fizz' '1627' '1628' 'fizz'\n",
      " 'buzz' '1631' 'fizz' '1633' '1634' 'fizzbuzz' '1636' '1637' 'fizz' '1639'\n",
      " 'buzz' 'fizz' '1642' '1643' 'fizz' 'buzz' '1646' 'fizz' '1648' '1649'\n",
      " 'fizzbuzz' '1651' '1652' 'fizz' '1654' 'buzz' 'fizz' '1657' '1658' 'fizz'\n",
      " 'buzz' '1661' 'fizz' '1663' '1664' 'fizzbuzz' '1666' '1667' 'fizz' '1669'\n",
      " 'buzz' 'fizz' '1672' '1673' 'fizz' 'buzz' '1676' 'fizz' '1678' '1679'\n",
      " 'fizzbuzz' '1681' '1682' 'fizz' '1684' 'buzz' 'fizz' '1687' '1688' 'fizz'\n",
      " 'buzz' '1691' 'fizz' '1693' '1694' 'fizzbuzz' '1696' '1697' 'fizz' '1699'\n",
      " 'buzz' 'fizz' '1702' '1703' 'fizz' 'buzz' '1706' 'fizz' '1708' '1709'\n",
      " 'fizzbuzz' '1711' '1712' 'fizz' '1714' 'buzz' 'fizz' '1717' '1718' 'fizz'\n",
      " 'buzz' '1721' 'fizz' '1723' '1724' 'fizzbuzz' '1726' '1727' 'fizz' '1729'\n",
      " 'buzz' 'fizz' '1732' '1733' 'fizz' 'buzz' '1736' 'fizz' '1738' '1739'\n",
      " 'fizzbuzz' '1741' '1742' 'fizz' '1744' 'buzz' 'fizz' '1747' '1748' 'fizz'\n",
      " 'buzz' '1751' 'fizz' '1753' '1754' 'fizzbuzz' '1756' '1757' 'fizz' '1759'\n",
      " 'buzz' 'fizz' '1762' '1763' 'fizz' 'buzz' '1766' 'fizz' '1768' '1769'\n",
      " 'fizzbuzz' '1771' '1772' 'fizz' '1774' 'buzz' 'fizz' '1777' '1778' 'fizz'\n",
      " 'buzz' '1781' 'fizz' '1783' '1784' 'fizzbuzz' '1786' '1787' 'fizz' '1789'\n",
      " 'buzz' 'fizz' '1792' '1793' 'fizz' 'buzz' '1796' 'fizz' '1798' '1799'\n",
      " 'fizzbuzz' '1801' '1802' 'fizz' '1804' 'buzz' 'fizz' '1807' '1808' 'fizz'\n",
      " 'buzz' '1811' 'fizz' '1813' '1814' 'fizzbuzz' '1816' '1817' 'fizz' '1819'\n",
      " 'buzz' 'fizz' '1822' '1823' 'fizz' 'buzz' '1826' 'fizz' '1828' '1829'\n",
      " 'fizzbuzz' '1831' '1832' 'fizz' '1834' 'buzz' 'fizz' '1837' '1838' 'fizz'\n",
      " 'buzz' '1841' 'fizz' '1843' '1844' 'fizzbuzz' '1846' '1847' 'fizz' '1849'\n",
      " 'buzz' 'fizz' '1852' '1853' 'fizz' 'buzz' '1856' 'fizz' '1858' '1859'\n",
      " 'fizzbuzz' '1861' '1862' 'fizz' '1864' 'buzz' 'fizz' '1867' '1868' 'fizz'\n",
      " 'buzz' '1871' 'fizz' '1873' '1874' 'fizzbuzz' '1876' '1877' 'fizz' '1879'\n",
      " 'buzz' 'fizz' '1882' '1883' 'fizz' 'buzz' '1886' 'fizz' '1888' '1889'\n",
      " 'fizzbuzz' '1891' '1892' 'fizz' '1894' 'buzz' 'fizz' '1897' '1898' 'fizz'\n",
      " 'buzz' '1901' 'fizz' '1903' '1904' 'fizzbuzz' '1906' '1907' 'fizz' '1909'\n",
      " 'buzz' 'fizz' '1912' '1913' 'fizz' 'buzz' '1916' 'fizz' '1918' '1919'\n",
      " 'fizzbuzz' '1921' '1922' 'fizz' '1924' 'buzz' 'fizz' '1927' '1928' 'fizz'\n",
      " 'buzz' '1931' 'fizz' '1933' '1934' 'fizzbuzz' '1936' '1937' 'fizz' '1939'\n",
      " 'buzz' 'fizz' '1942' '1943' 'fizz' 'buzz' '1946' 'fizz' '1948' '1949'\n",
      " 'fizzbuzz' '1951' '1952' 'fizz' '1954' 'buzz' 'fizz' '1957' '1958' 'fizz'\n",
      " 'buzz' '1961' 'fizz' '1963' '1964' 'fizzbuzz' '1966' '1967' 'fizz' '1969'\n",
      " 'buzz' 'fizz' '1972' '1973' 'fizz' 'buzz' '1976' 'fizz' '1978' '1979'\n",
      " 'fizzbuzz' '1981' '1982' 'fizz' '1984' 'buzz' 'fizz' '1987' '1988' 'fizz'\n",
      " 'buzz' '1991' 'fizz' '1993' '1994' 'fizzbuzz' '1996' '1997' 'fizz' '1999']\n",
      "1000 to 1100's accuracy: 1.0\n",
      "['buzz' '1001' 'fizz' '1003' '1004' 'fizzbuzz' '1006' '1007' 'fizz' '1009'\n",
      " 'buzz' 'fizz' '1012' '1013' 'fizz' 'buzz' '1016' 'fizz' '1018' '1019'\n",
      " 'fizzbuzz' '1021' '1022' 'fizz' '1024' 'buzz' 'fizz' '1027' '1028' 'fizz'\n",
      " 'buzz' '1031' 'fizz' '1033' '1034' 'fizzbuzz' '1036' '1037' 'fizz' '1039'\n",
      " 'buzz' 'fizz' '1042' '1043' 'fizz' 'buzz' '1046' 'fizz' '1048' '1049'\n",
      " 'fizzbuzz' '1051' '1052' 'fizz' '1054' 'buzz' 'fizz' '1057' '1058' 'fizz'\n",
      " 'buzz' '1061' 'fizz' '1063' '1064' 'fizzbuzz' '1066' '1067' 'fizz' '1069'\n",
      " 'buzz' 'fizz' '1072' '1073' 'fizz' 'buzz' '1076' 'fizz' '1078' '1079'\n",
      " 'fizzbuzz' '1081' '1082' 'fizz' '1084' 'buzz' 'fizz' '1087' '1088' 'fizz'\n",
      " 'buzz' '1091' 'fizz' '1093' '1094' 'fizzbuzz' '1096' '1097' 'fizz' '1099']\n"
     ]
    }
   ],
   "source": [
    "with tf.Session() as sess:\n",
    "    tf.global_variables_initializer().run()\n",
    "    #进行10000轮，每一轮打乱训练数据的顺序，用mini-batch训练\n",
    "    for epoch in range(8000):\n",
    "        p = np.random.permutation(range(len(train_X)))#生成随机序列\n",
    "        train_X, train_Y = train_X[p], train_Y[p]\n",
    "        for start in range(0, len(train_X), BATCH_SIZE):\n",
    "            end = start + BATCH_SIZE\n",
    "            sess.run(train_op, feed_dict={X:train_X[start:end], Y:train_Y[start:end]})\n",
    "        if epoch % 1000 == 0 :\n",
    "            prediction = sess.run(predict,feed_dict={X:train_X,Y:train_Y})\n",
    "            print('after %d epoch,the accuracy is %f'%(epoch, np.mean(np.argmax(train_Y,axis=1) == prediction)))\n",
    "        \n",
    "        \n",
    "    test_prediction = sess.run(predict, feed_dict={X:test_X})\n",
    "    output = np.vectorize(fizz_buzz)(numbers,test_prediction)\n",
    "    print('test accuracy:',np.mean(np.argmax(test_Y,axis=1) == test_prediction))\n",
    "    print(output)\n",
    " \n",
    "\n",
    "    test_prediction2 = sess.run(predict, feed_dict={X:X_1000_2000})\n",
    "    output = np.vectorize(fizz_buzz)(numbers_1000_2000,test_prediction2)\n",
    "    print('1000 to 2000\\'s accuracy:',np.mean(np.argmax(Y_1000_2000,axis=1) == test_prediction2))\n",
    "    print(output)\n",
    "    \n",
    "    test_prediction3 = sess.run(predict, feed_dict={X:X_1000_1100})\n",
    "    output = np.vectorize(fizz_buzz)(numbers_1000_1100,test_prediction3)\n",
    "    print('1000 to 1100\\'s accuracy:',np.mean(np.argmax(Y_1000_1100,axis=1) == test_prediction3))\n",
    "    print(output)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "已解决：前边扩充了数据维度，仍然不可识别1000以上的数据？但是准确率已经1了，其实是向量化那传错了，做了广播，非fizz buzz时打印都是那个size值，其实向量化那里必须要一个整型的自然数序列，改了之后恢复正常显示。\n",
    "\n",
    "\n",
    "两个可能的因素分析：\n",
    "从数据讲，训练数据只有1024以下的，现在要预测1000以上的，确实最高位几乎就没见过1,所以也不会预测1,这个先天问题应该是不可回避的。不过这个模型并没有让网络来预测所有数字，只是预测fizzbuzz标签，一旦认为是普通数据，会打印原数值，不通过神经网络，所以没问题。\n",
    "\n",
    "神经元只有1000个，够不够？同上，只是四分类，神经元已经足够。\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 如果肉眼看打印结果，会发现fizz buzz规律极其简单，其实很容易就学到，3*5=15就是一个循环，极端点说，15个神经元就够吧。考虑到输入形式，这个想法可能太极端了，下一步再探索。\n",
    "用NUM_HIDDEN = 100替换上边的1000再重新试.1000~2000和1000~1100都是1.0的准确率，100以下是0.96的准确率。\n",
    "所以说，单层100还是不够用,非线性拟合能力不强。做成深层再试。\n",
    "\n",
    "\n",
    "1.双层100,准确率1.0\n",
    "\n",
    "2.如果要拟合非线性，ReLU好像也不划算，ReLU的优势不是体现在本例的，所以把ReLU改成sigmoid再试？实际效果惨不忍睹，看来sigmoid不能用，两层简单网络也不行。\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1947, 11)\n",
      "(1947, 4)\n",
      "(100, 11)\n"
     ]
    }
   ],
   "source": [
    "NUM_DIGITS = 11\n",
    "train_X = np.array([binary_encode(i, NUM_DIGITS) for i in range(101, 2**NUM_DIGITS)])\n",
    "train_Y = np.array([fizz_buzz_encode(i) for i in range(101, 2**NUM_DIGITS)])\n",
    "print(train_X.shape)\n",
    "print(train_Y.shape)\n",
    "\n",
    "numbers = np.arange(1,101)\n",
    "#两种写法一样\n",
    "test_X = np.array([binary_encode(i, NUM_DIGITS) for i in range(1, 101)])\n",
    "test_Y = np.array([fizz_buzz_encode(i) for i in range(1, 101)])\n",
    "\n",
    "numbers_1000_2000 = np.arange(1000,2000)\n",
    "X_1000_2000 = np.array([binary_encode(i, NUM_DIGITS) for i in range(1000, 2000)])\n",
    "Y_1000_2000 = np.array([fizz_buzz_encode(i) for i in range(1000, 2000)])\n",
    "\n",
    "numbers_1000_1100 = np.arange(1000,1100)\n",
    "X_1000_1100 = np.array([binary_encode(i, NUM_DIGITS) for i in range(1000, 1100)])\n",
    "Y_1000_1100 = np.array([fizz_buzz_encode(i) for i in range(1000, 1100)])\n",
    "print(X_1000_1100.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "H1_UNITS = 100\n",
    "H2_UNITS = 100\n",
    "BATCH_SIZE = 128\n",
    "\n",
    "X = tf.placeholder('float', [None, NUM_DIGITS])\n",
    "Y = tf.placeholder('float', [None, 4])\n",
    "def init_weights(shape):\n",
    "    return tf.Variable(tf.random_normal(shape, stddev = 0.01))\n",
    "w_h1 = init_weights([NUM_DIGITS,H1_UNITS])\n",
    "w_h2 = init_weights([H1_UNITS,H2_UNITS])\n",
    "w_o = init_weights([H2_UNITS, 4])\n",
    "def model(X, w_h1, w_h2, w_o):\n",
    "    h1 = tf.nn.relu(tf.matmul(X, w_h1))\n",
    "    h2 = tf.nn.relu(tf.matmul(h1, w_h2))\n",
    "    return tf.matmul(h2,w_o)\n",
    "\n",
    "y_ = model(X, w_h1,w_h2, w_o)\n",
    "cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits = y_, labels = Y))#强制要求带变量名。。。。。\n",
    "train_op = tf.train.GradientDescentOptimizer(0.05).minimize(cost)\n",
    "\n",
    "predict = tf.argmax(y_, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "after 0 epoch,the accuracy is 0.154083\n",
      "after 1000 epoch,the accuracy is 0.844890\n",
      "after 2000 epoch,the accuracy is 0.999486\n",
      "after 3000 epoch,the accuracy is 1.000000\n",
      "after 4000 epoch,the accuracy is 1.000000\n",
      "after 5000 epoch,the accuracy is 1.000000\n",
      "after 6000 epoch,the accuracy is 1.000000\n",
      "after 7000 epoch,the accuracy is 1.000000\n",
      "test accuracy: 1.0\n",
      "['1' '2' 'fizz' '4' 'buzz' 'fizz' '7' '8' 'fizz' 'buzz' '11' 'fizz' '13'\n",
      " '14' 'fizzbuzz' '16' '17' 'fizz' '19' 'buzz' 'fizz' '22' '23' 'fizz'\n",
      " 'buzz' '26' 'fizz' '28' '29' 'fizzbuzz' '31' '32' 'fizz' '34' 'buzz'\n",
      " 'fizz' '37' '38' 'fizz' 'buzz' '41' 'fizz' '43' '44' 'fizzbuzz' '46' '47'\n",
      " 'fizz' '49' 'buzz' 'fizz' '52' '53' 'fizz' 'buzz' '56' 'fizz' '58' '59'\n",
      " 'fizzbuzz' '61' '62' 'fizz' '64' 'buzz' 'fizz' '67' '68' 'fizz' 'buzz'\n",
      " '71' 'fizz' '73' '74' 'fizzbuzz' '76' '77' 'fizz' '79' 'buzz' 'fizz' '82'\n",
      " '83' 'fizz' 'buzz' '86' 'fizz' '88' '89' 'fizzbuzz' '91' '92' 'fizz' '94'\n",
      " 'buzz' 'fizz' '97' '98' 'fizz' 'buzz']\n",
      "1000 to 2000's accuracy: 1.0\n",
      "['buzz' '1001' 'fizz' '1003' '1004' 'fizzbuzz' '1006' '1007' 'fizz' '1009'\n",
      " 'buzz' 'fizz' '1012' '1013' 'fizz' 'buzz' '1016' 'fizz' '1018' '1019'\n",
      " 'fizzbuzz' '1021' '1022' 'fizz' '1024' 'buzz' 'fizz' '1027' '1028' 'fizz'\n",
      " 'buzz' '1031' 'fizz' '1033' '1034' 'fizzbuzz' '1036' '1037' 'fizz' '1039'\n",
      " 'buzz' 'fizz' '1042' '1043' 'fizz' 'buzz' '1046' 'fizz' '1048' '1049'\n",
      " 'fizzbuzz' '1051' '1052' 'fizz' '1054' 'buzz' 'fizz' '1057' '1058' 'fizz'\n",
      " 'buzz' '1061' 'fizz' '1063' '1064' 'fizzbuzz' '1066' '1067' 'fizz' '1069'\n",
      " 'buzz' 'fizz' '1072' '1073' 'fizz' 'buzz' '1076' 'fizz' '1078' '1079'\n",
      " 'fizzbuzz' '1081' '1082' 'fizz' '1084' 'buzz' 'fizz' '1087' '1088' 'fizz'\n",
      " 'buzz' '1091' 'fizz' '1093' '1094' 'fizzbuzz' '1096' '1097' 'fizz' '1099'\n",
      " 'buzz' 'fizz' '1102' '1103' 'fizz' 'buzz' '1106' 'fizz' '1108' '1109'\n",
      " 'fizzbuzz' '1111' '1112' 'fizz' '1114' 'buzz' 'fizz' '1117' '1118' 'fizz'\n",
      " 'buzz' '1121' 'fizz' '1123' '1124' 'fizzbuzz' '1126' '1127' 'fizz' '1129'\n",
      " 'buzz' 'fizz' '1132' '1133' 'fizz' 'buzz' '1136' 'fizz' '1138' '1139'\n",
      " 'fizzbuzz' '1141' '1142' 'fizz' '1144' 'buzz' 'fizz' '1147' '1148' 'fizz'\n",
      " 'buzz' '1151' 'fizz' '1153' '1154' 'fizzbuzz' '1156' '1157' 'fizz' '1159'\n",
      " 'buzz' 'fizz' '1162' '1163' 'fizz' 'buzz' '1166' 'fizz' '1168' '1169'\n",
      " 'fizzbuzz' '1171' '1172' 'fizz' '1174' 'buzz' 'fizz' '1177' '1178' 'fizz'\n",
      " 'buzz' '1181' 'fizz' '1183' '1184' 'fizzbuzz' '1186' '1187' 'fizz' '1189'\n",
      " 'buzz' 'fizz' '1192' '1193' 'fizz' 'buzz' '1196' 'fizz' '1198' '1199'\n",
      " 'fizzbuzz' '1201' '1202' 'fizz' '1204' 'buzz' 'fizz' '1207' '1208' 'fizz'\n",
      " 'buzz' '1211' 'fizz' '1213' '1214' 'fizzbuzz' '1216' '1217' 'fizz' '1219'\n",
      " 'buzz' 'fizz' '1222' '1223' 'fizz' 'buzz' '1226' 'fizz' '1228' '1229'\n",
      " 'fizzbuzz' '1231' '1232' 'fizz' '1234' 'buzz' 'fizz' '1237' '1238' 'fizz'\n",
      " 'buzz' '1241' 'fizz' '1243' '1244' 'fizzbuzz' '1246' '1247' 'fizz' '1249'\n",
      " 'buzz' 'fizz' '1252' '1253' 'fizz' 'buzz' '1256' 'fizz' '1258' '1259'\n",
      " 'fizzbuzz' '1261' '1262' 'fizz' '1264' 'buzz' 'fizz' '1267' '1268' 'fizz'\n",
      " 'buzz' '1271' 'fizz' '1273' '1274' 'fizzbuzz' '1276' '1277' 'fizz' '1279'\n",
      " 'buzz' 'fizz' '1282' '1283' 'fizz' 'buzz' '1286' 'fizz' '1288' '1289'\n",
      " 'fizzbuzz' '1291' '1292' 'fizz' '1294' 'buzz' 'fizz' '1297' '1298' 'fizz'\n",
      " 'buzz' '1301' 'fizz' '1303' '1304' 'fizzbuzz' '1306' '1307' 'fizz' '1309'\n",
      " 'buzz' 'fizz' '1312' '1313' 'fizz' 'buzz' '1316' 'fizz' '1318' '1319'\n",
      " 'fizzbuzz' '1321' '1322' 'fizz' '1324' 'buzz' 'fizz' '1327' '1328' 'fizz'\n",
      " 'buzz' '1331' 'fizz' '1333' '1334' 'fizzbuzz' '1336' '1337' 'fizz' '1339'\n",
      " 'buzz' 'fizz' '1342' '1343' 'fizz' 'buzz' '1346' 'fizz' '1348' '1349'\n",
      " 'fizzbuzz' '1351' '1352' 'fizz' '1354' 'buzz' 'fizz' '1357' '1358' 'fizz'\n",
      " 'buzz' '1361' 'fizz' '1363' '1364' 'fizzbuzz' '1366' '1367' 'fizz' '1369'\n",
      " 'buzz' 'fizz' '1372' '1373' 'fizz' 'buzz' '1376' 'fizz' '1378' '1379'\n",
      " 'fizzbuzz' '1381' '1382' 'fizz' '1384' 'buzz' 'fizz' '1387' '1388' 'fizz'\n",
      " 'buzz' '1391' 'fizz' '1393' '1394' 'fizzbuzz' '1396' '1397' 'fizz' '1399'\n",
      " 'buzz' 'fizz' '1402' '1403' 'fizz' 'buzz' '1406' 'fizz' '1408' '1409'\n",
      " 'fizzbuzz' '1411' '1412' 'fizz' '1414' 'buzz' 'fizz' '1417' '1418' 'fizz'\n",
      " 'buzz' '1421' 'fizz' '1423' '1424' 'fizzbuzz' '1426' '1427' 'fizz' '1429'\n",
      " 'buzz' 'fizz' '1432' '1433' 'fizz' 'buzz' '1436' 'fizz' '1438' '1439'\n",
      " 'fizzbuzz' '1441' '1442' 'fizz' '1444' 'buzz' 'fizz' '1447' '1448' 'fizz'\n",
      " 'buzz' '1451' 'fizz' '1453' '1454' 'fizzbuzz' '1456' '1457' 'fizz' '1459'\n",
      " 'buzz' 'fizz' '1462' '1463' 'fizz' 'buzz' '1466' 'fizz' '1468' '1469'\n",
      " 'fizzbuzz' '1471' '1472' 'fizz' '1474' 'buzz' 'fizz' '1477' '1478' 'fizz'\n",
      " 'buzz' '1481' 'fizz' '1483' '1484' 'fizzbuzz' '1486' '1487' 'fizz' '1489'\n",
      " 'buzz' 'fizz' '1492' '1493' 'fizz' 'buzz' '1496' 'fizz' '1498' '1499'\n",
      " 'fizzbuzz' '1501' '1502' 'fizz' '1504' 'buzz' 'fizz' '1507' '1508' 'fizz'\n",
      " 'buzz' '1511' 'fizz' '1513' '1514' 'fizzbuzz' '1516' '1517' 'fizz' '1519'\n",
      " 'buzz' 'fizz' '1522' '1523' 'fizz' 'buzz' '1526' 'fizz' '1528' '1529'\n",
      " 'fizzbuzz' '1531' '1532' 'fizz' '1534' 'buzz' 'fizz' '1537' '1538' 'fizz'\n",
      " 'buzz' '1541' 'fizz' '1543' '1544' 'fizzbuzz' '1546' '1547' 'fizz' '1549'\n",
      " 'buzz' 'fizz' '1552' '1553' 'fizz' 'buzz' '1556' 'fizz' '1558' '1559'\n",
      " 'fizzbuzz' '1561' '1562' 'fizz' '1564' 'buzz' 'fizz' '1567' '1568' 'fizz'\n",
      " 'buzz' '1571' 'fizz' '1573' '1574' 'fizzbuzz' '1576' '1577' 'fizz' '1579'\n",
      " 'buzz' 'fizz' '1582' '1583' 'fizz' 'buzz' '1586' 'fizz' '1588' '1589'\n",
      " 'fizzbuzz' '1591' '1592' 'fizz' '1594' 'buzz' 'fizz' '1597' '1598' 'fizz'\n",
      " 'buzz' '1601' 'fizz' '1603' '1604' 'fizzbuzz' '1606' '1607' 'fizz' '1609'\n",
      " 'buzz' 'fizz' '1612' '1613' 'fizz' 'buzz' '1616' 'fizz' '1618' '1619'\n",
      " 'fizzbuzz' '1621' '1622' 'fizz' '1624' 'buzz' 'fizz' '1627' '1628' 'fizz'\n",
      " 'buzz' '1631' 'fizz' '1633' '1634' 'fizzbuzz' '1636' '1637' 'fizz' '1639'\n",
      " 'buzz' 'fizz' '1642' '1643' 'fizz' 'buzz' '1646' 'fizz' '1648' '1649'\n",
      " 'fizzbuzz' '1651' '1652' 'fizz' '1654' 'buzz' 'fizz' '1657' '1658' 'fizz'\n",
      " 'buzz' '1661' 'fizz' '1663' '1664' 'fizzbuzz' '1666' '1667' 'fizz' '1669'\n",
      " 'buzz' 'fizz' '1672' '1673' 'fizz' 'buzz' '1676' 'fizz' '1678' '1679'\n",
      " 'fizzbuzz' '1681' '1682' 'fizz' '1684' 'buzz' 'fizz' '1687' '1688' 'fizz'\n",
      " 'buzz' '1691' 'fizz' '1693' '1694' 'fizzbuzz' '1696' '1697' 'fizz' '1699'\n",
      " 'buzz' 'fizz' '1702' '1703' 'fizz' 'buzz' '1706' 'fizz' '1708' '1709'\n",
      " 'fizzbuzz' '1711' '1712' 'fizz' '1714' 'buzz' 'fizz' '1717' '1718' 'fizz'\n",
      " 'buzz' '1721' 'fizz' '1723' '1724' 'fizzbuzz' '1726' '1727' 'fizz' '1729'\n",
      " 'buzz' 'fizz' '1732' '1733' 'fizz' 'buzz' '1736' 'fizz' '1738' '1739'\n",
      " 'fizzbuzz' '1741' '1742' 'fizz' '1744' 'buzz' 'fizz' '1747' '1748' 'fizz'\n",
      " 'buzz' '1751' 'fizz' '1753' '1754' 'fizzbuzz' '1756' '1757' 'fizz' '1759'\n",
      " 'buzz' 'fizz' '1762' '1763' 'fizz' 'buzz' '1766' 'fizz' '1768' '1769'\n",
      " 'fizzbuzz' '1771' '1772' 'fizz' '1774' 'buzz' 'fizz' '1777' '1778' 'fizz'\n",
      " 'buzz' '1781' 'fizz' '1783' '1784' 'fizzbuzz' '1786' '1787' 'fizz' '1789'\n",
      " 'buzz' 'fizz' '1792' '1793' 'fizz' 'buzz' '1796' 'fizz' '1798' '1799'\n",
      " 'fizzbuzz' '1801' '1802' 'fizz' '1804' 'buzz' 'fizz' '1807' '1808' 'fizz'\n",
      " 'buzz' '1811' 'fizz' '1813' '1814' 'fizzbuzz' '1816' '1817' 'fizz' '1819'\n",
      " 'buzz' 'fizz' '1822' '1823' 'fizz' 'buzz' '1826' 'fizz' '1828' '1829'\n",
      " 'fizzbuzz' '1831' '1832' 'fizz' '1834' 'buzz' 'fizz' '1837' '1838' 'fizz'\n",
      " 'buzz' '1841' 'fizz' '1843' '1844' 'fizzbuzz' '1846' '1847' 'fizz' '1849'\n",
      " 'buzz' 'fizz' '1852' '1853' 'fizz' 'buzz' '1856' 'fizz' '1858' '1859'\n",
      " 'fizzbuzz' '1861' '1862' 'fizz' '1864' 'buzz' 'fizz' '1867' '1868' 'fizz'\n",
      " 'buzz' '1871' 'fizz' '1873' '1874' 'fizzbuzz' '1876' '1877' 'fizz' '1879'\n",
      " 'buzz' 'fizz' '1882' '1883' 'fizz' 'buzz' '1886' 'fizz' '1888' '1889'\n",
      " 'fizzbuzz' '1891' '1892' 'fizz' '1894' 'buzz' 'fizz' '1897' '1898' 'fizz'\n",
      " 'buzz' '1901' 'fizz' '1903' '1904' 'fizzbuzz' '1906' '1907' 'fizz' '1909'\n",
      " 'buzz' 'fizz' '1912' '1913' 'fizz' 'buzz' '1916' 'fizz' '1918' '1919'\n",
      " 'fizzbuzz' '1921' '1922' 'fizz' '1924' 'buzz' 'fizz' '1927' '1928' 'fizz'\n",
      " 'buzz' '1931' 'fizz' '1933' '1934' 'fizzbuzz' '1936' '1937' 'fizz' '1939'\n",
      " 'buzz' 'fizz' '1942' '1943' 'fizz' 'buzz' '1946' 'fizz' '1948' '1949'\n",
      " 'fizzbuzz' '1951' '1952' 'fizz' '1954' 'buzz' 'fizz' '1957' '1958' 'fizz'\n",
      " 'buzz' '1961' 'fizz' '1963' '1964' 'fizzbuzz' '1966' '1967' 'fizz' '1969'\n",
      " 'buzz' 'fizz' '1972' '1973' 'fizz' 'buzz' '1976' 'fizz' '1978' '1979'\n",
      " 'fizzbuzz' '1981' '1982' 'fizz' '1984' 'buzz' 'fizz' '1987' '1988' 'fizz'\n",
      " 'buzz' '1991' 'fizz' '1993' '1994' 'fizzbuzz' '1996' '1997' 'fizz' '1999']\n",
      "1000 to 1100's accuracy: 1.0\n",
      "['buzz' '1001' 'fizz' '1003' '1004' 'fizzbuzz' '1006' '1007' 'fizz' '1009'\n",
      " 'buzz' 'fizz' '1012' '1013' 'fizz' 'buzz' '1016' 'fizz' '1018' '1019'\n",
      " 'fizzbuzz' '1021' '1022' 'fizz' '1024' 'buzz' 'fizz' '1027' '1028' 'fizz'\n",
      " 'buzz' '1031' 'fizz' '1033' '1034' 'fizzbuzz' '1036' '1037' 'fizz' '1039'\n",
      " 'buzz' 'fizz' '1042' '1043' 'fizz' 'buzz' '1046' 'fizz' '1048' '1049'\n",
      " 'fizzbuzz' '1051' '1052' 'fizz' '1054' 'buzz' 'fizz' '1057' '1058' 'fizz'\n",
      " 'buzz' '1061' 'fizz' '1063' '1064' 'fizzbuzz' '1066' '1067' 'fizz' '1069'\n",
      " 'buzz' 'fizz' '1072' '1073' 'fizz' 'buzz' '1076' 'fizz' '1078' '1079'\n",
      " 'fizzbuzz' '1081' '1082' 'fizz' '1084' 'buzz' 'fizz' '1087' '1088' 'fizz'\n",
      " 'buzz' '1091' 'fizz' '1093' '1094' 'fizzbuzz' '1096' '1097' 'fizz' '1099']\n"
     ]
    }
   ],
   "source": [
    "with tf.Session() as sess:\n",
    "    tf.global_variables_initializer().run()\n",
    "    #进行10000轮，每一轮打乱训练数据的顺序，用mini-batch训练\n",
    "    for epoch in range(8000):\n",
    "        p = np.random.permutation(range(len(train_X)))#生成随机序列\n",
    "        train_X, train_Y = train_X[p], train_Y[p]\n",
    "        for start in range(0, len(train_X), BATCH_SIZE):\n",
    "            end = start + BATCH_SIZE\n",
    "            sess.run(train_op, feed_dict={X:train_X[start:end], Y:train_Y[start:end]})\n",
    "        if epoch % 1000 == 0 :\n",
    "            prediction = sess.run(predict,feed_dict={X:train_X,Y:train_Y})\n",
    "            print('after %d epoch,the accuracy is %f'%(epoch, np.mean(np.argmax(train_Y,axis=1) == prediction)))\n",
    "        \n",
    "        \n",
    "    test_prediction = sess.run(predict, feed_dict={X:test_X})\n",
    "    output = np.vectorize(fizz_buzz)(numbers,test_prediction)\n",
    "    print('test accuracy:',np.mean(np.argmax(test_Y,axis=1) == test_prediction))\n",
    "    print(output)\n",
    " \n",
    "\n",
    "    test_prediction2 = sess.run(predict, feed_dict={X:X_1000_2000})\n",
    "    output = np.vectorize(fizz_buzz)(numbers_1000_2000,test_prediction2)\n",
    "    print('1000 to 2000\\'s accuracy:',np.mean(np.argmax(Y_1000_2000,axis=1) == test_prediction2))\n",
    "    print(output)\n",
    "    \n",
    "    test_prediction3 = sess.run(predict, feed_dict={X:X_1000_1100})\n",
    "    output = np.vectorize(fizz_buzz)(numbers_1000_1100,test_prediction3)\n",
    "    print('1000 to 1100\\'s accuracy:',np.mean(np.argmax(Y_1000_1100,axis=1) == test_prediction3))\n",
    "    print(output)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 更深的网络，更少的参数：\n",
    "\n",
    "三个隐藏层，都是15,总共45个神经元，参数量：\n",
    "11*15+15*15+15*15+15*4=225*3=675\n",
    "预测情况，1~100最差0.76，1000~1100最好0.99（不光和数据集近，还存在部分交集），1000~1100其次0.968\n",
    "\n",
    "如果用最粗暴的算法：11个input，15个hidden units，4个output，总共就15个神经元，参数个数：\n",
    "11*15+15*4=165+60=225。\n",
    "不过太极端了实际训练不出来，因为我也并没有使用饱和数据0~2047,所以算是双方打平，trade-off吧。\n",
    "所以，这也是深度学习的一个基本现状，数据不是饱和的，网络也不可能按照能够记住所有答案的底线来设计。\n",
    "\n",
    "\n",
    "对比前一个网络，两层100，参数量：\n",
    "11*100 + 100*100 + 100*4，是一万的数量级。\n",
    "\n",
    "结论：三层15效果还算不错，看以什么为目的，如果是追求100%预测，用来生成fizzbuzz，其实不如两层100,但是参数量什么的都要小。而且15也可以扩展到20或者30,随便测了一个25,已经全是1.0的准确率了。\n",
    "11*25 + 25*25 + 25*25 + 25*4 = 25*(65) = 1625,相比两层一万还是有很多优势（两层一定得用100个神经元么？也不一定，具体两层需要多少个，我不测了，参数肯定要比一千六大的）\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "#第一次，三层15\n",
    "H1_UNITS = 15\n",
    "H2_UNITS = 15\n",
    "H3_UNITS = 15\n",
    "#第二次,三层25\n",
    "H1_UNITS = 25\n",
    "H2_UNITS = 25\n",
    "H3_UNITS = 25\n",
    "\n",
    "BATCH_SIZE = 128\n",
    "\n",
    "X = tf.placeholder('float', [None, NUM_DIGITS])\n",
    "Y = tf.placeholder('float', [None, 4])\n",
    "def init_weights(shape):\n",
    "    return tf.Variable(tf.random_normal(shape, stddev = 0.01))\n",
    "w_h1 = init_weights([NUM_DIGITS,H1_UNITS])\n",
    "w_h2 = init_weights([H1_UNITS,H2_UNITS])\n",
    "w_h3 = init_weights([H2_UNITS,H3_UNITS])\n",
    "w_o = init_weights([H3_UNITS, 4])\n",
    "\n",
    "h1 = tf.nn.relu(tf.matmul(X, w_h1))\n",
    "h2 = tf.nn.relu(tf.matmul(h1, w_h2))\n",
    "h3 = tf.nn.relu(tf.matmul(h2, w_h3))\n",
    "y_ = tf.matmul(h3,w_o)\n",
    "\n",
    "cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits = y_, labels = Y))#强制要求带变量名。。。。。\n",
    "train_op = tf.train.GradientDescentOptimizer(0.05).minimize(cost)\n",
    "\n",
    "predict = tf.argmax(y_, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "after 0 epoch,the accuracy is 0.202876\n",
      "after 1000 epoch,the accuracy is 0.533641\n",
      "after 2000 epoch,the accuracy is 0.533641\n",
      "after 3000 epoch,the accuracy is 0.611710\n",
      "after 4000 epoch,the accuracy is 1.000000\n",
      "after 5000 epoch,the accuracy is 1.000000\n",
      "after 6000 epoch,the accuracy is 1.000000\n",
      "after 7000 epoch,the accuracy is 1.000000\n",
      "after 8000 epoch,the accuracy is 1.000000\n",
      "after 9000 epoch,the accuracy is 1.000000\n",
      "test accuracy: 1.0\n",
      "['1' '2' 'fizz' '4' 'buzz' 'fizz' '7' '8' 'fizz' 'buzz' '11' 'fizz' '13'\n",
      " '14' 'fizzbuzz' '16' '17' 'fizz' '19' 'buzz' 'fizz' '22' '23' 'fizz'\n",
      " 'buzz' '26' 'fizz' '28' '29' 'fizzbuzz' '31' '32' 'fizz' '34' 'buzz'\n",
      " 'fizz' '37' '38' 'fizz' 'buzz' '41' 'fizz' '43' '44' 'fizzbuzz' '46' '47'\n",
      " 'fizz' '49' 'buzz' 'fizz' '52' '53' 'fizz' 'buzz' '56' 'fizz' '58' '59'\n",
      " 'fizzbuzz' '61' '62' 'fizz' '64' 'buzz' 'fizz' '67' '68' 'fizz' 'buzz'\n",
      " '71' 'fizz' '73' '74' 'fizzbuzz' '76' '77' 'fizz' '79' 'buzz' 'fizz' '82'\n",
      " '83' 'fizz' 'buzz' '86' 'fizz' '88' '89' 'fizzbuzz' '91' '92' 'fizz' '94'\n",
      " 'buzz' 'fizz' '97' '98' 'fizz' 'buzz']\n",
      "1000 to 2000's accuracy: 1.0\n",
      "['buzz' '1001' 'fizz' '1003' '1004' 'fizzbuzz' '1006' '1007' 'fizz' '1009'\n",
      " 'buzz' 'fizz' '1012' '1013' 'fizz' 'buzz' '1016' 'fizz' '1018' '1019'\n",
      " 'fizzbuzz' '1021' '1022' 'fizz' '1024' 'buzz' 'fizz' '1027' '1028' 'fizz'\n",
      " 'buzz' '1031' 'fizz' '1033' '1034' 'fizzbuzz' '1036' '1037' 'fizz' '1039'\n",
      " 'buzz' 'fizz' '1042' '1043' 'fizz' 'buzz' '1046' 'fizz' '1048' '1049'\n",
      " 'fizzbuzz' '1051' '1052' 'fizz' '1054' 'buzz' 'fizz' '1057' '1058' 'fizz'\n",
      " 'buzz' '1061' 'fizz' '1063' '1064' 'fizzbuzz' '1066' '1067' 'fizz' '1069'\n",
      " 'buzz' 'fizz' '1072' '1073' 'fizz' 'buzz' '1076' 'fizz' '1078' '1079'\n",
      " 'fizzbuzz' '1081' '1082' 'fizz' '1084' 'buzz' 'fizz' '1087' '1088' 'fizz'\n",
      " 'buzz' '1091' 'fizz' '1093' '1094' 'fizzbuzz' '1096' '1097' 'fizz' '1099'\n",
      " 'buzz' 'fizz' '1102' '1103' 'fizz' 'buzz' '1106' 'fizz' '1108' '1109'\n",
      " 'fizzbuzz' '1111' '1112' 'fizz' '1114' 'buzz' 'fizz' '1117' '1118' 'fizz'\n",
      " 'buzz' '1121' 'fizz' '1123' '1124' 'fizzbuzz' '1126' '1127' 'fizz' '1129'\n",
      " 'buzz' 'fizz' '1132' '1133' 'fizz' 'buzz' '1136' 'fizz' '1138' '1139'\n",
      " 'fizzbuzz' '1141' '1142' 'fizz' '1144' 'buzz' 'fizz' '1147' '1148' 'fizz'\n",
      " 'buzz' '1151' 'fizz' '1153' '1154' 'fizzbuzz' '1156' '1157' 'fizz' '1159'\n",
      " 'buzz' 'fizz' '1162' '1163' 'fizz' 'buzz' '1166' 'fizz' '1168' '1169'\n",
      " 'fizzbuzz' '1171' '1172' 'fizz' '1174' 'buzz' 'fizz' '1177' '1178' 'fizz'\n",
      " 'buzz' '1181' 'fizz' '1183' '1184' 'fizzbuzz' '1186' '1187' 'fizz' '1189'\n",
      " 'buzz' 'fizz' '1192' '1193' 'fizz' 'buzz' '1196' 'fizz' '1198' '1199'\n",
      " 'fizzbuzz' '1201' '1202' 'fizz' '1204' 'buzz' 'fizz' '1207' '1208' 'fizz'\n",
      " 'buzz' '1211' 'fizz' '1213' '1214' 'fizzbuzz' '1216' '1217' 'fizz' '1219'\n",
      " 'buzz' 'fizz' '1222' '1223' 'fizz' 'buzz' '1226' 'fizz' '1228' '1229'\n",
      " 'fizzbuzz' '1231' '1232' 'fizz' '1234' 'buzz' 'fizz' '1237' '1238' 'fizz'\n",
      " 'buzz' '1241' 'fizz' '1243' '1244' 'fizzbuzz' '1246' '1247' 'fizz' '1249'\n",
      " 'buzz' 'fizz' '1252' '1253' 'fizz' 'buzz' '1256' 'fizz' '1258' '1259'\n",
      " 'fizzbuzz' '1261' '1262' 'fizz' '1264' 'buzz' 'fizz' '1267' '1268' 'fizz'\n",
      " 'buzz' '1271' 'fizz' '1273' '1274' 'fizzbuzz' '1276' '1277' 'fizz' '1279'\n",
      " 'buzz' 'fizz' '1282' '1283' 'fizz' 'buzz' '1286' 'fizz' '1288' '1289'\n",
      " 'fizzbuzz' '1291' '1292' 'fizz' '1294' 'buzz' 'fizz' '1297' '1298' 'fizz'\n",
      " 'buzz' '1301' 'fizz' '1303' '1304' 'fizzbuzz' '1306' '1307' 'fizz' '1309'\n",
      " 'buzz' 'fizz' '1312' '1313' 'fizz' 'buzz' '1316' 'fizz' '1318' '1319'\n",
      " 'fizzbuzz' '1321' '1322' 'fizz' '1324' 'buzz' 'fizz' '1327' '1328' 'fizz'\n",
      " 'buzz' '1331' 'fizz' '1333' '1334' 'fizzbuzz' '1336' '1337' 'fizz' '1339'\n",
      " 'buzz' 'fizz' '1342' '1343' 'fizz' 'buzz' '1346' 'fizz' '1348' '1349'\n",
      " 'fizzbuzz' '1351' '1352' 'fizz' '1354' 'buzz' 'fizz' '1357' '1358' 'fizz'\n",
      " 'buzz' '1361' 'fizz' '1363' '1364' 'fizzbuzz' '1366' '1367' 'fizz' '1369'\n",
      " 'buzz' 'fizz' '1372' '1373' 'fizz' 'buzz' '1376' 'fizz' '1378' '1379'\n",
      " 'fizzbuzz' '1381' '1382' 'fizz' '1384' 'buzz' 'fizz' '1387' '1388' 'fizz'\n",
      " 'buzz' '1391' 'fizz' '1393' '1394' 'fizzbuzz' '1396' '1397' 'fizz' '1399'\n",
      " 'buzz' 'fizz' '1402' '1403' 'fizz' 'buzz' '1406' 'fizz' '1408' '1409'\n",
      " 'fizzbuzz' '1411' '1412' 'fizz' '1414' 'buzz' 'fizz' '1417' '1418' 'fizz'\n",
      " 'buzz' '1421' 'fizz' '1423' '1424' 'fizzbuzz' '1426' '1427' 'fizz' '1429'\n",
      " 'buzz' 'fizz' '1432' '1433' 'fizz' 'buzz' '1436' 'fizz' '1438' '1439'\n",
      " 'fizzbuzz' '1441' '1442' 'fizz' '1444' 'buzz' 'fizz' '1447' '1448' 'fizz'\n",
      " 'buzz' '1451' 'fizz' '1453' '1454' 'fizzbuzz' '1456' '1457' 'fizz' '1459'\n",
      " 'buzz' 'fizz' '1462' '1463' 'fizz' 'buzz' '1466' 'fizz' '1468' '1469'\n",
      " 'fizzbuzz' '1471' '1472' 'fizz' '1474' 'buzz' 'fizz' '1477' '1478' 'fizz'\n",
      " 'buzz' '1481' 'fizz' '1483' '1484' 'fizzbuzz' '1486' '1487' 'fizz' '1489'\n",
      " 'buzz' 'fizz' '1492' '1493' 'fizz' 'buzz' '1496' 'fizz' '1498' '1499'\n",
      " 'fizzbuzz' '1501' '1502' 'fizz' '1504' 'buzz' 'fizz' '1507' '1508' 'fizz'\n",
      " 'buzz' '1511' 'fizz' '1513' '1514' 'fizzbuzz' '1516' '1517' 'fizz' '1519'\n",
      " 'buzz' 'fizz' '1522' '1523' 'fizz' 'buzz' '1526' 'fizz' '1528' '1529'\n",
      " 'fizzbuzz' '1531' '1532' 'fizz' '1534' 'buzz' 'fizz' '1537' '1538' 'fizz'\n",
      " 'buzz' '1541' 'fizz' '1543' '1544' 'fizzbuzz' '1546' '1547' 'fizz' '1549'\n",
      " 'buzz' 'fizz' '1552' '1553' 'fizz' 'buzz' '1556' 'fizz' '1558' '1559'\n",
      " 'fizzbuzz' '1561' '1562' 'fizz' '1564' 'buzz' 'fizz' '1567' '1568' 'fizz'\n",
      " 'buzz' '1571' 'fizz' '1573' '1574' 'fizzbuzz' '1576' '1577' 'fizz' '1579'\n",
      " 'buzz' 'fizz' '1582' '1583' 'fizz' 'buzz' '1586' 'fizz' '1588' '1589'\n",
      " 'fizzbuzz' '1591' '1592' 'fizz' '1594' 'buzz' 'fizz' '1597' '1598' 'fizz'\n",
      " 'buzz' '1601' 'fizz' '1603' '1604' 'fizzbuzz' '1606' '1607' 'fizz' '1609'\n",
      " 'buzz' 'fizz' '1612' '1613' 'fizz' 'buzz' '1616' 'fizz' '1618' '1619'\n",
      " 'fizzbuzz' '1621' '1622' 'fizz' '1624' 'buzz' 'fizz' '1627' '1628' 'fizz'\n",
      " 'buzz' '1631' 'fizz' '1633' '1634' 'fizzbuzz' '1636' '1637' 'fizz' '1639'\n",
      " 'buzz' 'fizz' '1642' '1643' 'fizz' 'buzz' '1646' 'fizz' '1648' '1649'\n",
      " 'fizzbuzz' '1651' '1652' 'fizz' '1654' 'buzz' 'fizz' '1657' '1658' 'fizz'\n",
      " 'buzz' '1661' 'fizz' '1663' '1664' 'fizzbuzz' '1666' '1667' 'fizz' '1669'\n",
      " 'buzz' 'fizz' '1672' '1673' 'fizz' 'buzz' '1676' 'fizz' '1678' '1679'\n",
      " 'fizzbuzz' '1681' '1682' 'fizz' '1684' 'buzz' 'fizz' '1687' '1688' 'fizz'\n",
      " 'buzz' '1691' 'fizz' '1693' '1694' 'fizzbuzz' '1696' '1697' 'fizz' '1699'\n",
      " 'buzz' 'fizz' '1702' '1703' 'fizz' 'buzz' '1706' 'fizz' '1708' '1709'\n",
      " 'fizzbuzz' '1711' '1712' 'fizz' '1714' 'buzz' 'fizz' '1717' '1718' 'fizz'\n",
      " 'buzz' '1721' 'fizz' '1723' '1724' 'fizzbuzz' '1726' '1727' 'fizz' '1729'\n",
      " 'buzz' 'fizz' '1732' '1733' 'fizz' 'buzz' '1736' 'fizz' '1738' '1739'\n",
      " 'fizzbuzz' '1741' '1742' 'fizz' '1744' 'buzz' 'fizz' '1747' '1748' 'fizz'\n",
      " 'buzz' '1751' 'fizz' '1753' '1754' 'fizzbuzz' '1756' '1757' 'fizz' '1759'\n",
      " 'buzz' 'fizz' '1762' '1763' 'fizz' 'buzz' '1766' 'fizz' '1768' '1769'\n",
      " 'fizzbuzz' '1771' '1772' 'fizz' '1774' 'buzz' 'fizz' '1777' '1778' 'fizz'\n",
      " 'buzz' '1781' 'fizz' '1783' '1784' 'fizzbuzz' '1786' '1787' 'fizz' '1789'\n",
      " 'buzz' 'fizz' '1792' '1793' 'fizz' 'buzz' '1796' 'fizz' '1798' '1799'\n",
      " 'fizzbuzz' '1801' '1802' 'fizz' '1804' 'buzz' 'fizz' '1807' '1808' 'fizz'\n",
      " 'buzz' '1811' 'fizz' '1813' '1814' 'fizzbuzz' '1816' '1817' 'fizz' '1819'\n",
      " 'buzz' 'fizz' '1822' '1823' 'fizz' 'buzz' '1826' 'fizz' '1828' '1829'\n",
      " 'fizzbuzz' '1831' '1832' 'fizz' '1834' 'buzz' 'fizz' '1837' '1838' 'fizz'\n",
      " 'buzz' '1841' 'fizz' '1843' '1844' 'fizzbuzz' '1846' '1847' 'fizz' '1849'\n",
      " 'buzz' 'fizz' '1852' '1853' 'fizz' 'buzz' '1856' 'fizz' '1858' '1859'\n",
      " 'fizzbuzz' '1861' '1862' 'fizz' '1864' 'buzz' 'fizz' '1867' '1868' 'fizz'\n",
      " 'buzz' '1871' 'fizz' '1873' '1874' 'fizzbuzz' '1876' '1877' 'fizz' '1879'\n",
      " 'buzz' 'fizz' '1882' '1883' 'fizz' 'buzz' '1886' 'fizz' '1888' '1889'\n",
      " 'fizzbuzz' '1891' '1892' 'fizz' '1894' 'buzz' 'fizz' '1897' '1898' 'fizz'\n",
      " 'buzz' '1901' 'fizz' '1903' '1904' 'fizzbuzz' '1906' '1907' 'fizz' '1909'\n",
      " 'buzz' 'fizz' '1912' '1913' 'fizz' 'buzz' '1916' 'fizz' '1918' '1919'\n",
      " 'fizzbuzz' '1921' '1922' 'fizz' '1924' 'buzz' 'fizz' '1927' '1928' 'fizz'\n",
      " 'buzz' '1931' 'fizz' '1933' '1934' 'fizzbuzz' '1936' '1937' 'fizz' '1939'\n",
      " 'buzz' 'fizz' '1942' '1943' 'fizz' 'buzz' '1946' 'fizz' '1948' '1949'\n",
      " 'fizzbuzz' '1951' '1952' 'fizz' '1954' 'buzz' 'fizz' '1957' '1958' 'fizz'\n",
      " 'buzz' '1961' 'fizz' '1963' '1964' 'fizzbuzz' '1966' '1967' 'fizz' '1969'\n",
      " 'buzz' 'fizz' '1972' '1973' 'fizz' 'buzz' '1976' 'fizz' '1978' '1979'\n",
      " 'fizzbuzz' '1981' '1982' 'fizz' '1984' 'buzz' 'fizz' '1987' '1988' 'fizz'\n",
      " 'buzz' '1991' 'fizz' '1993' '1994' 'fizzbuzz' '1996' '1997' 'fizz' '1999']\n",
      "1000 to 1100's accuracy: 1.0\n",
      "['buzz' '1001' 'fizz' '1003' '1004' 'fizzbuzz' '1006' '1007' 'fizz' '1009'\n",
      " 'buzz' 'fizz' '1012' '1013' 'fizz' 'buzz' '1016' 'fizz' '1018' '1019'\n",
      " 'fizzbuzz' '1021' '1022' 'fizz' '1024' 'buzz' 'fizz' '1027' '1028' 'fizz'\n",
      " 'buzz' '1031' 'fizz' '1033' '1034' 'fizzbuzz' '1036' '1037' 'fizz' '1039'\n",
      " 'buzz' 'fizz' '1042' '1043' 'fizz' 'buzz' '1046' 'fizz' '1048' '1049'\n",
      " 'fizzbuzz' '1051' '1052' 'fizz' '1054' 'buzz' 'fizz' '1057' '1058' 'fizz'\n",
      " 'buzz' '1061' 'fizz' '1063' '1064' 'fizzbuzz' '1066' '1067' 'fizz' '1069'\n",
      " 'buzz' 'fizz' '1072' '1073' 'fizz' 'buzz' '1076' 'fizz' '1078' '1079'\n",
      " 'fizzbuzz' '1081' '1082' 'fizz' '1084' 'buzz' 'fizz' '1087' '1088' 'fizz'\n",
      " 'buzz' '1091' 'fizz' '1093' '1094' 'fizzbuzz' '1096' '1097' 'fizz' '1099']\n"
     ]
    }
   ],
   "source": [
    "with tf.Session() as sess:\n",
    "    tf.global_variables_initializer().run()\n",
    "    #进行10000轮，每一轮打乱训练数据的顺序，用mini-batch训练\n",
    "    for epoch in range(10000):\n",
    "        p = np.random.permutation(range(len(train_X)))#生成随机序列\n",
    "        train_X, train_Y = train_X[p], train_Y[p]\n",
    "        for start in range(0, len(train_X), BATCH_SIZE):\n",
    "            end = start + BATCH_SIZE\n",
    "            sess.run(train_op, feed_dict={X:train_X[start:end], Y:train_Y[start:end]})\n",
    "        if epoch % 1000 == 0 :\n",
    "            prediction = sess.run(predict,feed_dict={X:train_X,Y:train_Y})\n",
    "            print('after %d epoch,the accuracy is %f'%(epoch, np.mean(np.argmax(train_Y,axis=1) == prediction)))\n",
    "        \n",
    "        \n",
    "    test_prediction = sess.run(predict, feed_dict={X:test_X})\n",
    "    output = np.vectorize(fizz_buzz)(numbers,test_prediction)\n",
    "    print('test accuracy:',np.mean(np.argmax(test_Y,axis=1) == test_prediction))\n",
    "    print(output)\n",
    " \n",
    "\n",
    "    test_prediction2 = sess.run(predict, feed_dict={X:X_1000_2000})\n",
    "    output = np.vectorize(fizz_buzz)(numbers_1000_2000,test_prediction2)\n",
    "    print('1000 to 2000\\'s accuracy:',np.mean(np.argmax(Y_1000_2000,axis=1) == test_prediction2))\n",
    "    print(output)\n",
    "    \n",
    "    test_prediction3 = sess.run(predict, feed_dict={X:X_1000_1100})\n",
    "    output = np.vectorize(fizz_buzz)(numbers_1000_1100,test_prediction3)\n",
    "    print('1000 to 1100\\'s accuracy:',np.mean(np.argmax(Y_1000_1100,axis=1) == test_prediction3))\n",
    "    print(output)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#测fizz buzz接口\n",
    "# print(Y_1000_1100)\n",
    "print(test_prediction3)\n",
    "print(X_1000_1100)\n",
    "output = np.vectorize(fizz_buzz)(X_1000_1100.shape[0],test_prediction3)\n",
    "print(output)\n",
    "\n",
    "\n",
    "print(fizz_buzz(1026,0))\n",
    "print(fizz_buzz(1026,1))\n",
    "print(fizz_buzz(1026,2))\n",
    "print(fizz_buzz(1026,3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 测试vectorize\n",
    "本例用到的向量化，是把一一对应的自然数和预测通过fizzbuzz函数输出对应字符串，不涉及广播和复杂应用\n",
    "下边的复杂例子其实我也没细看。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10,)\n",
      "['fizz' 'fizz' 'fizz' 'fizz' 'fizz' 'fizz' 'fizz' 'fizz' 'fizz' 'fizz']\n"
     ]
    }
   ],
   "source": [
    "numbers = np.arange(1,11)\n",
    "prediction = np.array([1 for i in range(1,11)])\n",
    "print(prediction.shape)\n",
    "output = np.vectorize(fizz_buzz)(numbers,prediction)\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "i is  [1, 11, 111]  j is  [2, 22, 222]\n",
      "<numpy.lib.function_base.vectorize object at 0x7fdeb6c36780>\n"
     ]
    }
   ],
   "source": [
    "#同上，一一对应的情形。\n",
    "def test(i, j):\n",
    "    print('i is ',i,' j is ',j)\n",
    "    return [i,j]\n",
    "out = np.vectorize(test([1,11,111],[2,22,222]))\n",
    "print(out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<numpy.lib.function_base.vectorize object at 0x7fdeb6c36d30>\n",
      "<class 'numpy.lib.function_base.vectorize'>\n",
      "[3 4 1 2]\n",
      "[11 12 13 14]\n"
     ]
    }
   ],
   "source": [
    "def myfunc(a, b):\n",
    "    if a > b:\n",
    "        return a - b\n",
    "    else:\n",
    "        return a + b\n",
    "vfunc = np.vectorize(myfunc)#向量化函数句柄\n",
    "print(vfunc)\n",
    "print(type(vfunc))\n",
    "print(vfunc([1,2,3,4],2))#两个输入不用对等，就是广播\n",
    "print(vfunc([1,2,3,4],[10,10,10,10]))#但是如果对等，当然是一一计算的\n",
    "#print(vfunc([1,2,3,4],[10,10,10]))#这样就不行了，不支持强行后延，只有两种：只有一个元素，广播；有一个数组，必须shape对应"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x input is: 0\n",
      "res is : 3\n",
      "x input is: 0\n",
      "res is : 3\n",
      "x input is: 1\n",
      "res is : 6\n",
      "[3 6]\n"
     ]
    }
   ],
   "source": [
    "#“excluded”参数可用于防止对某些参数进行向量化。这对于固定长度的数组类参数（如“polyval”中多项式的系数）很有用：\n",
    "#说人话，这个情况就是上边说到的不能支持的情况，用参数excluded可以办到，具体的，参数'p'可能也是专用的方法，不深究了。\n",
    "def mypolyval(p,x):\n",
    "    print('x input is:',x)\n",
    "    _p = list(p)#list化了一个_p，然后逐个取出\n",
    "    res = _p.pop(0)\n",
    "    while _p:#polynomial是多项式\n",
    "        res = res*x + _p.pop(0)\n",
    "    print('res is :',res)\n",
    "    return res\n",
    "vpolyval = np.vectorize(mypolyval, excluded = ['p'])\n",
    "print(vpolyval(p = [1,2,3], x = [0,1]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x input is: 0\n",
      "res is : 3\n",
      "x input is: 0\n",
      "res is : 3\n",
      "x input is: 1\n",
      "res is : 6\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([3, 6])"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vpolyval.excluded.add(0)\n",
    "vpolyval([1, 2, 3], x=[0, 1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(array([ 1., -1.]), array([ 0.,  0.]))\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[ 1.,  2.,  1.,  0.,  0.,  0.],\n",
       "       [ 0.,  1.,  2.,  1.,  0.,  0.],\n",
       "       [ 0.,  0.,  1.,  2.,  1.,  0.],\n",
       "       [ 0.,  0.,  0.,  1.,  2.,  1.]])"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#'signature'参数允许对作用于固定长度的非标量数组的函数进行向量化。例如，您可以将其用于皮尔逊相关系数及其p值的矢量化计算：\n",
    "\n",
    "import scipy.stats\n",
    "pearsonr = np.vectorize(scipy.stats.pearsonr,signature='(n),(n)->(),()')\n",
    "print(pearsonr([[0, 1, 2, 3]], [[1, 2, 3, 4], [4, 3, 2, 1]]))\n",
    "#(array([ 1., -1.]), array([ 0.,  0.]))\n",
    "\n",
    "convolve = np.vectorize(np.convolve, signature='(n),(m)->(k)')\n",
    "convolve(np.eye(4), [1, 2, 1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[4 0 2 1 3 8 7 5 6 9]\n",
      "[     5    345  12312     34    653 123123     54  23443]\n",
      "permute b: [    34    345  23443 123123     54  12312    653      5]\n",
      "real b: [     5    345  12312     34    653 123123     54  23443]\n",
      "[ 0  1  2  3  4  5  6  7  8  9 10 11]\n",
      "[ 6  9  4  2  7  1 11  5  3 10  8  0]\n"
     ]
    }
   ],
   "source": [
    "#return一个打乱的顺序，或者生成一个混乱的顺序\n",
    "a = np.random.permutation(10)#类似arange用法,不不过不支持双参数。\n",
    "print(a)\n",
    "# a2 = np.random.permutation(5, 10)\n",
    "# print(a2)\n",
    "b = np.random.permutation([12312,123123,23443,5,34,653,54,345])\n",
    "print(b)\n",
    "\n",
    "#不改变原数组。\n",
    "print('permute b:',np.random.permutation(b))\n",
    "print('real b:',b)\n",
    "c = np.arange(12)\n",
    "print(c)\n",
    "print(np.random.permutation(c))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on built-in function permutation:\n",
      "\n",
      "permutation(...) method of mtrand.RandomState instance\n",
      "    permutation(x)\n",
      "    \n",
      "    Randomly permute a sequence, or return a permuted range.\n",
      "    \n",
      "    If `x` is a multi-dimensional array, it is only shuffled along its\n",
      "    first index.\n",
      "    \n",
      "    Parameters\n",
      "    ----------\n",
      "    x : int or array_like\n",
      "        If `x` is an integer, randomly permute ``np.arange(x)``.\n",
      "        If `x` is an array, make a copy and shuffle the elements\n",
      "        randomly.\n",
      "    \n",
      "    Returns\n",
      "    -------\n",
      "    out : ndarray\n",
      "        Permuted sequence or array range.\n",
      "    \n",
      "    Examples\n",
      "    --------\n",
      "    >>> np.random.permutation(10)\n",
      "    array([1, 7, 4, 3, 0, 9, 2, 5, 8, 6])\n",
      "    \n",
      "    >>> np.random.permutation([1, 4, 9, 12, 15])\n",
      "    array([15,  1,  9,  4, 12])\n",
      "    \n",
      "    >>> arr = np.arange(9).reshape((3, 3))\n",
      "    >>> np.random.permutation(arr)\n",
      "    array([[6, 7, 8],\n",
      "           [0, 1, 2],\n",
      "           [3, 4, 5]])\n",
      "\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "virtual py36",
   "language": "python",
   "name": "py36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
