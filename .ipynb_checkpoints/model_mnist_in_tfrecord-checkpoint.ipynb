{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 把数据转存到tfrecord，然后从tfrecord拿数据训练网络\n",
    "\n",
    "\n",
    "具体一些细节和疑问：\n",
    "所谓那套mnist的jpg从哪取:\n",
    "如果自己转，需要面临的问题有，txt文件和里边对应标签的处理。繁琐一些。\n",
    "先找到他提供的数据\n",
    "\n",
    "单次拿数据能否超过capacity数量？测试使用coord是否多此一举？还是coord反而是为了解决这个问题而存在的？\n",
    "\n",
    "可以再多练的：\n",
    "tfrecord基本操作。\n",
    "管道和线程\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "INPUT_SIZE = 784\n",
    "OUTPUT_SIZE = 10\n",
    "L1_SIZE = 500\n",
    "\n",
    "BATCH_SIZE = 200\n",
    "LEARNING_RATE_BASE = 0.1\n",
    "LEARNING_RATE_DECAY = 0.99\n",
    "REGULAR_SCALE = 0.0001\n",
    "STEPS = 50000\n",
    "MOVING_AVERAGE_DECAY = 0.99\n",
    "MODEL_SAVE_PATH = './model/'\n",
    "MODEL_NAME = 'mnist_model'\n",
    "\n",
    "#DATA_DIR = 'MNIST_data'#本例没用，走tfrecord了\n",
    "\n",
    "train_num_examples = 60000#也就是mnist.train.num_examples返回的那个数值。\n",
    "\n",
    "image_train_path = 'tensorflow_mooc/fc4/mnist_data_jpg/mnist_train_jpg_60000/'\n",
    "label_train_path = 'tensorflow_mooc/fc4/mnist_data_jpg/mnist_train_jpg_60000.txt'\n",
    "image_test_path = 'tensorflow_mooc/fc4/mnist_data_jpg/mnist_test_jpg_10000/'\n",
    "label_test_path = 'tensorflow_mooc/fc4/mnist_data_jpg/mnist_test_jpg_10000.txt'\n",
    "\n",
    "tfRecord_train = 'tensorflow_mooc/fc4/data/mnist_train.tfrecords'\n",
    "tfRecord_test = 'tensorflow_mooc/fc4/data/mnist_test.tfrecords'\n",
    "\n",
    "data_path = 'tensorflow_mooc/fc4/my_mnist_tfRecord/'#同tensorflow_mooc/fc4/data\n",
    "resize_height = 28\n",
    "resize_width = 28\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def read_tfRecord(tfRecord_path):\n",
    "    filename_queue = tf.train.string_input_producer([tfRecord_path])#这应该是拿着路径生成一个文件名的队列\n",
    "    reader = tf.TFRecordReader()#reader对象\n",
    "    _, serialized_example = reader.read(filename_queue)#读出来，已经是example了？\n",
    "    features = tf.parse_single_example(serialized_example, features={'label':tf.FixedLenFeature([10],tf.int64),\n",
    "                                                                    'img_raw':tf.FixedLenFeature([],tf.string)})\n",
    "    img = tf.decode_raw(features['img_raw'], tf.uint8)\n",
    "    img.set_shape([784])\n",
    "    img = tf.cast(img, tf.float32) * (1./255.)\n",
    "    label = tf.cast(features['label'], tf.float32)\n",
    "    return img, label\n",
    "    \n",
    "def get_tfRecord(num, isTrain = True):\n",
    "    if isTrain:\n",
    "        tfRecord_path = tfRecord_train\n",
    "    else:\n",
    "        tfRecord_path = tfRecord_test\n",
    "    img, label = read_tfRecord(tfRecord_path)\n",
    "        \n",
    "    img_batch, label_batch = tf.train.shuffle_batch([img, label],\n",
    "                                                    batch_size = num,\n",
    "                                                   num_threads = 2,\n",
    "                                                   capacity = 1000,\n",
    "                                                   min_after_dequeue = 700)#因为是动态的，好下功能是池子剩余不到700就再填充。\n",
    "    \n",
    "    return img_batch, label_batch\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def write_tfRecord(tfRecordName, image_path, label_path):\n",
    "    writer = tf.python_io.TFRecordWriter(tfRecordName)\n",
    "    num_pic = 0\n",
    "    f = open(label_path, 'r')\n",
    "    contents = f.readlines()\n",
    "    f.close()\n",
    "    for content in contents:#遍历所有行（数据和标签）\n",
    "        value = content.split()#空格分割\n",
    "        img_path = image_path + value[0]\n",
    "        img = Image.open(img_path)\n",
    "        img_raw = img.tobytes()\n",
    "        labels = [0] * 10#one-hot形式\n",
    "        labels[int(value[1])] = 1\n",
    "        example = tf.train.Example(features = tf.train.Features(feature={#图片+标签\n",
    "            'img_raw':tf.train.Feature(bytes_list = tf.train.BytesList(value=[img_raw])),\n",
    "            'label':tf.train.Feature(int64_list = tf.train.Int64List(value=labels))\n",
    "        }))\n",
    "        writer.write(example.SerializeToString())\n",
    "        num_pic += 1\n",
    "        print('the number of picture is ',num_pic)\n",
    "    writer.close()\n",
    "    print('write tfrecord successfully')\n",
    "        \n",
    "def generate_tfRecord():\n",
    "    isExists = os.path.exists(data_path)\n",
    "    if not isExists:\n",
    "        os.makedirs(data_path)\n",
    "    else:\n",
    "        print('dir exists')\n",
    "    #从后两者取数据生成前者，所以，我还是得先准备好数据放到指定路径，目前没有\n",
    "    write_tfRecord(tfRecord_train, image_train_path, label_train_path)\n",
    "    write_tfRecord(tfRecord_test, image_test_path, label_test_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_weight(shape, regular_scale):\n",
    "    w = tf.Variable(tf.truncated_normal(shape,stddev=0.1))\n",
    "    if regular_scale != None:\n",
    "        tf.add_to_collection('regular_loss',tf.contrib.layers.l2_regularizer(regular_scale)(w))\n",
    "    return w\n",
    "def get_bias(shape):\n",
    "    b = tf.Variable(tf.zeros(shape))\n",
    "    return b\n",
    "def net(x,regular_scale):\n",
    "    w1 = get_weight([INPUT_SIZE,L1_SIZE], regular_scale)\n",
    "    b1 = get_bias([L1_SIZE])\n",
    "    l1 = tf.nn.relu(tf.matmul(x,w1)+b1)\n",
    "    \n",
    "    w2 = get_weight([L1_SIZE, OUTPUT_SIZE], regular_scale)\n",
    "    b2 = get_bias([OUTPUT_SIZE])\n",
    "    y = tf.matmul(l1,w2)+b2\n",
    "    return y  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def train():\n",
    "    x = tf.placeholder(tf.float32, [None,INPUT_SIZE])\n",
    "    y_ = tf.placeholder(tf.float32, [None,OUTPUT_SIZE])\n",
    "    y = net(x,REGULAR_SCALE)\n",
    "    global_step = tf.Variable(0, trainable = False)\n",
    "    \n",
    "    ce = tf.nn.sparse_softmax_cross_entropy_with_logits(logits = y, labels = tf.argmax(y_,1))#label需要转换成非one-hot\n",
    "    cem = tf.reduce_mean(ce)\n",
    "    loss = cem + tf.add_n(tf.get_collection('regular_loss'))\n",
    "    \n",
    "    learning_rate = tf.train.exponential_decay(LEARNING_RATE_BASE,global_step,\n",
    "                                               train_num_examples / BATCH_SIZE,\n",
    "                                              LEARNING_RATE_DECAY,\n",
    "                                               staircase = True)\n",
    "    train_op = tf.train.GradientDescentOptimizer(learning_rate).minimize(loss, global_step = global_step)\n",
    "    \n",
    "    ema = tf.train.ExponentialMovingAverage(MOVING_AVERAGE_DECAY, global_step)\n",
    "    ema_op = ema.apply(tf.trainable_variables())\n",
    "    with tf.control_dependencies([train_op, ema_op]):\n",
    "        train_op = tf.no_op(name='train')#train_op重名，也可以改名\n",
    "    \n",
    "    saver = tf.train.Saver()\n",
    "    img_batch, label_batch = get_tfRecord(BATCH_SIZE, isTrain = True)#替换原来的数据来源\n",
    "    \n",
    "    \n",
    "    writer = tf.summary.FileWriter('model_mnist_in_tfrecord_graph')\n",
    "    with tf.Session() as sess:\n",
    "        tf.global_variables_initializer().run()\n",
    "        \n",
    "        #断点续训：\n",
    "        ckpt = tf.train.get_checkpoint_state(MODEL_SAVE_PATH)\n",
    "        if ckpt and ckpt.model_checkpoint_path:\n",
    "            saver.restore(sess, ckpt.model_checkpoint_path)\n",
    "            \n",
    "        #如何使for循环中的img_batch每次不同？用sess.run()从管道取的,赋值给xs，ys\n",
    "        #临时开启多线程，提升管道的读取效率。\n",
    "        coord = tf.train.Coordinator()\n",
    "        threads = tf.train.start_queue_runners(sess = sess, coord = coord)\n",
    "        \n",
    "        for i in range(STEPS):\n",
    "            xs, ys = sess.run([img_batch, label_batch])#data.train.next_batch(BATCH_SIZE)\n",
    "            _, loss_val, step = sess.run([train_op,loss,global_step],feed_dict={x:xs, y_:ys})#替换原来的数据来源\n",
    "            if i % 1000 == 0:\n",
    "                print('after %d steps, total loss is %g'%(step,loss_val))\n",
    "                saver.save(sess, os.path.join(MODEL_SAVE_PATH, MODEL_NAME), global_step=global_step)\n",
    "        coord.request_stop()\n",
    "        coord.join(threads)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from ./model/mnist_model-54002\n",
      "after 54003 steps, total loss is 0.124507\n",
      "after 55003 steps, total loss is 0.122255\n",
      "after 56003 steps, total loss is 0.118437\n",
      "after 57003 steps, total loss is 0.128632\n",
      "after 58003 steps, total loss is 0.120297\n",
      "after 59003 steps, total loss is 0.121506\n",
      "after 60003 steps, total loss is 0.129775\n",
      "after 61003 steps, total loss is 0.122284\n",
      "after 62003 steps, total loss is 0.121707\n",
      "after 63003 steps, total loss is 0.117267\n",
      "after 64003 steps, total loss is 0.120999\n",
      "after 65003 steps, total loss is 0.116291\n",
      "after 66003 steps, total loss is 0.124315\n",
      "INFO:tensorflow:Error reported to Coordinator: <class 'tensorflow.python.framework.errors_impl.CancelledError'>, Run call was cancelled\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-6-31984fdcd049>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m     \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0m__name__\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'__main__'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m     \u001b[0mmain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-6-31984fdcd049>\u001b[0m in \u001b[0;36mmain\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mmain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m     \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0m__name__\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'__main__'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0mmain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-5-950095e07db4>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m()\u001b[0m\n\u001b[1;32m     38\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     39\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mSTEPS\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 40\u001b[0;31m             \u001b[0mxs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mys\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msess\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mimg_batch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabel_batch\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;31m#data.train.next_batch(BATCH_SIZE)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     41\u001b[0m             \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss_val\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstep\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msess\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtrain_op\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mglobal_step\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mfeed_dict\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mxs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mys\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;31m#替换原来的数据来源\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     42\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mi\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;36m1000\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/qw/anaconda3/envs/py36/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    787\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    788\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 789\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    790\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    791\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/qw/anaconda3/envs/py36/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    995\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    996\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[0;32m--> 997\u001b[0;31m                              feed_dict_string, options, run_metadata)\n\u001b[0m\u001b[1;32m    998\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    999\u001b[0m       \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/qw/anaconda3/envs/py36/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_run\u001b[0;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1130\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1131\u001b[0m       return self._do_call(_run_fn, self._session, feed_dict, fetch_list,\n\u001b[0;32m-> 1132\u001b[0;31m                            target_list, options, run_metadata)\n\u001b[0m\u001b[1;32m   1133\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1134\u001b[0m       return self._do_call(_prun_fn, self._session, handle, feed_dict,\n",
      "\u001b[0;32m/home/qw/anaconda3/envs/py36/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1137\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1138\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1139\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1140\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1141\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/qw/anaconda3/envs/py36/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[0;34m(session, feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[1;32m   1119\u001b[0m         return tf_session.TF_Run(session, options,\n\u001b[1;32m   1120\u001b[0m                                  \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1121\u001b[0;31m                                  status, run_metadata)\n\u001b[0m\u001b[1;32m   1122\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1123\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_prun_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msession\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "def main():\n",
    "    train()\n",
    "if __name__ == '__main__':\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ema_restore length: 4\n",
      "ema_restore: {'Variable_1/ExponentialMovingAverage': <tf.Variable 'Variable_1:0' shape=(500,) dtype=float32_ref>, 'Variable_3/ExponentialMovingAverage': <tf.Variable 'Variable_3:0' shape=(10,) dtype=float32_ref>, 'Variable/ExponentialMovingAverage': <tf.Variable 'Variable:0' shape=(784, 500) dtype=float32_ref>, 'Variable_2/ExponentialMovingAverage': <tf.Variable 'Variable_2:0' shape=(500, 10) dtype=float32_ref>}\n",
      "saver: <tensorflow.python.training.saver.Saver object at 0x7f4b0c9f0780>\n",
      "saver print end\n",
      "INFO:tensorflow:Restoring parameters from ./model/mnist_model-66003\n",
      "after 66003teps, accuracy is 0.981100\n",
      "INFO:tensorflow:Restoring parameters from ./model/mnist_model-66003\n",
      "after 66003teps, accuracy is 0.981200\n",
      "INFO:tensorflow:Restoring parameters from ./model/mnist_model-66003\n",
      "after 66003teps, accuracy is 0.981500\n",
      "INFO:tensorflow:Restoring parameters from ./model/mnist_model-66003\n",
      "after 66003teps, accuracy is 0.981500\n",
      "INFO:tensorflow:Restoring parameters from ./model/mnist_model-66003\n",
      "after 66003teps, accuracy is 0.981000\n",
      "INFO:tensorflow:Restoring parameters from ./model/mnist_model-66003\n",
      "after 66003teps, accuracy is 0.980900\n",
      "INFO:tensorflow:Restoring parameters from ./model/mnist_model-66003\n",
      "after 66003teps, accuracy is 0.981300\n",
      "INFO:tensorflow:Restoring parameters from ./model/mnist_model-66003\n",
      "after 66003teps, accuracy is 0.981000\n",
      "INFO:tensorflow:Restoring parameters from ./model/mnist_model-66003\n",
      "after 66003teps, accuracy is 0.981000\n",
      "INFO:tensorflow:Restoring parameters from ./model/mnist_model-66003\n",
      "after 66003teps, accuracy is 0.980900\n"
     ]
    }
   ],
   "source": [
    "#测试\n",
    "\n",
    "import tensorflow as tf\n",
    "import time\n",
    "TEST_INTERVAL_SECS = 5\n",
    "TEST_NUM = 10000\n",
    "def test():\n",
    "    with tf.Graph().as_default() as g:\n",
    "        x = tf.placeholder(tf.float32, [None, INPUT_SIZE])\n",
    "        y_ = tf.placeholder(tf.float32, [None, OUTPUT_SIZE])\n",
    "        y = net(x,None)#Net结构是直接读取的定义，ema的变量都是从这找的\n",
    "        \n",
    "        ema = tf.train.ExponentialMovingAverage(MOVING_AVERAGE_DECAY)\n",
    "        #这是一套映射：如果变量有ema，就读ema，否则，用普通。\n",
    "        #用ema resotre来初始化saver，影响saver.restore的读取内容吗？也就是说，能读取bias吗？\n",
    "        #根据形状判断（因为变量名不可读），应该是有bias的\n",
    "        ema_restore = ema.variables_to_restore()\n",
    "        #打印的含义？为何有Exponential，是已经读到了？还是按规则自己假设的变量名？应该是按规则预设的变量名\n",
    "        print('ema_restore length:',len(ema_restore))\n",
    "        print('ema_restore:',ema_restore)\n",
    "        saver = tf.train.Saver(ema_restore)\n",
    "        print('saver:',saver)#<class 'tensorflow.python.training.saver.Saver'>\n",
    "        print('saver print end')\n",
    "        correct_prediction = tf.equal(tf.argmax(y,1), tf.argmax(y_,1))\n",
    "        accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
    "        \n",
    "        \n",
    "        img_batch, label_batch = get_tfRecord(TEST_NUM, isTrain = False)#取数据流程同train\n",
    "        #为了测这个，可以写个for循环，任意次\n",
    "#         while True:#把N秒一次改为总共一次\n",
    "        for i in range(10):\n",
    "            with tf.Session() as sess:\n",
    "                ckpt = tf.train.get_checkpoint_state(MODEL_SAVE_PATH)\n",
    "                if ckpt and ckpt.model_checkpoint_path:\n",
    "                    saver.restore(sess, ckpt.model_checkpoint_path)#他只是从proto拿了路径，还是自己的restore\n",
    "                    global_step = ckpt.model_checkpoint_path.split('/')[-1].split('-')[-1]\n",
    "                    \n",
    "                    #在这个大循环下，使用这个模式是否多此一举？\n",
    "                    #capacity只有1000的，你这要获取10000？拿得出来？\n",
    "                    #如果不用，会怎么样？\n",
    "                    \n",
    "                    coord = tf.train.Coordinator()\n",
    "                    threads = tf.train.start_queue_runners(sess = sess, coord = coord)\n",
    "                    xs, ys = sess.run([img_batch, label_batch])\n",
    "                    accuracy_score = sess.run(accuracy,feed_dict={x:xs,y_:ys})\n",
    "                    \n",
    "                    print('after % steps, accuracy is %f'%(global_step,accuracy_score))\n",
    "\n",
    "                    coord.request_stop()\n",
    "                    coord.join(threads)\n",
    "                    \n",
    "                else:\n",
    "                    print('no ckpt')\n",
    "                    return\n",
    "            time.sleep(TEST_INTERVAL_SECS)\n",
    "def main():\n",
    "    test()\n",
    "if __name__ == '__main__':\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 测试生成一套小数据集的tfrecord"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dir exists\n",
      "<class 'tensorflow.core.example.feature_pb2.Int64List'>\n",
      "example label:\n",
      " value: 1\n",
      "value: 0\n",
      "value: 0\n",
      "value: 0\n",
      "value: 0\n",
      "value: 0\n",
      "value: 0\n",
      "value: 0\n",
      "value: 0\n",
      "value: 0\n",
      "\n",
      "the number of picture is  1\n",
      "<class 'tensorflow.core.example.feature_pb2.Int64List'>\n",
      "example label:\n",
      " value: 0\n",
      "value: 0\n",
      "value: 0\n",
      "value: 0\n",
      "value: 0\n",
      "value: 1\n",
      "value: 0\n",
      "value: 0\n",
      "value: 0\n",
      "value: 0\n",
      "\n",
      "the number of picture is  2\n",
      "<class 'tensorflow.core.example.feature_pb2.Int64List'>\n",
      "example label:\n",
      " value: 0\n",
      "value: 0\n",
      "value: 0\n",
      "value: 0\n",
      "value: 0\n",
      "value: 1\n",
      "value: 0\n",
      "value: 0\n",
      "value: 0\n",
      "value: 0\n",
      "\n",
      "the number of picture is  3\n",
      "<class 'tensorflow.core.example.feature_pb2.Int64List'>\n",
      "example label:\n",
      " value: 0\n",
      "value: 0\n",
      "value: 0\n",
      "value: 0\n",
      "value: 0\n",
      "value: 1\n",
      "value: 0\n",
      "value: 0\n",
      "value: 0\n",
      "value: 0\n",
      "\n",
      "the number of picture is  4\n",
      "<class 'tensorflow.core.example.feature_pb2.Int64List'>\n",
      "example label:\n",
      " value: 0\n",
      "value: 0\n",
      "value: 0\n",
      "value: 0\n",
      "value: 0\n",
      "value: 1\n",
      "value: 0\n",
      "value: 0\n",
      "value: 0\n",
      "value: 0\n",
      "\n",
      "the number of picture is  5\n",
      "write tfrecord successfully\n"
     ]
    }
   ],
   "source": [
    "data_path_mini = 'tensorflow_mooc/fc4/mini_tfRecord/'#同tensorflow_mooc/fc4/data\n",
    "tfRecord_train_mini = 'tensorflow_mooc/fc4/mini_tfRecord/mini_mnist_train.tfrecords'\n",
    "tfRecord_test_mini = 'tensorflow_mooc/fc4/mini_tfRecord/mini_mnist_test.tfrecords'\n",
    "\n",
    "MINI_BATCH_NUM = 5\n",
    "\n",
    "def write_tfRecord_mini(tfRecordName, image_path, label_path):\n",
    "    writer = tf.python_io.TFRecordWriter(tfRecordName)\n",
    "    num_pic = 0\n",
    "    f = open(label_path, 'r')\n",
    "#     contents = f.readlines()\n",
    "    contents = [f.readline() for x in range(MINI_BATCH_NUM)]\n",
    "    f.close()\n",
    "    for content in contents[:MINI_BATCH_NUM]:#mini，做少点\n",
    "        value = content.split()#空格分割\n",
    "        img_path = image_path + value[0]\n",
    "        img = Image.open(img_path)\n",
    "        img_raw = img.tobytes()\n",
    "        labels = [0] * 10#one-hot形式\n",
    "        labels[int(value[1])] = 1\n",
    "        example = tf.train.Example(features = tf.train.Features(feature={#图片+标签\n",
    "            'img_raw':tf.train.Feature(bytes_list = tf.train.BytesList(value=[img_raw])),\n",
    "            'label':tf.train.Feature(int64_list = tf.train.Int64List(value=labels))\n",
    "        }))\n",
    "        print(type(example.features.feature['label'].int64_list))\n",
    "        print('example label:\\n',example.features.feature['label'].int64_list)\n",
    "        writer.write(example.SerializeToString())\n",
    "        num_pic += 1\n",
    "        print('the number of picture is ',num_pic)\n",
    "    writer.close()\n",
    "    print('write tfrecord successfully')\n",
    "        \n",
    "def generate_tfRecord_mini():\n",
    "    isExists = os.path.exists(data_path_mini)\n",
    "    if not isExists:\n",
    "        os.makedirs(data_path_mini)\n",
    "    else:\n",
    "        print('dir exists')\n",
    "    write_tfRecord_mini(tfRecord_train_mini, image_train_path, label_train_path)\n",
    "#     write_tfRecord_mini(tfRecord_test_mini, image_test_path, label_test_path)\n",
    "generate_tfRecord_mini()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 对比shuffle和原始的读取\n",
    "原始的读取，一次只读一个，每一次都是严格按照写入顺序，循环读取\n",
    "\n",
    "shuffle，可以指定一次读取几个，一次性的打乱顺序，每次都按固定顺序读出。\n",
    "capacity的大小并不影响逻辑，prefetching，预读取，所以还是性能的关系。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "img: Tensor(\"mul_71:0\", shape=(784,), dtype=float32)\n",
      "label: Tensor(\"Cast_143:0\", shape=(10,), dtype=float32)\n",
      "run img: [ 1.  0.  0.  0.  0.  0.  0.  0.  0.  0.]\n",
      "run img: [ 0.  0.  0.  0.  0.  1.  0.  0.  0.  0.]\n",
      "run img: [ 0.  0.  0.  0.  0.  1.  0.  0.  0.  0.]\n",
      "run img: [ 0.  0.  0.  0.  0.  1.  0.  0.  0.  0.]\n",
      "run img: [ 0.  0.  0.  0.  0.  1.  0.  0.  0.  0.]\n",
      "run img: [ 1.  0.  0.  0.  0.  0.  0.  0.  0.  0.]\n",
      "run img: [ 0.  0.  0.  0.  0.  1.  0.  0.  0.  0.]\n",
      "run img: [ 0.  0.  0.  0.  0.  1.  0.  0.  0.  0.]\n",
      "run img: [ 0.  0.  0.  0.  0.  1.  0.  0.  0.  0.]\n",
      "run img: [ 0.  0.  0.  0.  0.  1.  0.  0.  0.  0.]\n"
     ]
    }
   ],
   "source": [
    "#label应该是所有样本的label都算，raw也是。\n",
    "#注意不能直接sess.run，有producer。\n",
    "\n",
    "def read_tfRecord_mini(tfRecord_path):\n",
    "    filename_queue = tf.train.string_input_producer([tfRecord_path])#这应该是拿着路径生成一个文件名的队列\n",
    "    reader = tf.TFRecordReader()#reader对象\n",
    "    _, serialized_example = reader.read(filename_queue)#读出来，已经是example了？\n",
    "    features = tf.parse_single_example(serialized_example, features={'label':tf.FixedLenFeature([10],tf.int64),\n",
    "                                                                    'img_raw':tf.FixedLenFeature([],tf.string)})\n",
    "    #features本身就是dict\n",
    "    \n",
    "    img = tf.decode_raw(features['img_raw'], tf.uint8)\n",
    "    img.set_shape([784])\n",
    "    img = tf.cast(img, tf.float32) * (1./255.)\n",
    "    label = tf.cast(features['label'], tf.float32)\n",
    "    return img, label\n",
    "\n",
    "img,label = read_tfRecord_mini(tfRecord_train_mini)\n",
    "# img,label = read_tfRecord_mini(tfRecord_test_mini)\n",
    "print('img:',img)\n",
    "print('label:',label)\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    coord=tf.train.Coordinator()\n",
    "    threads=tf.train.start_queue_runners(sess=sess,coord=coord)\n",
    "    for i in range(MINI_BATCH_NUM):\n",
    "        print('run img:',sess.run(label))\n",
    "#     for i in range(MINI_BATCH_NUM):#再来一遍，还是一样的顺序\n",
    "#         print('run img:',sess.run(label))\n",
    "#     for i in range(MINI_BATCH_NUM):#再来一遍，还是一样的顺序\n",
    "#         print('run img:',sess.run(label))\n",
    "    coord.request_stop()\n",
    "    coord.join(threads)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "run：\n",
      "5,5,0,5,5,0,5,5,5,5,\n",
      "run：\n",
      "5,5,5,5,5,5,0,5,5,5,\n",
      "run：\n",
      "5,5,5,0,5,0,5,5,5,5,\n"
     ]
    }
   ],
   "source": [
    "#对比前一个例子的顺序读取，这个顺序还是有所打乱的。\n",
    "def get_tfRecord_mini(num, isTrain = True):\n",
    "    if isTrain:\n",
    "        tfRecord_path = tfRecord_train_mini\n",
    "    else:\n",
    "        tfRecord_path = tfRecord_test_mini\n",
    "    img, label = read_tfRecord_mini(tfRecord_path)\n",
    "\n",
    "    img_batch, label_batch = tf.train.shuffle_batch([img, label],\n",
    "                                                    batch_size = num,\n",
    "                                                   num_threads = 2,\n",
    "                                                   capacity = MINI_BATCH_NUM*10,\n",
    "                                                   min_after_dequeue = 1)\n",
    "    return img_batch, label_batch\n",
    "\n",
    "#测试，一次取大于capacity的，当然也能取的出。\n",
    "img_batch, label_batch = get_tfRecord_mini(MINI_BATCH_NUM*2,True)#MINI_BATCH_NUM\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    coord=tf.train.Coordinator()\n",
    "    threads=tf.train.start_queue_runners(sess=sess,coord=coord)\n",
    "    \n",
    "    for i in range(3):\n",
    "        img_,label_ = sess.run([img_batch,label_batch])#重点，之前不小心把run写前边去了，for循环显得没意义。\n",
    "        print('run：')\n",
    "        for l in label_:\n",
    "            num = np.argmax(l)\n",
    "            print(num,end=',')\n",
    "        print()\n",
    "    coord.request_stop()\n",
    "    coord.join(threads)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#读一下filename_queue的内容，不提取内容\n",
    "#不允许的\n",
    "#TypeError: Can not convert a FIFOQueue into a Tensor or Operation.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on function shuffle_batch in module tensorflow.python.training.input:\n",
      "\n",
      "shuffle_batch(tensors, batch_size, capacity, min_after_dequeue, num_threads=1, seed=None, enqueue_many=False, shapes=None, allow_smaller_final_batch=False, shared_name=None, name=None)\n",
      "    Creates batches by randomly shuffling tensors.\n",
      "    \n",
      "    This function adds the following to the current `Graph`:\n",
      "    \n",
      "    * A shuffling queue into which tensors from `tensors` are enqueued.\n",
      "    * A `dequeue_many` operation to create batches from the queue.\n",
      "    * A `QueueRunner` to `QUEUE_RUNNER` collection, to enqueue the tensors\n",
      "      from `tensors`.\n",
      "    \n",
      "    If `enqueue_many` is `False`, `tensors` is assumed to represent a\n",
      "    single example.  An input tensor with shape `[x, y, z]` will be output\n",
      "    as a tensor with shape `[batch_size, x, y, z]`.\n",
      "    \n",
      "    If `enqueue_many` is `True`, `tensors` is assumed to represent a\n",
      "    batch of examples, where the first dimension is indexed by example,\n",
      "    and all members of `tensors` should have the same size in the\n",
      "    first dimension.  If an input tensor has shape `[*, x, y, z]`, the\n",
      "    output will have shape `[batch_size, x, y, z]`.\n",
      "    \n",
      "    The `capacity` argument controls the how long the prefetching is allowed to\n",
      "    grow the queues.\n",
      "    \n",
      "    The returned operation is a dequeue operation and will throw\n",
      "    `tf.errors.OutOfRangeError` if the input queue is exhausted. If this\n",
      "    operation is feeding another input queue, its queue runner will catch\n",
      "    this exception, however, if this operation is used in your main thread\n",
      "    you are responsible for catching this yourself.\n",
      "    \n",
      "    For example:\n",
      "    \n",
      "    ```python\n",
      "    # Creates batches of 32 images and 32 labels.\n",
      "    image_batch, label_batch = tf.train.shuffle_batch(\n",
      "          [single_image, single_label],\n",
      "          batch_size=32,\n",
      "          num_threads=4,\n",
      "          capacity=50000,\n",
      "          min_after_dequeue=10000)\n",
      "    ```\n",
      "    \n",
      "    *N.B.:* You must ensure that either (i) the `shapes` argument is\n",
      "    passed, or (ii) all of the tensors in `tensors` must have\n",
      "    fully-defined shapes. `ValueError` will be raised if neither of\n",
      "    these conditions holds.\n",
      "    \n",
      "    If `allow_smaller_final_batch` is `True`, a smaller batch value than\n",
      "    `batch_size` is returned when the queue is closed and there are not enough\n",
      "    elements to fill the batch, otherwise the pending elements are discarded.\n",
      "    In addition, all output tensors' static shapes, as accessed via the\n",
      "    `get_shape` method will have a first `Dimension` value of `None`, and\n",
      "    operations that depend on fixed batch_size would fail.\n",
      "    \n",
      "    Args:\n",
      "      tensors: The list or dictionary of tensors to enqueue.\n",
      "      batch_size: The new batch size pulled from the queue.\n",
      "      capacity: An integer. The maximum number of elements in the queue.\n",
      "      min_after_dequeue: Minimum number elements in the queue after a\n",
      "        dequeue, used to ensure a level of mixing of elements.\n",
      "      num_threads: The number of threads enqueuing `tensor_list`.\n",
      "      seed: Seed for the random shuffling within the queue.\n",
      "      enqueue_many: Whether each tensor in `tensor_list` is a single example.\n",
      "      shapes: (Optional) The shapes for each example.  Defaults to the\n",
      "        inferred shapes for `tensor_list`.\n",
      "      allow_smaller_final_batch: (Optional) Boolean. If `True`, allow the final\n",
      "        batch to be smaller if there are insufficient items left in the queue.\n",
      "      shared_name: (Optional) If set, this queue will be shared under the given\n",
      "        name across multiple sessions.\n",
      "      name: (Optional) A name for the operations.\n",
      "    \n",
      "    Returns:\n",
      "      A list or dictionary of tensors with the types as `tensors`.\n",
      "    \n",
      "    Raises:\n",
      "      ValueError: If the `shapes` are not specified, and cannot be\n",
      "        inferred from the elements of `tensors`.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(tf.train.shuffle_batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 基本接口和示例"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "writer = tf.python_io.TFRecordWriter('recordname')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "2\n"
     ]
    }
   ],
   "source": [
    "#测coord\n",
    "#普通python循环在这里不行，既然他也绑定sess，肯定还是得用tf的操作\n",
    "#不好直观测，这个需要多个线程，本例是用的start_queue_runners直接黑盒做的。\n",
    "a = tf.Variable(1)\n",
    "op = tf.assign_add(a,1)\n",
    "b = tf.Variable(1)\n",
    "op2 = tf.assign_add(b,1)\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    tf.global_variables_initializer().run()\n",
    "    print(sess.run(a))\n",
    "    sess.run(op)\n",
    "    print(sess.run(a))\n",
    "    coord = tf.train.Coordinator()\n",
    "    threads = tf.train.start_queue_runners(sess = sess, coord = coord)\n",
    "#     for i in range(1000):\n",
    "#         print(sess.run(a),end=',')\n",
    "#         sess.run(op)\n",
    "#     for i in range(1000):\n",
    "#         print(sess.run(b),end=',')\n",
    "#         sess.run(op2)\n",
    "    coord.request_stop()\n",
    "    coord.join(threads)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# help(tf.train.Coordinator)\n",
    "#这个协调器打开以后，打开多个线程，能等待他们一起结束\n",
    "# .|  #### Usage:\n",
    "#  |  \n",
    "#  |  ```python\n",
    "#  |  # Create a coordinator.\n",
    "#  |  coord = Coordinator()\n",
    "#  |  # Start a number of threads, passing the coordinator to each of them.\n",
    "#  |  ...start thread 1...(coord, ...)\n",
    "#  |  ...start thread N...(coord, ...)\n",
    "#  |  # Wait for all the threads to terminate.\n",
    "#  |  coord.join(threads)\n",
    "#  |  ```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on function start_queue_runners in module tensorflow.python.training.queue_runner_impl:\n",
      "\n",
      "start_queue_runners(sess=None, coord=None, daemon=True, start=True, collection='queue_runners')\n",
      "    Starts all queue runners collected in the graph.\n",
      "    \n",
      "    This is a companion method to `add_queue_runner()`.  It just starts\n",
      "    threads for all queue runners collected in the graph.  It returns\n",
      "    the list of all threads.\n",
      "    \n",
      "    Args:\n",
      "      sess: `Session` used to run the queue ops.  Defaults to the\n",
      "        default session.\n",
      "      coord: Optional `Coordinator` for coordinating the started threads.\n",
      "      daemon: Whether the threads should be marked as `daemons`, meaning\n",
      "        they don't block program exit.\n",
      "      start: Set to `False` to only create the threads, not start them.\n",
      "      collection: A `GraphKey` specifying the graph collection to\n",
      "        get the queue runners from.  Defaults to `GraphKeys.QUEUE_RUNNERS`.\n",
      "    \n",
      "    Raises:\n",
      "      ValueError: if `sess` is None and there isn't any default session.\n",
      "      TypeError: if `sess` is not a `tf.Session` object.\n",
      "    \n",
      "    Returns:\n",
      "      A list of threads.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(tf.train.start_queue_runners)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "virtual py36",
   "language": "python",
   "name": "py36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
