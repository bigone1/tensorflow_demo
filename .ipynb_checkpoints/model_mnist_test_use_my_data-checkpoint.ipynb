{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "简单例子:\n",
    "\n",
    "重点关注那些save/load，还有一些附加操作的依赖,global_step。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "import os\n",
    "from PIL import Image\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "INPUT_SIZE = 784\n",
    "OUTPUT_SIZE = 10\n",
    "L1_SIZE = 500\n",
    "\n",
    "BATCH_SIZE = 200\n",
    "LEARNING_RATE_BASE = 0.1\n",
    "LEARNING_RATE_DECAY = 0.99\n",
    "REGULAR_SCALE = 0.0001\n",
    "STEPS = 50000\n",
    "MOVING_AVERAGE_DECAY = 0.99\n",
    "MODEL_SAVE_PATH = './model/'\n",
    "MODEL_NAME = 'mnist_model'\n",
    "\n",
    "DATA_DIR = 'MNIST_data'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_weight(shape, regular_scale):\n",
    "    w = tf.Variable(tf.truncated_normal(shape,stddev=0.1))\n",
    "    if regular_scale != None:\n",
    "        tf.add_to_collection('regular_loss',tf.contrib.layers.l2_regularizer(regular_scale)(w))\n",
    "    return w\n",
    "def get_bias(shape):\n",
    "    b = tf.Variable(tf.zeros(shape))\n",
    "    return b\n",
    "def net(x,regular_scale):\n",
    "    w1 = get_weight([INPUT_SIZE,L1_SIZE], regular_scale)\n",
    "    b1 = get_bias([L1_SIZE])\n",
    "    l1 = tf.nn.relu(tf.matmul(x,w1)+b1)\n",
    "    \n",
    "    w2 = get_weight([L1_SIZE, OUTPUT_SIZE], regular_scale)\n",
    "    b2 = get_bias([OUTPUT_SIZE])\n",
    "    y = tf.matmul(l1,w2)+b2\n",
    "    return y  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def train(data):\n",
    "    x = tf.placeholder(tf.float32, [None,INPUT_SIZE])\n",
    "    y_ = tf.placeholder(tf.float32, [None,OUTPUT_SIZE])\n",
    "    y = net(x,REGULAR_SCALE)\n",
    "    global_step = tf.Variable(0, trainable = False)\n",
    "    \n",
    "    ce = tf.nn.sparse_softmax_cross_entropy_with_logits(logits = y, labels = tf.argmax(y_,1))#label需要转换成非one-hot\n",
    "    cem = tf.reduce_mean(ce)\n",
    "    loss = cem + tf.add_n(tf.get_collection('regular_loss'))\n",
    "    \n",
    "    learning_rate = tf.train.exponential_decay(LEARNING_RATE_BASE,global_step,data.train.num_examples / BATCH_SIZE,\n",
    "                                              LEARNING_RATE_DECAY, staircase = True)\n",
    "    train_op = tf.train.GradientDescentOptimizer(learning_rate).minimize(loss, global_step = global_step)\n",
    "    \n",
    "    ema = tf.train.ExponentialMovingAverage(MOVING_AVERAGE_DECAY, global_step)\n",
    "    ema_op = ema.apply(tf.trainable_variables())\n",
    "    with tf.control_dependencies([train_op, ema_op]):\n",
    "        train_op = tf.no_op(name='train')#train_op重名，也可以改名\n",
    "    \n",
    "    saver = tf.train.Saver()\n",
    "    writer = tf.summary.FileWriter('model_mnist_ema_ckpt_graph')\n",
    "    with tf.Session() as sess:\n",
    "        tf.global_variables_initializer().run()\n",
    "        \n",
    "        #断点续训：\n",
    "        ckpt = tf.train.get_checkpoint_state(MODEL_SAVE_PATH)\n",
    "        if ckpt and ckpt.model_checkpoint_path:\n",
    "            saver.restore(sess, ckpt.model_checkpoint_path)\n",
    "        \n",
    "        for i in range(STEPS):\n",
    "            xs, ys = data.train.next_batch(BATCH_SIZE)\n",
    "            _, loss_val, step = sess.run([train_op,loss,global_step],feed_dict={x:xs,y_:ys})\n",
    "            if i % 1000 == 0:\n",
    "                print('after %d steps, total loss is %g'%(step,loss_val))\n",
    "                saver.save(sess, os.path.join(MODEL_SAVE_PATH, MODEL_NAME), global_step=global_step)\n",
    "                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting MNIST_data/train-images-idx3-ubyte.gz\n",
      "Extracting MNIST_data/train-labels-idx1-ubyte.gz\n",
      "Extracting MNIST_data/t10k-images-idx3-ubyte.gz\n",
      "Extracting MNIST_data/t10k-labels-idx1-ubyte.gz\n",
      "INFO:tensorflow:Restoring parameters from ./model/mnist_model-49001\n",
      "after 49002 steps, total loss is 0.126964\n",
      "after 50002 steps, total loss is 0.124086\n",
      "after 51002 steps, total loss is 0.122567\n",
      "after 52002 steps, total loss is 0.127575\n",
      "after 53002 steps, total loss is 0.120838\n",
      "after 54002 steps, total loss is 0.125351\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m--------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-30-1d32b825dd09>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmnist\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0m__name__\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'__main__'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m     \u001b[0mmain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-30-1d32b825dd09>\u001b[0m in \u001b[0;36mmain\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mmain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m     \u001b[0mmnist\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minput_data\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_data_sets\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mDATA_DIR\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mone_hot\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m     \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmnist\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0m__name__\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'__main__'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mmain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-29-4cd0c08b71de>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(data)\u001b[0m\n\u001b[1;32m     30\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mSTEPS\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     31\u001b[0m             \u001b[0mxs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mys\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnext_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mBATCH_SIZE\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 32\u001b[0;31m             \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss_val\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstep\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msess\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtrain_op\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mglobal_step\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mfeed_dict\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mxs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0my_\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mys\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     33\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mi\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;36m1000\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     34\u001b[0m                 \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'after %d steps, total loss is %g'\u001b[0m\u001b[0;34m%\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mloss_val\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/qw/anaconda3/envs/py36/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    787\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    788\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 789\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    790\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    791\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/qw/anaconda3/envs/py36/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    995\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    996\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[0;32m--> 997\u001b[0;31m                              feed_dict_string, options, run_metadata)\n\u001b[0m\u001b[1;32m    998\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    999\u001b[0m       \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/qw/anaconda3/envs/py36/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_run\u001b[0;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1130\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1131\u001b[0m       return self._do_call(_run_fn, self._session, feed_dict, fetch_list,\n\u001b[0;32m-> 1132\u001b[0;31m                            target_list, options, run_metadata)\n\u001b[0m\u001b[1;32m   1133\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1134\u001b[0m       return self._do_call(_prun_fn, self._session, handle, feed_dict,\n",
      "\u001b[0;32m/home/qw/anaconda3/envs/py36/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1137\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1138\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1139\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1140\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1141\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/qw/anaconda3/envs/py36/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[0;34m(session, feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[1;32m   1119\u001b[0m         return tf_session.TF_Run(session, options,\n\u001b[1;32m   1120\u001b[0m                                  \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1121\u001b[0;31m                                  status, run_metadata)\n\u001b[0m\u001b[1;32m   1122\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1123\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_prun_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msession\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "def main():\n",
    "    mnist = input_data.read_data_sets(DATA_DIR,one_hot=True)\n",
    "    train(mnist)\n",
    "if __name__ == '__main__':\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting MNIST_data/train-images-idx3-ubyte.gz\n",
      "Extracting MNIST_data/train-labels-idx1-ubyte.gz\n",
      "Extracting MNIST_data/t10k-images-idx3-ubyte.gz\n",
      "Extracting MNIST_data/t10k-labels-idx1-ubyte.gz\n",
      "ema_restore length: 4\n",
      "ema_restore: {'Variable_1/ExponentialMovingAverage': <tf.Variable 'Variable_1:0' shape=(500,) dtype=float32_ref>, 'Variable/ExponentialMovingAverage': <tf.Variable 'Variable:0' shape=(784, 500) dtype=float32_ref>, 'Variable_2/ExponentialMovingAverage': <tf.Variable 'Variable_2:0' shape=(500, 10) dtype=float32_ref>, 'Variable_3/ExponentialMovingAverage': <tf.Variable 'Variable_3:0' shape=(10,) dtype=float32_ref>}\n",
      "saver: <tensorflow.python.training.saver.Saver object at 0x7fbfe82a0470>\n",
      "saver print end\n",
      "ckpt type: <class 'tensorflow.python.training.checkpoint_state_pb2.CheckpointState'>\n",
      "------------------ckpt obj print start-------------------\n",
      " model_checkpoint_path: \"./model/mnist_model-54002\"\n",
      "all_model_checkpoint_paths: \"./model/mnist_model-50002\"\n",
      "all_model_checkpoint_paths: \"./model/mnist_model-51002\"\n",
      "all_model_checkpoint_paths: \"./model/mnist_model-52002\"\n",
      "all_model_checkpoint_paths: \"./model/mnist_model-53002\"\n",
      "all_model_checkpoint_paths: \"./model/mnist_model-54002\"\n",
      "\n",
      "------------------ckpt obj print end--------------------------\n",
      "ckpt all_model_checkpoint_paths type: <class 'google.protobuf.internal.containers.RepeatedScalarFieldContainer'>\n",
      "ckpt all_model_checkpoint_paths: ['./model/mnist_model-50002', './model/mnist_model-51002', './model/mnist_model-52002', './model/mnist_model-53002', './model/mnist_model-54002']\n",
      "0th path is ./model/mnist_model-50002\n",
      "1th path is ./model/mnist_model-51002\n",
      "2th path is ./model/mnist_model-52002\n",
      "3th path is ./model/mnist_model-53002\n",
      "4th path is ./model/mnist_model-54002\n",
      "INFO:tensorflow:Restoring parameters from ./model/mnist_model-54002\n",
      "after 54002teps, accuracy is 0.980900\n",
      "ckpt.model_checkpoint_path: ./model/mnist_model-54002\n",
      "ckpt path print end\n",
      "all trainable variables: [<tf.Variable 'Variable:0' shape=(784, 500) dtype=float32_ref>, <tf.Variable 'Variable_1:0' shape=(500,) dtype=float32_ref>, <tf.Variable 'Variable_2:0' shape=(500, 10) dtype=float32_ref>, <tf.Variable 'Variable_3:0' shape=(10,) dtype=float32_ref>]\n"
     ]
    }
   ],
   "source": [
    "#测试\n",
    "#假设这是独立的文件和进程，变量肯定要读出来\n",
    "#这几个读取方式交叉互动有点乱。\n",
    "#saver是拿变量的，利用ckpt从路径找到了具体带后缀的文件名\n",
    "#global_step直接就是文件名的隐含信息。\n",
    "#这里主要是拿ema，ckpt的之后再说\n",
    "\n",
    "#因为带ema的模型，预测就只需要拿ema就行了（但是ema没有bias吧，怎么办？），而拿ckpt的所有参数主要是训练期间的续训。。。\n",
    "#根据这句代码，ema肯定包含了bias，甚至global_step都要专门处理，才能防止被ema\n",
    "#ema_op = ema.apply(tf.trainable_variables())\n",
    "\n",
    "import tensorflow as tf\n",
    "import time\n",
    "TEST_INTERVAL_SECS = 5\n",
    "def test(data):\n",
    "    with tf.Graph().as_default() as g:\n",
    "        x = tf.placeholder(tf.float32, [None, INPUT_SIZE])\n",
    "        y_ = tf.placeholder(tf.float32, [None, OUTPUT_SIZE])\n",
    "        y = net(x,None)#Net结构是直接读取的定义，ema的变量都是从这找的\n",
    "        \n",
    "        ema = tf.train.ExponentialMovingAverage(MOVING_AVERAGE_DECAY)\n",
    "        #这是一套映射：如果变量有ema，就读ema，否则，用普通。\n",
    "        #用ema resotre来初始化saver，影响saver.restore的读取内容吗？也就是说，能读取bias吗？\n",
    "        #根据形状判断（因为变量名不可读），应该是有bias的\n",
    "        ema_restore = ema.variables_to_restore()\n",
    "        #打印的含义？为何有Exponential，是已经读到了？还是按规则自己假设的变量名？应该是按规则预设的变量名\n",
    "        print('ema_restore length:',len(ema_restore))\n",
    "        print('ema_restore:',ema_restore)\n",
    "        saver = tf.train.Saver(ema_restore)\n",
    "        print('saver:',saver)#<class 'tensorflow.python.training.saver.Saver'>\n",
    "        print('saver print end')\n",
    "        correct_prediction = tf.equal(tf.argmax(y,1), tf.argmax(y_,1))\n",
    "        accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
    "        \n",
    "#         while True:#把无秒一次改为总共一次\n",
    "        with tf.Session() as sess:\n",
    "            ckpt = tf.train.get_checkpoint_state(MODEL_SAVE_PATH)\n",
    "            \n",
    "            #从路径里读出来一个'checkpoint'文件，这个文件就包含了最新的model文件名和最近N个model文件名。\n",
    "            #字典形式：model_checkpoint_path，all_model_checkpoint_paths，下边已经打印。\n",
    "            print('ckpt type:', type(ckpt))\n",
    "            print('------------------ckpt obj print start-------------------\\n',ckpt)\n",
    "            print('------------------ckpt obj print end--------------------------')\n",
    "            print('ckpt all_model_checkpoint_paths type:',type(ckpt.all_model_checkpoint_paths))#算是protobuf的一个容器\n",
    "            print('ckpt all_model_checkpoint_paths:',ckpt.all_model_checkpoint_paths)\n",
    "            for i in range(len(ckpt.all_model_checkpoint_paths)):\n",
    "                print('%dth path is %s'%(i,ckpt.all_model_checkpoint_paths[i]))\n",
    "            if ckpt and ckpt.model_checkpoint_path:\n",
    "                saver.restore(sess, ckpt.model_checkpoint_path)#他只是从proto拿了路径，还是自己的restore\n",
    "                global_step = ckpt.model_checkpoint_path.split('/')[-1].split('-')[-1]\n",
    "                accuracy_score = sess.run(accuracy,feed_dict={x:data.test.images,y_:data.test.labels})\n",
    "                print('after % steps, accuracy is %f'%(global_step,accuracy_score))\n",
    "                \n",
    "                print('ckpt.model_checkpoint_path:',ckpt.model_checkpoint_path)#./model/mnist_model-49001\n",
    "                print('ckpt path print end')\n",
    "                \n",
    "                print('all trainable variables:', tf.trainable_variables())\n",
    "                \n",
    "            else:\n",
    "                print('no ckpt')\n",
    "                return\n",
    "#             time.sleep(TEST_INTERVAL_SECS)\n",
    "def main():\n",
    "    mnist = input_data.read_data_sets(DATA_DIR, one_hot=True)\n",
    "    test(mnist)\n",
    "if __name__ == '__main__':\n",
    "    main()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 下面输入手写数字图片，进行预处理并进行预测。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#from PIL import Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(28, 28)\n",
      "(28, 28)\n",
      "INFO:tensorflow:Restoring parameters from ./model/mnist_model-54002\n",
      "[8]\n",
      "INFO:tensorflow:Restoring parameters from ./model/mnist_model-54002\n",
      "[6]\n",
      "(28, 28)\n",
      "INFO:tensorflow:Restoring parameters from ./model/mnist_model-54002\n",
      "[5]\n",
      "(28, 28)\n",
      "INFO:tensorflow:Restoring parameters from ./model/mnist_model-54002\n",
      "[8]\n",
      "(28, 28)\n",
      "INFO:tensorflow:Restoring parameters from ./model/mnist_model-54002\n",
      "[2]\n",
      "(28, 28)\n",
      "INFO:tensorflow:Restoring parameters from ./model/mnist_model-54002\n",
      "[5]\n",
      "(28, 28)\n",
      "INFO:tensorflow:Restoring parameters from ./model/mnist_model-54002\n",
      "[5]\n",
      "(28, 28)\n",
      "INFO:tensorflow:Restoring parameters from ./model/mnist_model-54002\n",
      "[5]\n",
      "(28, 28)\n",
      "INFO:tensorflow:Restoring parameters from ./model/mnist_model-54002\n",
      "[4]\n",
      "(28, 28)\n",
      "INFO:tensorflow:Restoring parameters from ./model/mnist_model-54002\n",
      "[2]\n",
      "(28, 28)\n",
      "INFO:tensorflow:Restoring parameters from ./model/mnist_model-54002\n",
      "[5]\n",
      "(28, 28)\n",
      "INFO:tensorflow:Restoring parameters from ./model/mnist_model-54002\n",
      "[5]\n",
      "white:\n",
      "(28, 28)\n",
      "INFO:tensorflow:Restoring parameters from ./model/mnist_model-54002\n",
      "[5]\n",
      "(28, 28)\n",
      "INFO:tensorflow:Restoring parameters from ./model/mnist_model-54002\n",
      "[7]\n",
      "(28, 28)\n",
      "INFO:tensorflow:Restoring parameters from ./model/mnist_model-54002\n",
      "[3]\n",
      "(28, 28)\n",
      "INFO:tensorflow:Restoring parameters from ./model/mnist_model-54002\n",
      "[3]\n",
      "(28, 28)\n",
      "INFO:tensorflow:Restoring parameters from ./model/mnist_model-54002\n",
      "[8]\n",
      "(28, 28)\n",
      "INFO:tensorflow:Restoring parameters from ./model/mnist_model-54002\n",
      "[7]\n",
      "(28, 28)\n",
      "INFO:tensorflow:Restoring parameters from ./model/mnist_model-54002\n",
      "[2]\n",
      "(28, 28)\n",
      "INFO:tensorflow:Restoring parameters from ./model/mnist_model-54002\n",
      "[7]\n",
      "(28, 28)\n",
      "INFO:tensorflow:Restoring parameters from ./model/mnist_model-54002\n",
      "[1]\n"
     ]
    }
   ],
   "source": [
    "#同上面的测试阶段，主要是恢复EMA\n",
    "#他起名太烂，这何止是restore？restore之后也没返回给外界，这是restore之后直接原地预测\n",
    "#而且地址都是写死的，这严格讲，只能算预测\n",
    "#拿mnist的test去测，准确率98%，但是自己手写还是差强人意。\n",
    "\n",
    "MNIST_TEST = False\n",
    "\n",
    "\n",
    "def predict(testPicArr):\n",
    "    with tf.Graph().as_default() as tg:\n",
    "        x = tf.placeholder(tf.float32, [None, INPUT_SIZE])\n",
    "        y_ = tf.placeholder(tf.float32, [None, OUTPUT_SIZE])\n",
    "        y = net(x,None)\n",
    "        pred = tf.argmax(y,1)\n",
    "        \n",
    "        variable_averages = tf.train.ExponentialMovingAverage(MOVING_AVERAGE_DECAY)\n",
    "        variables_to_restore = variable_averages.variables_to_restore()\n",
    "        saver = tf.train.Saver(variables_to_restore)\n",
    "        \n",
    "        if MNIST_TEST:\n",
    "            mnist = input_data.read_data_sets(DATA_DIR, one_hot=True)\n",
    "            correct_prediction = tf.equal(tf.argmax(y,1), tf.argmax(y_,1))\n",
    "            accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
    "        \n",
    "        with tf.Session() as sess:\n",
    "            ckpt = tf.train.get_checkpoint_state(MODEL_SAVE_PATH)\n",
    "            if ckpt and ckpt.model_checkpoint_path:\n",
    "                saver.restore(sess, ckpt.model_checkpoint_path)\n",
    "                \n",
    "                if MNIST_TEST:\n",
    "                    accuracy_score = sess.run(accuracy,feed_dict={x:mnist.test.images,y_:mnist.test.labels})\n",
    "                    print('accuracy is %f'%(accuracy_score))\n",
    "                \n",
    "                preValue = sess.run(pred, feed_dict={x:testPicArr})\n",
    "                return preValue\n",
    "            else:\n",
    "                print('no ckpt file')\n",
    "                return -1\n",
    "            \n",
    "            \n",
    "#关于threshold，为了能使用网络图片，改成手动输入\n",
    "#灰度图改成极端情况，0是黑的255是白的，这个可以调。比如我的彩色图都是一百几到二百，可能就得弄一百大几十\n",
    "#如果是网上随便down的图，可能每个图都要设置不一样的参数，或者用平均值来代替？\n",
    "#最好是统一形式的数据\n",
    "def preprocess_picture(picDir,threshold=180):\n",
    "    img = Image.open(picDir)\n",
    "#     print(img)\n",
    "#     print(img.mode, img.size, img.format)\n",
    "#     img.show()\n",
    "    resized_img = img.resize((28,28), Image.ANTIALIAS)#好像是防锯齿\n",
    "\n",
    "    resized_img.save(picDir.split('.')[0]  +'resize' + '.png')#保存一下，但是下一步变ndarray，不能用Image的接口保存了。\n",
    "    \n",
    "    img_arr = np.array(resized_img.convert('L'))#单色，灰度\n",
    "    print(img_arr.shape)\n",
    "#     print(img_arr)\n",
    "\n",
    "    for i in range(28):\n",
    "        for j in range(28):\n",
    "            if (img_arr[i][j] < threshold):\n",
    "                img_arr[i][j] = 0\n",
    "            else:\n",
    "                img_arr[i][j] = 255\n",
    "    nm_arr = img_arr.reshape([1,784])#变成mnist形式\n",
    "    nm_arr = nm_arr.astype(np.float32)\n",
    "    img_ready = np.multiply(nm_arr, 1.0 / 255.0)#255变成1\n",
    "    \n",
    "    return img_ready\n",
    "    \n",
    "            \n",
    "#png的是RGBA，反正最终要黑白的，np.array之后就是单通道了。\n",
    "\n",
    "png_2 = preprocess_picture('mnist_pic_test/2.png',255)#windows画图的黑色手写，区分点基本就是255了。\n",
    "png_3 = preprocess_picture('mnist_pic_test/3.png',255)#windows画图的黑色手写，区分点基本就是255了。\n",
    "\n",
    "print(predict(png_2))#预测成了8.。。。。\n",
    "print(predict(png_3))#预测成了6.。。。。\n",
    "\n",
    "#换两个粗的，写的可能太细了。\n",
    "print(predict(preprocess_picture('mnist_pic_test/2_thick.png',255)))#预测成了5.。。。。\n",
    "print(predict(preprocess_picture('mnist_pic_test/3_thick.png',255)))#预测成了8.。。。。\n",
    "\n",
    "#这个刷子好像不纯黑，多涂了几次\n",
    "#稍微接近一些了\n",
    "print(predict(preprocess_picture('mnist_pic_test/2_thick_black.png',255)))#预测成了2.。。。。\n",
    "print(predict(preprocess_picture('mnist_pic_test/3_thick_black.png',255)))#预测成了5.。。。。\n",
    "\n",
    "#用纯黑笔写,写紧凑，截图时放大，就变粗了，\n",
    "#效果也不太好。\n",
    "print(predict(preprocess_picture('mnist_pic_test/2_enlarge.png',255)))#预测成了5.。。。。\n",
    "print(predict(preprocess_picture('mnist_pic_test/3_enlarge.png',255)))#预测成了5.。。。。\n",
    "\n",
    "#来俩简单的\n",
    "print(predict(preprocess_picture('mnist_pic_test/1_enlarge.png',255)))#预测成了4.。。。。\n",
    "print(predict(preprocess_picture('mnist_pic_test/4_enlarge.png',255)))#预测成了2.。。。。\n",
    "print(predict(preprocess_picture('mnist_pic_test/0_enlarge.png',255)))#预测成了5.。。。。\n",
    "print(predict(preprocess_picture('mnist_pic_test/7_enlarge.png',255)))#预测成了5.。。。。\n",
    "#万物皆可55555555\n",
    "\n",
    "#放弃，放弃，数据和模型果然得花功夫好好调才能匹配的上。\n",
    "#因为mnist本身就很简单，28*28,网络结构也简单，一个隐层。\n",
    "#手写字体是西方人的写法，自己用鼠标在画板上画的可能也不标准，所以误差大。\n",
    "#也不至于偏差这么大吧？可能颜色反了，网上搜mnist有白字有黑字，白字效果也一般。\n",
    "\n",
    "print('white:')\n",
    "# print(predict(preprocess_picture('mnist_pic_test/1_white.png',255)))#预测成了4.。。。。\n",
    "# print(predict(preprocess_picture('mnist_pic_test/2_white.png',255)))#预测成了4.。。。。\n",
    "print(predict(preprocess_picture('mnist_pic_test/3_white.png',255)))#预测成了5.。。。。\n",
    "print(predict(preprocess_picture('mnist_pic_test/4_white.png',255)))#预测成了7.。。。。\n",
    "# print(predict(preprocess_picture('mnist_pic_test/5_white.png',255)))#预测成了4.。。。。\n",
    "print(predict(preprocess_picture('mnist_pic_test/6_white.png',255)))#预测成了3.。。。。\n",
    "print(predict(preprocess_picture('mnist_pic_test/7_white.png',255)))#预测成了3.。。。。\n",
    "print(predict(preprocess_picture('mnist_pic_test/8_white.png',255)))#预测成了8.。。。。\n",
    "print(predict(preprocess_picture('mnist_pic_test/9_white.png',255)))#预测成了7.。。。。\n",
    "\n",
    "\n",
    "#另一个可能的问题，我的手写字体都是比较高的，28*28应该是正方形，所以，多少有点缩放，原则上区别也没这么大，就当写矮一点，也差不多\n",
    "\n",
    "#使用mnist自带的图片测试\n",
    "print(predict(preprocess_picture('MNIST_data/raw/mnist_train_16.jpg',255)))#预测成了2,原图也是2\n",
    "print(predict(preprocess_picture('MNIST_data/raw/mnist_train_23.jpg',255)))#预测成了1,原图是1\n",
    "print(predict(preprocess_picture('MNIST_data/raw/mnist_train_17.jpg',255)))#预测成了7,原图是9,也不绝对准确\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "<tensorflow.python.framework.ops.Graph object at 0x7f94bfb9e2e8>\n",
      "<tensorflow.python.framework.ops.Graph object at 0x7f94caed37b8>\n",
      "False\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'Graph' object has no attribute 'Session'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m--------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m               Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-49-3a7f4d3552a9>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0md\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgraph\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0mg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;31m#显式声明的g不是全局默认\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 25\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSession\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     26\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m \u001b[0;31m#用法2：直接as\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'Graph' object has no attribute 'Session'"
     ]
    }
   ],
   "source": [
    "#help(tf.Graph().as_default) #返回的是上下文manager，\n",
    "#     This method should be used if you want to create multiple graphs\n",
    "#     in the same process. For convenience, a global default graph is\n",
    "#     provided, and all ops will be added to this graph if you do not\n",
    "#     create a new graph explicitly. Use this method with the `with` keyword\n",
    "#     to specify that ops created within the scope of a block should be\n",
    "#     added to this graph.\n",
    "    \n",
    "#     The default graph is a property of the current thread. If you\n",
    "#     create a new thread, and wish to use the default graph in that\n",
    "#     thread, you must explicitly add a `with g.as_default():` in that\n",
    "#     thread's function.\n",
    "\n",
    "#下面两个用法相等\n",
    "#用法1：声明一个graph，as_default\n",
    "g = tf.Graph()\n",
    "\n",
    "with g.as_default():\n",
    "  c = tf.constant(5.0)\n",
    "  print(c.graph is g)\n",
    "d = tf.constant(6.0)\n",
    "print(d.graph)\n",
    "print(g)\n",
    "print(d.graph is g)#显式声明的g不是全局默认\n",
    "# print(g.Session())\n",
    "\n",
    "#用法2：直接as\n",
    "with tf.Graph().as_default() as g2:\n",
    "    c = tf.constant(5.0)\n",
    "    print(c.graph is g2)\n",
    "    print(c.graph is g)#无关\n",
    "    print(g2)\n",
    "\n",
    "print(tf.Graph())#每个都是一个生成的\n",
    "print(tf.Graph())\n",
    "\n",
    "# g和g2和默认graph都不一样"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'ckpt' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m--------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                    Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-5-3f266cfd0410>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mhelp\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mckpt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel_checkpoint_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'ckpt' is not defined"
     ]
    }
   ],
   "source": [
    "help(ckpt.model_checkpoint_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "variables_to_restore(moving_avg_variables=None) method of tensorflow.python.training.moving_averages.ExponentialMovingAverage instance\n",
    "    Returns a map of names to `Variables` to restore.\n",
    "    \n",
    "    If a variable has a moving average, use the moving average variable name as\n",
    "    the restore name; otherwise, use the variable name.\n",
    "    \n",
    "    For example,\n",
    "    \n",
    "    ```python\n",
    "      variables_to_restore = ema.variables_to_restore()\n",
    "      saver = tf.train.Saver(variables_to_restore)\n",
    "    ```\n",
    "    \n",
    "    Below is an example of such mapping:\n",
    "    \n",
    "    ```\n",
    "      conv/batchnorm/gamma/ExponentialMovingAverage: conv/batchnorm/gamma,\n",
    "      conv_4/conv2d_params/ExponentialMovingAverage: conv_4/conv2d_params,\n",
    "      global_step: global_step\n",
    "    ```\n",
    "    Args:\n",
    "      moving_avg_variables: a list of variables that require to use of the\n",
    "        moving variable name to be restored. If None, it will default to\n",
    "        variables.moving_average_variables() + variables.trainable_variables()\n",
    "    \n",
    "    Returns:\n",
    "      A map from restore_names to variables. The restore_name can be the\n",
    "      moving_average version of the variable name if it exist, or the original\n",
    "      variable name."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on class Saver in module tensorflow.python.training.saver:\n",
      "\n",
      "class Saver(builtins.object)\n",
      " |  Saves and restores variables.\n",
      " |  \n",
      " |  See @{$variables$Variables}\n",
      " |  for an overview of variables, saving and restoring.\n",
      " |  \n",
      " |  The `Saver` class adds ops to save and restore variables to and from\n",
      " |  *checkpoints*.  It also provides convenience methods to run these ops.\n",
      " |  \n",
      " |  Checkpoints are binary files in a proprietary format which map variable names\n",
      " |  to tensor values.  The best way to examine the contents of a checkpoint is to\n",
      " |  load it using a `Saver`.\n",
      " |  \n",
      " |  Savers can automatically number checkpoint filenames with a provided counter.\n",
      " |  This lets you keep multiple checkpoints at different steps while training a\n",
      " |  model.  For example you can number the checkpoint filenames with the training\n",
      " |  step number.  To avoid filling up disks, savers manage checkpoint files\n",
      " |  automatically. For example, they can keep only the N most recent files, or\n",
      " |  one checkpoint for every N hours of training.\n",
      " |  \n",
      " |  You number checkpoint filenames by passing a value to the optional\n",
      " |  `global_step` argument to `save()`:\n",
      " |  \n",
      " |  ```python\n",
      " |  saver.save(sess, 'my-model', global_step=0) ==> filename: 'my-model-0'\n",
      " |  ...\n",
      " |  saver.save(sess, 'my-model', global_step=1000) ==> filename: 'my-model-1000'\n",
      " |  ```\n",
      " |  \n",
      " |  Additionally, optional arguments to the `Saver()` constructor let you control\n",
      " |  the proliferation of checkpoint files on disk:\n",
      " |  \n",
      " |  * `max_to_keep` indicates the maximum number of recent checkpoint files to\n",
      " |    keep.  As new files are created, older files are deleted.  If None or 0,\n",
      " |    all checkpoint files are kept.  Defaults to 5 (that is, the 5 most recent\n",
      " |    checkpoint files are kept.)\n",
      " |  \n",
      " |  * `keep_checkpoint_every_n_hours`: In addition to keeping the most recent\n",
      " |    `max_to_keep` checkpoint files, you might want to keep one checkpoint file\n",
      " |    for every N hours of training.  This can be useful if you want to later\n",
      " |    analyze how a model progressed during a long training session.  For\n",
      " |    example, passing `keep_checkpoint_every_n_hours=2` ensures that you keep\n",
      " |    one checkpoint file for every 2 hours of training.  The default value of\n",
      " |    10,000 hours effectively disables the feature.\n",
      " |  \n",
      " |  Note that you still have to call the `save()` method to save the model.\n",
      " |  Passing these arguments to the constructor will not save variables\n",
      " |  automatically for you.\n",
      " |  \n",
      " |  A training program that saves regularly looks like:\n",
      " |  \n",
      " |  ```python\n",
      " |  ...\n",
      " |  # Create a saver.\n",
      " |  saver = tf.train.Saver(...variables...)\n",
      " |  # Launch the graph and train, saving the model every 1,000 steps.\n",
      " |  sess = tf.Session()\n",
      " |  for step in xrange(1000000):\n",
      " |      sess.run(..training_op..)\n",
      " |      if step % 1000 == 0:\n",
      " |          # Append the step number to the checkpoint name:\n",
      " |          saver.save(sess, 'my-model', global_step=step)\n",
      " |  ```\n",
      " |  \n",
      " |  In addition to checkpoint files, savers keep a protocol buffer on disk with\n",
      " |  the list of recent checkpoints. This is used to manage numbered checkpoint\n",
      " |  files and by `latest_checkpoint()`, which makes it easy to discover the path\n",
      " |  to the most recent checkpoint. That protocol buffer is stored in a file named\n",
      " |  'checkpoint' next to the checkpoint files.\n",
      " |  \n",
      " |  If you create several savers, you can specify a different filename for the\n",
      " |  protocol buffer file in the call to `save()`.\n",
      " |  \n",
      " |  Methods defined here:\n",
      " |  \n",
      " |  __init__(self, var_list=None, reshape=False, sharded=False, max_to_keep=5, keep_checkpoint_every_n_hours=10000.0, name=None, restore_sequentially=False, saver_def=None, builder=None, defer_build=False, allow_empty=False, write_version=2, pad_step_number=False, save_relative_paths=False)\n",
      " |      Creates a `Saver`.\n",
      " |      \n",
      " |      The constructor adds ops to save and restore variables.\n",
      " |      \n",
      " |      `var_list` specifies the variables that will be saved and restored. It can\n",
      " |      be passed as a `dict` or a list:\n",
      " |      \n",
      " |      * A `dict` of names to variables: The keys are the names that will be\n",
      " |        used to save or restore the variables in the checkpoint files.\n",
      " |      * A list of variables: The variables will be keyed with their op name in\n",
      " |        the checkpoint files.\n",
      " |      \n",
      " |      For example:\n",
      " |      \n",
      " |      ```python\n",
      " |      v1 = tf.Variable(..., name='v1')\n",
      " |      v2 = tf.Variable(..., name='v2')\n",
      " |      \n",
      " |      # Pass the variables as a dict:\n",
      " |      saver = tf.train.Saver({'v1': v1, 'v2': v2})\n",
      " |      \n",
      " |      # Or pass them as a list.\n",
      " |      saver = tf.train.Saver([v1, v2])\n",
      " |      # Passing a list is equivalent to passing a dict with the variable op names\n",
      " |      # as keys:\n",
      " |      saver = tf.train.Saver({v.op.name: v for v in [v1, v2]})\n",
      " |      ```\n",
      " |      \n",
      " |      The optional `reshape` argument, if `True`, allows restoring a variable from\n",
      " |      a save file where the variable had a different shape, but the same number\n",
      " |      of elements and type.  This is useful if you have reshaped a variable and\n",
      " |      want to reload it from an older checkpoint.\n",
      " |      \n",
      " |      The optional `sharded` argument, if `True`, instructs the saver to shard\n",
      " |      checkpoints per device.\n",
      " |      \n",
      " |      Args:\n",
      " |        var_list: A list of `Variable`/`SaveableObject`, or a dictionary mapping\n",
      " |          names to `SaveableObject`s. If `None`, defaults to the list of all\n",
      " |          saveable objects.\n",
      " |        reshape: If `True`, allows restoring parameters from a checkpoint\n",
      " |          where the variables have a different shape.\n",
      " |        sharded: If `True`, shard the checkpoints, one per device.\n",
      " |        max_to_keep: Maximum number of recent checkpoints to keep.\n",
      " |          Defaults to 5.\n",
      " |        keep_checkpoint_every_n_hours: How often to keep checkpoints.\n",
      " |          Defaults to 10,000 hours.\n",
      " |        name: String.  Optional name to use as a prefix when adding operations.\n",
      " |        restore_sequentially: A `Bool`, which if true, causes restore of different\n",
      " |          variables to happen sequentially within each device.  This can lower\n",
      " |          memory usage when restoring very large models.\n",
      " |        saver_def: Optional `SaverDef` proto to use instead of running the\n",
      " |          builder. This is only useful for specialty code that wants to recreate\n",
      " |          a `Saver` object for a previously built `Graph` that had a `Saver`.\n",
      " |          The `saver_def` proto should be the one returned by the\n",
      " |          `as_saver_def()` call of the `Saver` that was created for that `Graph`.\n",
      " |        builder: Optional `SaverBuilder` to use if a `saver_def` was not provided.\n",
      " |          Defaults to `BaseSaverBuilder()`.\n",
      " |        defer_build: If `True`, defer adding the save and restore ops to the\n",
      " |          `build()` call. In that case `build()` should be called before\n",
      " |          finalizing the graph or using the saver.\n",
      " |        allow_empty: If `False` (default) raise an error if there are no\n",
      " |          variables in the graph. Otherwise, construct the saver anyway and make\n",
      " |          it a no-op.\n",
      " |        write_version: controls what format to use when saving checkpoints.  It\n",
      " |          also affects certain filepath matching logic.  The V2 format is the\n",
      " |          recommended choice: it is much more optimized than V1 in terms of\n",
      " |          memory required and latency incurred during restore.  Regardless of\n",
      " |          this flag, the Saver is able to restore from both V2 and V1 checkpoints.\n",
      " |        pad_step_number: if True, pads the global step number in the checkpoint\n",
      " |          filepaths to some fixed width (8 by default).  This is turned off by\n",
      " |          default.\n",
      " |        save_relative_paths: If `True`, will write relative paths to the\n",
      " |          checkpoint state file. This is needed if the user wants to copy the\n",
      " |          checkpoint directory and reload from the copied directory.\n",
      " |      \n",
      " |      Raises:\n",
      " |        TypeError: If `var_list` is invalid.\n",
      " |        ValueError: If any of the keys or values in `var_list` are not unique.\n",
      " |  \n",
      " |  as_saver_def(self)\n",
      " |      Generates a `SaverDef` representation of this saver.\n",
      " |      \n",
      " |      Returns:\n",
      " |        A `SaverDef` proto.\n",
      " |  \n",
      " |  build(self)\n",
      " |      Builds saver_def.\n",
      " |  \n",
      " |  export_meta_graph(self, filename=None, collection_list=None, as_text=False, export_scope=None, clear_devices=False)\n",
      " |      Writes `MetaGraphDef` to save_path/filename.\n",
      " |      \n",
      " |      Args:\n",
      " |        filename: Optional meta_graph filename including the path.\n",
      " |        collection_list: List of string keys to collect.\n",
      " |        as_text: If `True`, writes the meta_graph as an ASCII proto.\n",
      " |        export_scope: Optional `string`. Name scope to remove.\n",
      " |        clear_devices: Whether or not to clear the device field for an `Operation`\n",
      " |          or `Tensor` during export.\n",
      " |      \n",
      " |      Returns:\n",
      " |        A `MetaGraphDef` proto.\n",
      " |  \n",
      " |  recover_last_checkpoints(self, checkpoint_paths)\n",
      " |      Recovers the internal saver state after a crash.\n",
      " |      \n",
      " |      This method is useful for recovering the \"self._last_checkpoints\" state.\n",
      " |      \n",
      " |      Globs for the checkpoints pointed to by `checkpoint_paths`.  If the files\n",
      " |      exist, use their mtime as the checkpoint timestamp.\n",
      " |      \n",
      " |      Args:\n",
      " |        checkpoint_paths: a list of checkpoint paths.\n",
      " |  \n",
      " |  restore(self, sess, save_path)\n",
      " |      Restores previously saved variables.\n",
      " |      \n",
      " |      This method runs the ops added by the constructor for restoring variables.\n",
      " |      It requires a session in which the graph was launched.  The variables to\n",
      " |      restore do not have to have been initialized, as restoring is itself a way\n",
      " |      to initialize variables.\n",
      " |      \n",
      " |      The `save_path` argument is typically a value previously returned from a\n",
      " |      `save()` call, or a call to `latest_checkpoint()`.\n",
      " |      \n",
      " |      Args:\n",
      " |        sess: A `Session` to use to restore the parameters.\n",
      " |        save_path: Path where parameters were previously saved.\n",
      " |  \n",
      " |  save(self, sess, save_path, global_step=None, latest_filename=None, meta_graph_suffix='meta', write_meta_graph=True, write_state=True)\n",
      " |      Saves variables.\n",
      " |      \n",
      " |      This method runs the ops added by the constructor for saving variables.\n",
      " |      It requires a session in which the graph was launched.  The variables to\n",
      " |      save must also have been initialized.\n",
      " |      \n",
      " |      The method returns the path of the newly created checkpoint file.  This\n",
      " |      path can be passed directly to a call to `restore()`.\n",
      " |      \n",
      " |      Args:\n",
      " |        sess: A Session to use to save the variables.\n",
      " |        save_path: String.  Path to the checkpoint filename.  If the saver is\n",
      " |          `sharded`, this is the prefix of the sharded checkpoint filename.\n",
      " |        global_step: If provided the global step number is appended to\n",
      " |          `save_path` to create the checkpoint filename. The optional argument\n",
      " |          can be a `Tensor`, a `Tensor` name or an integer.\n",
      " |        latest_filename: Optional name for the protocol buffer file that will\n",
      " |          contains the list of most recent checkpoint filenames.  That file,\n",
      " |          kept in the same directory as the checkpoint files, is automatically\n",
      " |          managed by the saver to keep track of recent checkpoints.  Defaults to\n",
      " |          'checkpoint'.\n",
      " |        meta_graph_suffix: Suffix for `MetaGraphDef` file. Defaults to 'meta'.\n",
      " |        write_meta_graph: `Boolean` indicating whether or not to write the meta\n",
      " |          graph file.\n",
      " |        write_state: `Boolean` indicating whether or not to write the\n",
      " |          `CheckpointStateProto`.\n",
      " |      \n",
      " |      Returns:\n",
      " |        A string: path at which the variables were saved.  If the saver is\n",
      " |          sharded, this string ends with: '-?????-of-nnnnn' where 'nnnnn'\n",
      " |          is the number of shards created.\n",
      " |        If the saver is empty, returns None.\n",
      " |      \n",
      " |      Raises:\n",
      " |        TypeError: If `sess` is not a `Session`.\n",
      " |        ValueError: If `latest_filename` contains path components, or if it\n",
      " |          collides with `save_path`.\n",
      " |        RuntimeError: If save and restore ops weren't built.\n",
      " |  \n",
      " |  set_last_checkpoints(self, last_checkpoints)\n",
      " |      DEPRECATED: Use set_last_checkpoints_with_time.\n",
      " |      \n",
      " |      Sets the list of old checkpoint filenames.\n",
      " |      \n",
      " |      Args:\n",
      " |        last_checkpoints: A list of checkpoint filenames.\n",
      " |      \n",
      " |      Raises:\n",
      " |        AssertionError: If last_checkpoints is not a list.\n",
      " |  \n",
      " |  set_last_checkpoints_with_time(self, last_checkpoints_with_time)\n",
      " |      Sets the list of old checkpoint filenames and timestamps.\n",
      " |      \n",
      " |      Args:\n",
      " |        last_checkpoints_with_time: A list of tuples of checkpoint filenames and\n",
      " |          timestamps.\n",
      " |      \n",
      " |      Raises:\n",
      " |        AssertionError: If last_checkpoints_with_time is not a list.\n",
      " |  \n",
      " |  to_proto(self, export_scope=None)\n",
      " |      Converts this `Saver` to a `SaverDef` protocol buffer.\n",
      " |      \n",
      " |      Args:\n",
      " |        export_scope: Optional `string`. Name scope to remove.\n",
      " |      \n",
      " |      Returns:\n",
      " |        A `SaverDef` protocol buffer.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Static methods defined here:\n",
      " |  \n",
      " |  from_proto(saver_def, import_scope=None)\n",
      " |      Returns a `Saver` object created from `saver_def`.\n",
      " |      \n",
      " |      Args:\n",
      " |        saver_def: a `SaveDef` protocol buffer.\n",
      " |        import_scope: Optional `string`. Name scope to use.\n",
      " |      \n",
      " |      Returns:\n",
      " |        A `Saver` built from saver_def.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data descriptors defined here:\n",
      " |  \n",
      " |  __dict__\n",
      " |      dictionary for instance variables (if defined)\n",
      " |  \n",
      " |  __weakref__\n",
      " |      list of weak references to the object (if defined)\n",
      " |  \n",
      " |  last_checkpoints\n",
      " |      List of not-yet-deleted checkpoint filenames.\n",
      " |      \n",
      " |      You can pass any of the returned values to `restore()`.\n",
      " |      \n",
      " |      Returns:\n",
      " |        A list of checkpoint filenames, sorted from oldest to newest.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(tf.train.Saver)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "module 'tensorflow.python.training.training' has no attribute 'saver'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m--------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m               Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-13-473e72cc0de6>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mhelp\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msaver\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrestore\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m: module 'tensorflow.python.training.training' has no attribute 'saver'"
     ]
    }
   ],
   "source": [
    "help(tf.train.saver.restore)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on function get_checkpoint_state in module tensorflow.python.training.saver:\n",
      "\n",
      "get_checkpoint_state(checkpoint_dir, latest_filename=None)\n",
      "    Returns CheckpointState proto from the \"checkpoint\" file.\n",
      "    \n",
      "    If the \"checkpoint\" file contains a valid CheckpointState\n",
      "    proto, returns it.\n",
      "    \n",
      "    Args:\n",
      "      checkpoint_dir: The directory of checkpoints.\n",
      "      latest_filename: Optional name of the checkpoint file.  Default to\n",
      "        'checkpoint'.\n",
      "    \n",
      "    Returns:\n",
      "      A CheckpointState if the state was available, None\n",
      "      otherwise.\n",
      "    \n",
      "    Raises:\n",
      "      ValueError: if the checkpoint read doesn't have model_checkpoint_path set.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(tf.train.get_checkpoint_state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "virtual py36",
   "language": "python",
   "name": "py36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
