{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 学习率衰减\n",
    "实际训练，如果我中途改学习率基数呢？\n",
    "\n",
    "关于global_step的变化，谁触发，主要是靠optimizer，optimizer训练一次，global_step变一次。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "LEARNING_RATE_BASE = 0.1\n",
    "LEARNING_RATE_DECAY = 0.99\n",
    "LEARNING_RATE_STEP = 5#多少轮mini BATCH后更新学习率，一般按总样本一轮，这里特殊，为了能看出staircase效果，不设1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "global_step = tf.Variable(0, trainable= False)#如果不设False的话,也没影响,计算图中没有他。\n",
    "#staircase，离散间隔降低学习速率，True是隔step步之后一次性改变，False是每一次都改变。\n",
    "learning_rate = tf.train.exponential_decay(learning_rate = LEARNING_RATE_BASE, global_step=global_step,\n",
    "                                          decay_steps = LEARNING_RATE_STEP,decay_rate = LEARNING_RATE_DECAY,\n",
    "                                           staircase = True, name='exp_decay')\n",
    "w = tf.Variable(tf.constant(5,dtype=tf.float32))\n",
    "loss = tf.square(w+1)#目标是-1\n",
    "train_op = tf.train.GradientDescentOptimizer(learning_rate).minimize(loss, global_step = global_step)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  0 steps: global_step is   1.0, w is 3.800000, learning_rate is 0.100000,loss is 23.040001\n",
      "  1 steps: global_step is   2.0, w is 2.840000, learning_rate is 0.100000,loss is 14.745600\n",
      "  2 steps: global_step is   3.0, w is 2.072000, learning_rate is 0.100000,loss is 9.437184\n",
      "  3 steps: global_step is   4.0, w is 1.457600, learning_rate is 0.100000,loss is 6.039798\n",
      "  4 steps: global_step is   5.0, w is 0.966080, learning_rate is 0.099000,loss is 3.865470\n",
      "  5 steps: global_step is   6.0, w is 0.576796, learning_rate is 0.099000,loss is 2.486286\n",
      "  6 steps: global_step is   7.0, w is 0.264591, learning_rate is 0.099000,loss is 1.599189\n",
      "  7 steps: global_step is   8.0, w is 0.014202, learning_rate is 0.099000,loss is 1.028605\n",
      "  8 steps: global_step is   9.0, w is -0.186610, learning_rate is 0.099000,loss is 0.661603\n",
      "  9 steps: global_step is  10.0, w is -0.347661, learning_rate is 0.098010,loss is 0.425546\n",
      " 10 steps: global_step is  11.0, w is -0.475533, learning_rate is 0.098010,loss is 0.275066\n",
      " 11 steps: global_step is  12.0, w is -0.578339, learning_rate is 0.098010,loss is 0.177798\n",
      " 12 steps: global_step is  13.0, w is -0.660993, learning_rate is 0.098010,loss is 0.114926\n",
      " 13 steps: global_step is  14.0, w is -0.727445, learning_rate is 0.098010,loss is 0.074286\n",
      " 14 steps: global_step is  15.0, w is -0.780871, learning_rate is 0.097030,loss is 0.048017\n",
      " 15 steps: global_step is  16.0, w is -0.823395, learning_rate is 0.097030,loss is 0.031189\n",
      " 16 steps: global_step is  17.0, w is -0.857667, learning_rate is 0.097030,loss is 0.020259\n",
      " 17 steps: global_step is  18.0, w is -0.885288, learning_rate is 0.097030,loss is 0.013159\n",
      " 18 steps: global_step is  19.0, w is -0.907549, learning_rate is 0.097030,loss is 0.008547\n",
      " 19 steps: global_step is  20.0, w is -0.925490, learning_rate is 0.096060,loss is 0.005552\n",
      " 20 steps: global_step is  21.0, w is -0.939805, learning_rate is 0.096060,loss is 0.003623\n",
      " 21 steps: global_step is  22.0, w is -0.951370, learning_rate is 0.096060,loss is 0.002365\n",
      " 22 steps: global_step is  23.0, w is -0.960712, learning_rate is 0.096060,loss is 0.001544\n",
      " 23 steps: global_step is  24.0, w is -0.968260, learning_rate is 0.096060,loss is 0.001007\n",
      " 24 steps: global_step is  25.0, w is -0.974358, learning_rate is 0.095099,loss is 0.000658\n",
      " 25 steps: global_step is  26.0, w is -0.979235, learning_rate is 0.095099,loss is 0.000431\n",
      " 26 steps: global_step is  27.0, w is -0.983185, learning_rate is 0.095099,loss is 0.000283\n",
      " 27 steps: global_step is  28.0, w is -0.986383, learning_rate is 0.095099,loss is 0.000185\n",
      " 28 steps: global_step is  29.0, w is -0.988973, learning_rate is 0.095099,loss is 0.000122\n",
      " 29 steps: global_step is  30.0, w is -0.991070, learning_rate is 0.094148,loss is 0.000080\n",
      " 30 steps: global_step is  31.0, w is -0.992752, learning_rate is 0.094148,loss is 0.000053\n",
      " 31 steps: global_step is  32.0, w is -0.994116, learning_rate is 0.094148,loss is 0.000035\n",
      " 32 steps: global_step is  33.0, w is -0.995224, learning_rate is 0.094148,loss is 0.000023\n",
      " 33 steps: global_step is  34.0, w is -0.996124, learning_rate is 0.094148,loss is 0.000015\n",
      " 34 steps: global_step is  35.0, w is -0.996853, learning_rate is 0.093207,loss is 0.000010\n",
      " 35 steps: global_step is  36.0, w is -0.997440, learning_rate is 0.093207,loss is 0.000007\n",
      " 36 steps: global_step is  37.0, w is -0.997917, learning_rate is 0.093207,loss is 0.000004\n",
      " 37 steps: global_step is  38.0, w is -0.998305, learning_rate is 0.093207,loss is 0.000003\n",
      " 38 steps: global_step is  39.0, w is -0.998621, learning_rate is 0.093207,loss is 0.000002\n",
      " 39 steps: global_step is  40.0, w is -0.998878, learning_rate is 0.092274,loss is 0.000001\n"
     ]
    }
   ],
   "source": [
    "with tf.Session() as sess:\n",
    "    init = tf.global_variables_initializer()\n",
    "    sess.run(init)\n",
    "    for i in range(40):\n",
    "        sess.run(train_op)\n",
    "        learning_rate_val = sess.run(learning_rate)\n",
    "        global_step_val = sess.run(global_step)\n",
    "        w_val = sess.run(w)\n",
    "        loss_val = sess.run(loss)\n",
    "        #格式控制：正负数有法对其吗？\n",
    "        print('%3s steps: global_step is %5.1f, w is %7f, learning_rate is %f,loss is %f'%(i,global_step_val,w_val,learning_rate_val,loss_val))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# 加载ckpt，更改学习率基数，观察学习率。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "LEARNING_RATE_BASE = 0.05\n",
    "LEARNING_RATE_DECAY = 0.99\n",
    "LEARNING_RATE_STEP = 5#多少轮mini BATCH后更新学习率，一般按总样本一轮，这里特殊，为了能看出staircase效果，不设1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#this is a learning rate decay examples\n",
    "import tensorflow as tf\n",
    "\n",
    "starter_learning_rate = 0.01\n",
    "\n",
    "x = tf.constant(5.0)\n",
    "label = tf.constant(17.0)#3x+2\n",
    "w = tf.Variable(0.8)\n",
    "b = tf.Variable(0.1)\n",
    "prediction = w * x + b\n",
    "loss = (prediction - label) ** 2\n",
    "\n",
    "train_op0 = tf.train.GradientDescentOptimizer(starter_learning_rate).minimize(loss)\n",
    "\n",
    "steps_per_decay = 10\n",
    "decay_factor = 0.96\n",
    "\n",
    "global_step = tf.Variable(0, trainable=False)\n",
    "learning_rate = tf.train.exponential_decay(learning_rate = starter_learning_rate,\n",
    "                                           global_step = global_step,\n",
    "                                           decay_steps = steps_per_decay,\n",
    "                                           decay_rate = decay_factor,\n",
    "                                           staircase = True,#If `True` decay the learning rate at discrete intervals\n",
    "                                           #staircase = False,change learning rate at every step\n",
    "                                           )\n",
    "\n",
    "\n",
    "#passing global_step to minimize() will increment it at each step\n",
    "train_op1 = (\n",
    "    tf.train.GradientDescentOptimizer(learning_rate).minimize(loss,global_step = global_step)\n",
    ")\n",
    "#this cannot change lr\n",
    "train_op2 = (\n",
    "    #minimize()'s param global_step is the one to updates global_step\n",
    "    #aka apply_gradients()\n",
    "    tf.train.GradientDescentOptimizer(learning_rate).minimize(loss)\n",
    ")\n",
    "\n",
    "#tchange lr,but not use it.\n",
    "train_op22 = (\n",
    "    #minimize()'s param global_step is the one to updates global_step\n",
    "    #aka apply_gradients()\n",
    "    tf.train.GradientDescentOptimizer(starter_learning_rate).minimize(loss,global_step = global_step)\n",
    ")\n",
    "#this will change lr too\n",
    "optimizer = tf.train.GradientDescentOptimizer(learning_rate)\n",
    "grads = optimizer.compute_gradients(loss)\n",
    "train_op3 = optimizer.apply_gradients(grads, global_step = global_step)\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    for i in range(30):\n",
    "        print('global_step:',sess.run(global_step))\n",
    "        print('predict:',sess.run(prediction))\n",
    "        print('learning rate:',sess.run(learning_rate))\n",
    "        print(sess.run(loss))\n",
    "        # sess.run(train_op0)\n",
    "        #sess.run(train_op1)\n",
    "        #sess.run(train_op2)\n",
    "        sess.run(train_op22)\n",
    "        #sess.run(train_op3)\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "virtual py36",
   "language": "python",
   "name": "py36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
